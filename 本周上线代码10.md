
### **Best Practices for Ryzen AI in Quark ONNX (Summary)**  

This guide outlines **Post-Training Quantization (PTQ)** techniques for optimizing models (e.g., YOLOv8) using **Quark ONNX** for **AMD Ryzen AI**.  

---

### **Key Steps**  

1. **Install Dependencies**  
   ```bash
   python -m pip install -r requirements.txt
   ```  

2. **Prepare Model**  
   - Download a pre-trained YOLOv8 model (PyTorch/ONNX).  
   - Convert PyTorch to ONNX (optional):  
     ```bash
     python export_yolo_to_onnx.py --input_model_path models/yolov8n-face.pt --output_model_path yolov8n-face.onnx
     ```  

3. **Calibration Data**  
   - Provide a folder of images (PNG/JPG) for calibration.  
   - Example:  
     ```bash
     mkdir calib_images
     wget -O calib_images/daisy.jpg https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/test_images/daisy.jpg?raw=true
     ```  

4. **Quantization Methods**  
   - **XINT8**: Symmetric INT8 (power-of-two scales, MinMSE calibration).  
   - **A8W8**: Symmetric INT8 (float scales, MinMax calibration).  
   - **A16W8**: INT16 activation + INT8 weights (float scales, MinMax).  
   - **BF16**: 16-bit floating-point (wide dynamic range).  
   - **BFP16**: Block Floating Point (shared exponent for efficiency).  
   - **CLE**: Cross-Layer Equalization (balances weights).  
   - **ADAROUND**: Adaptive Rounding (optimizes weight rounding).  
   - **ADAQUANT**: Adaptive Quantization (minimizes layer-wise errors).  
   - **Exclude Nodes/Subgraphs**: Skip quantization for specific layers (improves accuracy).  

   **Example Commands:**  
   ```bash
   # XINT8 Quantization
   python quantize_quark.py --input_model_path models/yolov8n-face.onnx --calib_data_path calib_images --output_model_path models/yolov8n-face_quantized.onnx --config XINT8

   # A16W8 Quantization (recommended for object detection)
   python quantize_quark.py --input_model_path models/yolov8n-face.onnx --calib_data_path calib_images --output_model_path models/yolov8n-face_quantized.onnx --config A16W8

   # BF16 Quantization
   python quantize_quark.py --input_model_path models/yolov8n-face.onnx --calib_data_path calib_images --output_model_path models/yolov8n-face_quantized.onnx --config BF16

   # ADAROUND (XINT8)
   python quantize_quark.py --input_model_path models/yolov8n-face.onnx --calib_data_path calib_images --output_model_path models/yolov8n-face_quantized.onnx --config XINT8 --adaround --learning_rate 0.1 --num_iters 3000

   # Exclude Nodes (XINT8)
   python quantize_quark.py --input_model_path models/yolov8n-face.onnx --calib_data_path calib_images --output_model_path models/yolov8n-face_quantized.onnx --config XINT8 --exclude_nodes "/model.22/Concat_5"
   ```  

5. **Inference**  
   - Run detection on an image:  
     ```bash
     mkdir detection_images
     python onnx_evaluate.py --input_model_path models/yolov8n-face.onnx --input_image [INPUT_IMAGE] --output_image [DETECTION_IMAGE]
     ```  

---

### **Best Practices**  
âœ… **For object detection (YOLO), A16W8, BF16, and node exclusion work best.**  
âœ… **Use ADAROUND/ADAQUANT for improved accuracy in low-bit quantization.**  
âœ… **Exclude post-processing subgraphs to retain detection accuracy.**  

This workflow ensures **efficient quantization** while maintaining **high accuracy** for **Ryzen AI-optimized models**. ðŸš€
