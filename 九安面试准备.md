# 微调训练调参的关键参数关系与调整策略
 
## 口语化面试回答版本

微调模型时，学习率和批量大小是两个最重要的参数，它们之间有着紧密联系。

说到学习率，我一般会采用"从小开始试"的策略。对于大模型微调，通常从1e-5开始，看模型表现后再调整。如果损失下降太慢，就适当提高；如果训练不稳定，就降低一些。我经常使用学习率预热和衰减策略，这样模型开始能稳定学习，后期又能精细调整。

批量大小方面，理论上它和学习率成正比关系 - 批量大小翻倍，学习率也应该近似翻倍。但实际操作中，我会先考虑硬件限制，用显存能承受的最大批量。对于微调任务，我发现小批量(16或32)往往效果更好，因为引入一定随机性有助于模型泛化。如果显存不够，我会用梯度累积来模拟更大批量。

这两个参数调整时我会相互考虑：大批量配大学习率，小批量用小学习率。此外，我还会关注权重衰减(通常0.01)和优化器选择(微调首选AdamW)。

最重要的是，没有放之四海而皆准的设置，我总是会针对具体任务做网格搜索或贝叶斯优化，找到最佳组合。实践经验告诉我，耐心尝试不同配置并密切监控验证集表现是找到最佳参数的关键。

# DPO和PPO的区别
 
## 口语化面试回答版本

DPO和PPO是两种用于大模型对齐的算法，它们有很大不同。

PPO是传统的强化学习方法，需要三个模型：策略模型、价值模型和奖励模型。它的训练过程比较复杂，首先要预训练模型，然后训练奖励模型，最后用强化学习来优化策略。这个过程需要不断与环境交互，计算成本高，而且训练容易不稳定，调参非常麻烦。

而DPO是最近才提出的方法，它的核心思想是直接从人类偏好数据学习，不需要显式构建奖励模型。比如说，我们有成对的回答A和B，人类认为A比B好，DPO就直接学习这种偏好关系。它把RLHF的复杂过程简化为一个单阶段的优化问题，只需要一个模型和一个简单的目标函数。

DPO的优势很明显：实现简单，只需要一个β参数来控制与基础模型的偏离程度；训练稳定，不容易出现模型崩溃；计算效率高，不需要额外的奖励模型；而且数据需求更简单，只需要成对的偏好数据。

在实际应用中，DPO因为简单高效，正在逐渐取代PPO成为大模型对齐的主流方法。不过PPO在某些需要复杂环境交互的场景中仍有其价值。选择哪种方法，主要取决于具体任务、可用资源和数据类型。
# 预训练阶段的loss和微调阶段的loss的区别
 

## 口语化面试回答版本

预训练和微调阶段的损失函数有很大不同，这主要体现在它们的目标和应用场景上。

预训练阶段，我们的目标是让模型学习语言的基本规律和知识。最常用的损失函数是交叉熵损失，用于自回归语言模型预测下一个token。比如GPT系列模型就是这样训练的。还有像BERT这样的模型使用掩码语言模型损失，预测被随机掩盖的词。预训练通常在海量数据上进行，损失计算涉及整个词表，学习率和批量也相对较大。

而到了微调阶段，损失函数就更加任务导向了。我们不再关注通用语言能力，而是特定任务的表现。最基础的是监督微调损失，仍然是交叉熵，但应用于特定任务数据。现在更流行的是指令微调损失，基于指令-回复对训练模型理解和执行指令。对于需要与人类偏好对齐的场景，我们会使用RLHF相关损失，如PPO或DPO损失。

微调阶段的一个关键特点是使用较小的学习率（通常1e-5左右）和较小的批量大小，以避免过拟合。同时，我们会加入更强的正则化，比如更大的权重衰减和KL散度惩罚，防止模型偏离预训练的知识太远。

总的来说，预训练损失关注"学习语言"，微调损失关注"解决问题"。预训练是在广阔的海洋中学习游泳，微调则是学习在特定河流中逆流而上。两者结合，才能让模型既有广泛的知识基础，又能精确地解决特定领域的问题。
# 分布式训练和分布式推理在模型生成时的区别
 

## 口语化面试回答版本

分布式训练和分布式推理在模型生成时有很大不同，这主要体现在工作模式和优化目标上。

在分布式训练中，生成过程其实是为了计算损失函数。我们通常使用教师强制方法，也就是说，我们已经知道正确答案，只是让模型预测下一个token，然后计算误差。整个序列的前向传播通常一次性完成，不需要真正的自回归生成。训练时我们关注的是如何最大化吞吐量，让模型尽快学习大量数据。

而分布式推理中的生成是真正的自回归过程。模型需要一个接一个地生成token，每生成一个token后，都要将其作为下一步的输入。这意味着我们需要进行多次前向传播，每次只生成一个新token。在这个过程中，模型并行是主要策略，比如张量并行或流水线并行。

一个关键区别是通信模式。训练时，主要通信发生在梯度同步阶段；而推理时，每生成一个token都需要跨设备通信，这使得通信延迟直接影响生成速度。

另一个重要区别是批处理方式。训练时，我们可以将所有样本填充到相同长度，高效处理大批量数据；而推理时，我们面临动态批处理的挑战，不同请求的序列长度不同，还需要处理不断变化的序列长度，这就需要连续批处理技术。

在内存管理上，训练关注梯度存储和优化器状态，而推理主要关注KV缓存管理。推理时，我们需要存储之前生成token的注意力键值对，这样才能高效生成下一个token。

总的来说，训练时我们优化的是整体吞吐量，而推理时我们需要平衡延迟和吞吐量，提供良好的用户体验。这就是为什么我们看到很多推理优化技术，如推理量化、KV缓存优化等，它们都是为了解决推理特有的挑战。

# 对KL散度（Kullback-Leibler Divergence）的理解
 

## 口语化面试回答版本

KL散度，全称Kullback-Leibler散度，是衡量两个概率分布差异的重要工具。我对它的理解主要从几个方面来说：

首先，从直观上理解，KL散度告诉我们两个分布有多"不像"。如果我们有真实分布P和近似分布Q，KL散度就衡量用Q来表示P时会损失多少信息。它的数学形式是P(x)乘以log(P(x)/Q(x))的求和或积分，这个值永远大于等于0，只有当两个分布完全相同时才等于0。

KL散度有个重要特性是不对称的，也就是说D_KL(P||Q)不等于D_KL(Q||P)。这不是缺点，而是它的特性，在不同应用场景下我们会选择不同方向的KL散度。

在离散情况下，KL散度是各个状态KL贡献的加权和，权重就是P分布的概率。这意味着P中概率高的状态对总散度贡献大，而P中概率接近0的状态，即使Q在该点很小或为0，对总散度贡献也很小。

这种特性在实际应用中非常有用。比如在大语言模型微调时，我们常用KL散度作为正则项，防止微调后的模型偏离原始预训练模型太远。具体来说，如果把原始模型分布看作P，微调模型分布看作Q，我们会在损失函数中加入D_KL(P||Q)项，这样可以确保微调模型在保持原始知识的同时学习新任务。

在强化学习中，特别是PPO算法，KL散度用来限制策略更新的幅度。在变分自编码器中，KL散度则用来让潜在变量分布接近标准正态分布。

计算KL散度时需要注意数值稳定性问题，特别是当Q(i)接近0时，log(P(i)/Q(i))会趋向无穷大。实践中通常会添加小常数或使用其他技巧来避免这个问题。

总的来说，KL散度是连接信息论和机器学习的重要桥梁，理解它的性质对于设计和优化各种学习算法都非常有帮助。

