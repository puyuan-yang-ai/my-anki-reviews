我来分别给出两个版本的回答：

**详细书面版本：**
BM25（Best Matching 25）是一个经典的信息检索相关性打分算法，是对传统TF-IDF的改进版本。它的核心思想是通过文档中词项的频率（TF）、反向文档频率（IDF）以及文档长度归一化三个要素来计算文档与查询的相关性得分。

BM25的计算公式如下：

\[
Score(D,Q) = \sum_{i=1}^n IDF(q_i) \cdot \frac{f(q_i,D) \cdot (k_1 + 1)}{f(q_i,D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{avgdl})}
\]

其中：
- \(f(q_i,D)\) 是词项在文档中的频率（TF）
- \(|D|\) 是文档长度
- \(avgdl\) 是平均文档长度
- \(k_1\) 和 \(b\) 是可调参数（通常 k1 = 1.5，b = 0.75）

BM25相比TF-IDF的主要改进在于：
1. 考虑了文档长度归一化
2. 对词频进行了上限控制，避免高频词主导整个得分
3. 引入可调参数，使算法更灵活

**口语化面试版本：**
BM25其实是搜索引擎领域特别经典的一个相关性算法，你可以把它理解成是TF-IDF的升级版。它主要解决了三个问题：

首先，它考虑到了文档长度的影响。比如说，一个词在长文档里出现10次和在短文档里出现10次，显然在短文档里的重要性更大，BM25就能很好地处理这个问题。

其次，它对词频做了一个上限处理。就是说，一个词出现得太多次，分数也不会无限增加，这样可以避免一些常用词霸占了整个排序。

最后，它引入了一些可以调节的参数，让我们能根据具体场景来优化效果。在实际工作中，像Elasticsearch这样的搜索引擎默认就用的是BM25算法，效果都挺不错的。

如果面试官继续追问，我们还可以补充说BM25在现代的RAG系统中经常用来做第一步的检索，因为它计算速度快，而且效果稳定，能够很好地筛选出相关的文档段落，然后再用更复杂的模型去做精确的相关性判断。

**详细书面版本：**
QLoRA (Quantized Low-Rank Adaptation) 是一种高效的大语言模型微调技术，它结合了量化和LoRA两种方法的优势。

核心技术要点：
1. **4位量化基础模型**
   - 使用一种特殊的4位NormalFloat (NF4) 量化格式
   - 在保持模型性能的同时大幅降低内存使用
   - 采用双重量化技术，进一步减少内存占用

2. **分页优化器**
   - 引入了基于分页的优化器
   - 通过在CPU和GPU之间动态移动数据来管理内存
   - 允许在有限显存条件下处理更大的批次大小

3. **LoRA适配器**
   - 仅训练低秩适配矩阵
   - 保持基础模型参数冻结
   - 显著减少可训练参数数量

4. **技术优势**
   - 内存效率：相比传统方法可节省高达80%的内存
   - 训练效率：可在单个GPU上微调大型语言模型
   - 性能保持：几乎不损失模型性能

**口语化面试版本：**
QLoRA其实是一个特别巧妙的模型微调方法，它主要解决了一个很现实的问题：怎么用普通的显卡来训练大模型。

简单来说，它用了两个主要的"省钱"技巧：

第一个是把模型做了4位量化，就是把原来动辄几百GB的模型压缩到很小。这里用了一个特殊的4位格式，叫NF4，它在压缩的同时能很好地保持模型精度。

第二个是用了LoRA技术，就是不去改动整个模型，而是训练一些小的适配器。这有点像是给模型装了个"外挂"，非常省内存和显存。

实际效果特别惊人，比如原来需要好几张A100才能训练的模型，现在用一张RTX 4090就能搞定，而且效果基本没什么损失。

如果面试官追问技术细节，我们可以补充说：
1. QLoRA引入了一个很巧妙的分页优化器，可以在CPU和GPU之间灵活调度数据
2. 它用了双重量化技术，就是对量化后的模型再做一次量化，进一步省内存
3. 在实际应用中，它已经成为了小型团队或个人训练大模型的标准方案之一

这个技术现在在开源社区特别受欢迎，因为它真正让普通开发者也能参与到大模型微调中来，降低了AI民主化的门槛。



**详细书面版本：**
Query改写（Query Rewriting）是RAG系统中的一个关键优化环节，它的目的是将用户的原始查询转换成更适合检索的形式。

主要改写策略包括：

1. **查询扩展（Query Expansion）**
   - 添加同义词
   - 扩充相关概念
   - 增加上下文信息
   
2. **查询分解（Query Decomposition）**
   - 将复杂问题拆分成多个子查询
   - 针对每个子查询单独检索
   - 最后合并检索结果

3. **查询重写方式**
   - 基于规则的改写：使用预定义的模板和规则
   - 基于模型的改写：使用LLM生成更好的检索查询
   - 混合方式：结合规则和模型的优势

改写的必要性：
1. 弥补语义鸿沟：用户查询与文档表达方式可能存在差异
2. 提高召回率：通过扩展查询覆盖更多相关文档
3. 处理复杂查询：将难以直接检索的查询转换为可检索形式
4. 消除歧义：明确查询意图，提高检索精确度

**口语化面试版本：**
Query改写其实就是在RAG系统中，把用户的问题"翻译"成更容易检索的形式。为什么要这么做呢？因为用户问问题的方式和我们存储的文档表达方式经常对不上。

举个简单的例子：
假设用户问"苹果公司最近的财报怎么样？"
我们可能会把它改写成：
"Apple Inc. 2024年第一季度财务报告 营收 利润 业绩"

改写主要有几种方式：
1. 最简单的是加同义词，比如"苹果"改成"Apple"
2. 复杂一点的是用AI模型去理解用户意图，生成更好的检索词
3. 有时候还需要把一个大问题拆成几个小问题分别去查

在实际工作中，我们经常会用LLM来做这个改写，因为它理解语义的能力比较强。比如用GPT来生成更适合检索的查询，或者把一个复杂问题拆解成多个简单查询。

如果面试官继续追问，我们可以补充：
1. 改写质量直接影响最终的检索效果
2. 需要注意改写后的查询不能偏离原始意图太远
3. 在生产环境中，通常会结合规则和模型的方式，既保证效果又控制延迟
4. 改写策略要根据具体的知识库特点来调整，比如专业领域可能需要特殊的术语转换

这个技术点在实际应用中特别重要，因为它往往是提升RAG系统效果的关键环节之一。

