
<html>
<head>
<meta charset="UTF-8">
<title>Export Notes by xxhk.org</title>
<style>

body { font-family: Arial, sans-serif; }
.card { border: 2px solid #000; padding: 10px; border-radius: 10px; margin-bottom: 20px; }
img {
max-width: 100%;
height: auto;
border: 1px solid lightgray;
border-radius: 10px;
display: inline-block;
margin: 10px 0px;
}
.separator { border-top: 1px dashed #000; margin: 10px 0; }
.tags { background-color: lightgray; padding: 5px 10px; margin-bottom: 10px; border-radius: 5px; display: inline-block; }
.footer {
text-align: center;
color: grey;
margin-top: 20px;
padding: 10px;
}

.video-container {
margin: 10px 0;
}
.video-placeholder {
position: relative;
cursor: pointer;
}
.play-button {
position: absolute;
top: 50%;
left: 50%;
transform: translate(-50%, -50%);
width: 68px;
height: 48px;
background-color: rgba(0, 0, 0, 0.7);
border-radius: 14px;
cursor: pointer;
}
.play-button::before {
content: '';
position: absolute;
top: 50%;
left: 55%;
transform: translate(-50%, -50%);
border-style: solid;
border-width: 12px 0 12px 20px;
border-color: transparent transparent transparent white;
}
.play-button:hover {
background-color: red;
}

</style>

</head>
<body>
<div id="cards-container">
<div class="card">
<div class="field_0"><div>使用类比，帮助更好地理解 SFT 模型、RM（Reward Model） 和 PPO（策略模型） ，使用的类比有，学生，老师，教练，考官。其中学生指的是什么？&nbsp;&nbsp;</div><div></div></div>
<div class="separator"></div>
<div class="field_1">学生（策略模型）只是看了<b><span style="color: rgb(255, 0, 127);">大量书本</span></b>（对应于“<span style="color: rgb(255, 0, 127);"><b>预训练阶段的语言模型</b></span>”），还没有学会怎么考试怎么答题。</div>
</div>
<div class="card">
<div class="field_0"><div>SFT 模型，对应的中文名称是什么？以及是在什么数据上进行微调？&nbsp;&nbsp;</div></div>
<div class="separator"></div>
<div class="field_1"><li>“有监督微调模型”或“指令微调模型”。</li><li>经过在高质量指令数据（问答对）上的微调，让预训练语言模型具备基础的指令跟随能力。</li></div>
</div>
<div class="card">
<div class="field_0">在 RL（强化学习）框架中，我们常常会把哪一部分称作策略？&nbsp;</div>
<div class="separator"></div>
<div class="field_1">“可生成动作或回答”的那部分。</div>
</div>
<div class="card">
<div class="field_0"><div>GPT 的训练是 带人类反馈的强化学习。</div><div><div>SFT 模型（有监督微调模型）对应的类比是什么？&nbsp;&nbsp;</div></div><div></div></div>
<div class="separator"></div>
<div class="field_1"><li>类比为“接受老师示范教学后”的学生。</li><li>让模型先具备“基本答题能力”，会按照一定格式或要求进行回答。</li></div>
</div>
<div class="card">
<div class="field_0">PPO 算法做的就是通过梯度上升/下降的方式，让“策略模型”去最大化来自于哪里的什么？&nbsp;</div>
<div class="separator"></div>
<div class="field_1">来自 RM 的奖励。</div>
</div>
<div class="card">
<div class="field_0"><div>RM 模型，对应的中文名称是什么？以及对什么进行评分？&nbsp;</div></div>
<div class="separator"></div>
<div class="field_1">奖励模型，独立训练，用来对回答进行评分（相当于“自动化的人类偏好打分”）。</div>
</div>
<div class="card">
<div class="field_0"><div>对这个策略模型进行 PPO 训练，这个策略模型是如何得到的？&nbsp;&nbsp;</div></div>
<div class="separator"></div>
<div class="field_1">有监督数据上微调后得到SFT 模型，然后再复制这份 SFT 模型的权重到一个新的“策略模型”实例。</div>
</div>
<div class="card">
<div class="field_0">SFT 模型，RM模型 以及 PPO 模型 分别相当于什么？ [类比]</div>
<div class="separator"></div>
<div class="field_1"><li><strong>SFT 模型 = “有监督微调”后的学生</strong>：已经掌握基本回答规范与知识。</li><li><strong>RM（Reward Model）= “考官”</strong>：提供对答案好坏的打分。</li><li><strong>PPO 模型 = “在教练PPO帮助下不断训练”的学生</strong>：在考官打分的反馈下持续改进，得到更高质量、对人类更友好的回答能力。</li></div>
</div>
<div class="card">
<div class="field_0">RM模型 它接收什么？然后输出什么？ 是否生成回答？&nbsp;&nbsp;</div>
<div class="separator"></div>
<div class="field_1">只接收“问题+回答”，然后输出一个分数，不用生成回答。&nbsp;</div>
</div>
<div class="card">
<div class="field_0"><div>使用类比，帮助更好地理解 SFT 模型、RM（Reward Model） 和 PPO（策略模型） 这三者的关系与作用。</div><div></div></div>
<div class="separator"></div>
<div class="field_1"><div><ul><li>学生（策略模型）只是看了大量书本（对应于“预训练阶段的语言模型”），还没有学会怎么考试怎么答题。</li><li>老师（SFT 数据提供者 + SFT 模型训练），老师会给学生提供许多题目和“正确示范答案”，手把手教学生如何回答（这就是“有监督微调 SFT 阶段”）。经过这一步，学生就具备了“基本的回答能力”，知道大致如何组织答案。</li><li>考官（奖励模型，RM）一旦学生回答了问题，考官就会给出一个分数。注意：考官自己并不教学生怎么答，只负责评判和打分。</li><li>教练（PPO 算法）“教练”来指导学生如何在回答过程中不断优化，目标是让回答越来越符合考官的打分标准。也就是 学生做出回答；考官打分；教练根据这个分数，给学生一些反馈和训练（更新模型参数）。</li></ul></div></div>
</div>
<div class="card">
<div class="field_0"><div>ChatGPT在训练过程中，</div><div>进入 PPO 阶段之前，我们会把 哪个模型的权重拷贝到一个新的实例？<br></div></div>
<div class="separator"></div>
<div class="field_1">SFT 模型。</div>
</div>
<div class="card">
<div class="field_0"><div>有监督数据上微调后 得到了SFT模型。&nbsp;&nbsp;复制这份 SFT 模型的权重到一个新的“策略模型”实例；为什么要这么做？以及后续用来干什么？&nbsp;&nbsp;</div></div>
<div class="separator"></div>
<div class="field_1">PPO 算法只是其中一个强化学习，还有多种的强化学习算法可以使用。<br>对这个策略模型进行 <b><span style="color: rgb(255, 0, 127);">PPO 训练</span></b>，最后得到“PPO 模型”。<br><br>一个训练完毕的 SFT 检查点可以<span style="color: rgb(255, 0, 127);"><b>复制到多个实例</b></span>上，每个实例都可以<span style="color: rgb(255, 0, 127);"><b>分别用 PPO</b></span> 或者其他方法进行后续<span style="color: rgb(255, 0, 127);"><b>强化学习或微调</b></span>。</div>
</div>
<div class="card">
<div class="field_0">RM模型 他是如何训练得到的？ 【具体选用什么数据？ 】</div>
<div class="separator"></div>
<div class="field_1">人类偏好数据训练得到， 让人工对不同回答进行排序或打分，拿到大量“哪种回答更好”的对比数据；然后训练一个可以自动输出“好坏打分”的 RM。</div>
</div>
<div class="card">
<div class="field_0"><div>ChatGPT在训练过程中，</div><div>进入 PPO 阶段之前，我们会初始一个新实例。&nbsp; 这个“新实例” 权重跟 SFT 是否完全相同？<br></div></div>
<div class="separator"></div>
<div class="field_1">是的，完全相同，但它要在 PPO 训练中被更新。</div>
</div>
<div class="card">
<div class="field_0">PPO 训练阶段，流程是什么？两个部分。&nbsp;</div>
<div class="separator"></div>
<div class="field_1">PPO 训练时，RM 接收“问题 + 策略模型的回答”，输出一个 reward 分数，然后 PPO 算法根据这个分数来更新“策略模型”的参数。</div>
</div>
<div class="card">
<div class="field_0"><div>GPT 的训练是 带人类反馈的强化学习。</div><div>后来学生通过“教练”的强化训练（哪个算法），并不断从“考官” （模型对应什么缩写）那里获得打分反馈，最后学生变得更擅长给出符合人类偏好的答案（哪个模型）。</div><div></div></div>
<div class="separator"></div>
<div class="field_1">PPO算法，RM模型，PPO&nbsp;模型。</div>
</div>
<div class="card">
<div class="field_0">SFT模型，&nbsp; 的训练方式是使用什么样的数据来进行微调？&nbsp;&nbsp;</div>
<div class="separator"></div>
<div class="field_1">有监督数据（如高质量的指令-回答对）。</div>
</div>
<div class="card">
<div class="field_0"><div>GPT 的训练是 带人类反馈的强化学习。</div><div>使用的类比有哪四个？<br></div><div></div></div>
<div class="separator"></div>
<div class="field_1">学生，老师，教练，考官。&nbsp;</div>
</div>
<div class="card">
<div class="field_0">ChatGPT利用&nbsp;RLHF 在训练过程中涉及到三个模型分别是什么？【中文名称，英文全称和英文简写。 】&nbsp;&nbsp;</div>
<div class="separator"></div>
<div class="field_1">SFT（Supervised Fine-Tuning）指令微调模型、<br>RM（Reward Model）奖励模型、<br>PPO（Policy Model）策略模型。&nbsp;</div>
</div>
<div class="card">
<div class="field_0">在 RL（强化学习）框架中，我们常常会把“可生成动作或回答”的那部分称作什么？</div>
<div class="separator"></div>
<div class="field_1">“策略（Policy）”。</div>
</div>
<div class="card">
<div class="field_0"><div>使用类比，帮助更好地理解 SFT 模型、RM（Reward Model） 和 PPO（策略模型） ，使用的类比有，学生，老师，教练，考官。其中教练 指的是什么？&nbsp;&nbsp;</div><div></div></div>
<div class="separator"></div>
<div class="field_1">教练（PPO 算法）“教练”来<span style="color: rgb(255, 0, 127);"><b>指导学生</b></span>如何在回答过程中不断<span style="color: rgb(255, 0, 127);"><b>优化</b></span>，目标是让回答越来越符合考官的打分标准。也就是 <span style="color: rgb(255, 0, 127);"><b>学生做出回答</b></span>；<span style="color: rgb(255, 0, 127);"><b>考官打分</b></span>；<span style="color: rgb(255, 0, 127);"><b>教练</b></span>根据这个分数，给学生一些<b><span style="color: rgb(255, 0, 127);">反馈和训练</span></b>（更新模型参数）。</div>
</div>
<div class="card">
<div class="field_0"><div>使用类比，帮助更好地理解 SFT 模型、RM（Reward Model） 和 PPO（策略模型） ，使用的类比有，学生，老师，教练，考官。其中老师指的是什么？&nbsp;&nbsp;</div><div></div></div>
<div class="separator"></div>
<div class="field_1">老师（SFT 数据提供者 + SFT 模型训练），老师会给学生提供许多<span style="color: rgb(255, 0, 127);"><b>题目和“正确示范答案</b></span>”，手把手教学生如何回答（这就是“有监督微调 SFT 阶段”）。经过这一步，学生就具备了“<b><span style="color: rgb(255, 0, 127);">基本的回答能力</span></b>”，知道大致如何组织答案。</div>
</div>
<div class="card">
<div class="field_0"><div>&nbsp;PPO 算法，属于什么样的算法？&nbsp;<br></div><div></div></div>
<div class="separator"></div>
<div class="field_1">属于强化学习算法。&nbsp;</div>
</div>
<div class="card">
<div class="field_0"><div>ChatGPT在训练过程中，</div><div>RLHF 的过程是什么？</div></div>
<div class="separator"></div>
<div class="field_1"><ul><li>首先 使用 <b><span style="color: rgb(255, 0, 127);">指令-回答对</span></b>上进行有监督微调，以此得到一个SFT模型。</li><li>随后，训练一个RM奖励模型，该模型能够对模型的回答进行打分。</li><li>接着，将训练好的SFT模型权重复制到一个新的策略模型中。针对这个新的策略模型，借助RM进行打分，之后运用PPO算法进行更新，最终得到PPO模型。</li></ul></div>
</div>
<div class="card">
<div class="field_0"><div>使用类比，帮助更好地理解 SFT 模型、RM（Reward Model） 和 PPO（策略模型） ，使用的类比有，学生，老师，教练，考官。其中考官指的是什么？&nbsp;&nbsp;</div><div></div></div>
<div class="separator"></div>
<div class="field_1">考官（奖励模型，RM）一旦学生回答了问题，考官就会给出一个分数。注意：考官自己并不教学生怎么答，只负责评判和打分。</div>
</div>
<div class="card">
<div class="field_0"><div>GPT 的训练是 带人类反馈的强化学习。</div><div>其中教练这个角色使用哪个算法来协调？具体的协调的过程是什么？&nbsp;&nbsp;</div><div></div></div>
<div class="separator"></div>
<div class="field_1">PPO 算法,<br><li>策略模型，学生做出回答；</li><li>RM模型，考官打分；</li><li>PPO算法，教练根据这个分数，给学生一些反馈和训练（更新模型参数），让学生下次回答时更接近高分答案。</li></div>
</div>
<div class="card">
<div class="field_0"><div>GPT 的训练是 带人类反馈的强化学习。</div><div><div>如果没有 RM 模型，会有什么结果？&nbsp;&nbsp;</div></div><div></div></div>
<div class="separator"></div>
<div class="field_1">RM 相当于考官。 如果没有考官。&nbsp;学生即便学会了基础回答（SFT），也没法进一步提升贴合人类喜好，因为没有“评分标准”，无法知道何谓“更好”。</div>
</div>
<div class="card">
<div class="field_0"><div>GPT 的训练是 带人类反馈的强化学习。</div><div><div>RM（Reward Model）奖励模型 对应的类比是什么？&nbsp;&nbsp;</div></div><div></div>以及作用是什么？</div>
<div class="separator"></div>
<div class="field_1"><li>类比为“考官”，对回答进行评分，但不直接告诉学生该怎么答。</li><li>只提供“人类打分的偏好”信息。</li></div>
</div>
<div class="card">
<div class="field_0"><div>ChatGPT在训练过程中，</div><div>进入 PPO 阶段之前，我们会初始一个新实例。&nbsp; 对这个新实例/策略模型不断与 谁交互，计算什么，用 PPO 等算法怎么样？最终得到什么？</div></div>
<div class="separator"></div>
<div class="field_1">RM（奖励模型），计算奖励，用PPO算法来更新参数，最终得到“PPO 模型”。</div>
</div>
<div class="card">
<div class="field_0"><div>经 PPO 训练的策略模型 ，可以有3种名称，分别是什么？&nbsp;</div></div>
<div class="separator"></div>
<div class="field_1">“PPO 模型” / “RLHF 模型” / “策略模型”。</div>
</div>
<div class="card">
<div class="field_0"><div>GPT 的训练是 带人类反馈的强化学习。</div><div><div>SFT 模型（是什么模型）?</div></div><div></div></div>
<div class="separator"></div>
<div class="field_1">有监督微调模型。</div>
</div>
<div class="card">
<div class="field_0"><div>ChatGPT在训练过程中，</div><div>拷贝哪个模型的权重到一个新的实例，然后这份新的实例被训练，就成了 PPO 模型。</div></div>
<div class="separator"></div>
<div class="field_1">SFT。</div>
</div>
<div class="card">
<div class="field_0"><div>GPT 的训练是 带人类反馈的强化学习。</div><div><div>PPO（策略模型） 对应的类比是什么？&nbsp;&nbsp;</div></div><div></div></div>
<div class="separator"></div>
<div class="field_1">类比为“学生经过教练（PPO 算法）强化训练后”的状态。通过考官的打分（奖励）不断提升，最终学到更符合偏好、更优质的回答模式。</div>
</div>
<div class="card">
<div class="field_0"><div>GPT 的训练是 带人类反馈的强化学习。</div><div><div>为什么叫“PPO 模型”或“RLHF 模型”?</div></div><div></div></div>
<div class="separator"></div>
<div class="field_1">是因为它是用“PPO 算法+RM 打分”的强化学习方式训练而成。</div>
</div>
<div class="card">
<div class="field_0"><div>GPT 的训练是 带人类反馈的强化学习。包括哪三个模型，对应的英文缩写名称？&nbsp;&nbsp;</div><div></div></div>
<div class="separator"></div>
<div class="field_1">SFT 模型、RM（Reward Model） 和 PPO（策略模型） 。</div>
</div>
<div class="card">
<div class="field_0"><div>GPT 的训练是 带人类反馈的强化学习。</div><div><div>如果没有 PPO 算法，会有什么结果？&nbsp;&nbsp;</div></div><div></div></div>
<div class="separator"></div>
<div class="field_1">PPO 算法 相当于教练,是强化学习算法。 如果没有教练， 即便有评分体系，学生也不会使用强化学习的方式去优化回答；可能停留在“模仿老师示范”的水平，无法进一步针对人类喜好做更深层次优化。</div>
</div>
<div class="card">
<div class="field_0">在 RL（强化学习）框架中，<br>“PPO 模型”就是指什么意思？&nbsp;</div>
<div class="separator"></div>
<div class="field_1">“采用 PPO 算法训练过的策略模型”。</div>
</div>
<div class="card">
<div class="field_0">SFT模型 是哪个阶段的起始模型？&nbsp;</div>
<div class="separator"></div>
<div class="field_1">PPO 训练阶段开始的时候。&nbsp;</div>
</div>
<div class="card">
<div class="field_0"><div>GPT 的训练是 带人类反馈的强化学习。</div><div><div>如果没有 SFT 模型，会有什么结果？&nbsp;&nbsp;</div></div><div></div></div>
<div class="separator"></div>
<div class="field_1">SFT 模型 相当于老师教学，<br>没有SFT，那么学生就没学过任何规范回答的示范，直接让他去“考官”那里接受打分，他都不知道如何写出可行的回答，训练可能会乱七八糟、效率低下。<div></div></div>
</div>
<div class="card">
<div class="field_0"><div>PPO 算法阶段训练出来的模型称为什么模型？&nbsp;&nbsp;</div> 【三种】</div>
<div class="separator"></div>
<div class="field_1">“策略模型”或“RLHF 模型” 或者PPO模型.</div>
</div>
<div class="card">
<div class="field_0"><div>GPT 的训练是 带人类反馈的强化学习。</div><div>学生在最初只接受了“老师”的知识（具体哪个阶段），得到一个“初始水平”的回答能力（具体什么模型）？</div><div></div></div>
<div class="separator"></div>
<div class="field_1">SFT阶段，<br>SFT 模型。</div>
</div>
<div class="card">
<div class="field_0"><div>ChatGPT在训练过程中是否有两个SFT模型？ 以及为什么？</div></div>
<div class="separator"></div>
<div class="field_1">不是，同一个模型架构，先跑完 SFT 冻结保存，然后复制这份检查点作为 PPO 的初始策略模型，再在 PPO 阶段被调整。最后产生一个新的策略模型权重”</div>
</div>

<script>
document.addEventListener('click', function(e) {
const container = e.target.closest('.video-container');
if (!container) return;

const placeholder = container.querySelector('.video-placeholder');
if (!placeholder) return;

const videoId = container.dataset.videoId;
const startParam = container.dataset.start || '';

const iframe = document.createElement('iframe');
iframe.width = '100%';
iframe.height = '100%';
iframe.src = `https://www.youtube.com/embed/${videoId}?${startParam.slice(1)}`;
iframe.title = 'YouTube video player';
iframe.frameBorder = '0';
iframe.style.position = 'absolute';
iframe.style.top = '0';
iframe.style.left = '0';
iframe.style.borderRadius = '15px';
iframe.allow = 'accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share';
iframe.allowFullscreen = true;

placeholder.innerHTML = '';
placeholder.appendChild(iframe);
});
</script>

</div>
<div class="footer">Made by <a href="https://xxhk.org">Export Notes - XXHK</a></div>
</body>
</html>

