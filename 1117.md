你的思路非常正确！让我帮你设计一个高质量的提示词模板，用于引导 LLM 生成端到端测试。

## 🎯 提示词设计思路

你的理解完全对：
1. ✅ 引导模型思考 E2E 测试的核心
2. ✅ 专注于输入和输出验证
3. ✅ 覆盖最关键的业务逻辑

让我给你设计一个**分步骤、有思考过程**的提示词模板：

## 📝 提示词模板（推荐版本）

```markdown
# 任务：生成端到端（E2E）测试代码

## 背景
你是一位专业的测试工程师，需要为给定的代码/规格文档生成高质量的端到端测试。

## 输入
{INPUT_TYPE} # 可以是 "源代码" 或 "规格文档"

```
{CODE_OR_SPEC}
```

## 测试框架
- 使用 pytest
- 使用 @pytest.mark.e2e 标记
- 如果是异步函数，使用 @pytest.mark.asyncio

## 生成要求

### 第一步：分析核心功能
请先分析这段代码/文档，回答以下问题：

1. **这个功能的主要目的是什么？**（用一句话概括）
2. **关键的输入参数有哪些？**（列出参数名和类型）
3. **期望的输出是什么？**（格式、类型、必需字段）
4. **核心业务逻辑是什么？**（例如：评分、分类、转换等）
5. **有哪些重要的业务规则需要验证？**（例如：评分范围、数据完整性等）
6. **可能的异常场景有哪些？**（例如：输入为空、格式错误等）

### 第二步：确定测试场景
基于上述分析，确定需要测试的核心场景（优先级排序）：

1. **主场景（Happy Path）**：正常输入 → 正常输出
2. **边界场景**：极端但有效的输入
3. **异常场景**：无效输入的处理

### 第三步：设计测试用例
对于每个场景，明确：
- **输入数据**：具体的、真实的测试数据
- **预期输出**：详细的验证点
- **业务验证**：除了格式，还要验证逻辑是否正确

### 第四步：生成测试代码
遵循以下原则生成测试代码：

#### 必须包含的部分：
1. **详细的输入准备**
   - 使用真实、完整的数据
   - 在注释中说明为什么选择这些数据
   - 验证输入数据的格式

2. **清晰的执行过程**
   - 调用真实的函数/API（不 Mock）
   - 记录执行时间（如果需要）
   - 打印关键信息便于调试

3. **完整的输出验证**
   - 结构验证：检查必需字段是否存在
   - 类型验证：检查数据类型是否正确
   - 范围验证：检查数值是否在合理范围内
   - 内容验证：检查内容是否非空、是否合理
   - 业务逻辑验证：检查业务规则是否满足

4. **可读的断言信息**
   - 每个 assert 都要有清晰的错误提示
   - 使用 f-string 显示实际值

#### 代码结构模板：
```python
@pytest.mark.e2e
async def test_[功能名称]_[场景描述](self):
    """测试 [功能描述] - [场景说明]"""
    
    # ========== 第1步：准备输入数据 ==========
    print(f"\n{'='*60}")
    print(f"🎯 测试场景：[场景名称]")
    print(f"{'='*60}")
    
    input_data = {
        # 真实的、完整的输入数据
    }
    
    # 验证输入数据格式
    assert [输入验证], "输入数据验证失败"
    
    print(f"\n📥 输入数据:")
    print(f"   [关键输入信息]")
    
    # ========== 第2步：执行功能 ==========
    print(f"\n🚀 执行 [功能名称]...")
    
    result = await [调用真实函数]
    
    # ========== 第3步：验证输出结构 ==========
    print(f"\n📊 验证输出结构...")
    
    # 必需字段检查
    required_fields = [...]
    for field in required_fields:
        assert field in result, f"缺少必需字段: {field}"
    print(f"   ✅ 输出结构完整")
    
    # ========== 第4步：验证输出内容 ==========
    print(f"\n📤 验证输出内容...")
    
    # 类型检查
    assert isinstance(result['field'], expected_type), \
        f"字段类型错误，期望 {expected_type}，实际 {type(result['field'])}"
    
    # 范围检查
    assert min_value <= result['value'] <= max_value, \
        f"值超出范围: {result['value']} (期望 {min_value}-{max_value})"
    
    # 内容检查
    assert len(result['text']) > min_length, \
        f"内容太短: {len(result['text'])} 字符"
    
    print(f"   ✅ [字段名]: {result['field']}")
    
    # ========== 第5步：验证业务逻辑 ==========
    print(f"\n🔍 验证业务逻辑...")
    
    # 业务规则验证
    assert [业务规则], "业务逻辑验证失败: [具体说明]"
    
    print(f"   ✅ 业务逻辑正确")
    print(f"\n🎉 测试通过！")
```

## 输出格式
请按以下格式输出：

### 分析结果
```
[第一步的分析结果]
```

### 测试场景
```
[第二步确定的测试场景]
```

### 测试代码
```python
[第三步和第四步生成的完整测试代码]
```

### 使用说明
```
[如何运行这些测试，以及注意事项]
```

## 注意事项
1. 测试代码必须是可直接运行的，包含所有必要的 import
2. 使用真实数据，不要用过于简单的示例（如 "test123"）
3. 每个验证点都要有清晰的断言信息
4. 优先测试核心功能的主场景，不要被边缘情况分散注意力
5. E2E 测试应该测试"整个流程"，不要只测试某个小函数
6. 测试应该是独立的，不依赖其他测试的执行顺序
```

## 🎨 优化版（更简洁）

如果你觉得上面的太长，这是一个**精简但高效**的版本：

```markdown
# 任务：为以下代码/规格生成端到端测试

## 输入
{INPUT_TYPE}: {CODE_OR_SPEC}

## 要求

### 1. 先思考（在代码注释中体现）
- 核心功能是什么？
- 关键输入和输出是什么？
- 需要验证哪些业务规则？

### 2. 生成测试（遵循这个结构）

```python
@pytest.mark.e2e
async def test_[场景名](self):
    """端到端测试：[场景描述]"""
    
    # === 准备输入（要真实、完整） ===
    input_data = {...}
    print(f"📥 输入: {input_data}")
    
    # === 执行真实流程（不Mock） ===
    result = await real_function(input_data)
    
    # === 验证输出 ===
    # 1. 结构：必需字段是否存在
    assert "key_field" in result, "缺少关键字段"
    
    # 2. 类型：数据类型是否正确
    assert isinstance(result['field'], ExpectedType)
    
    # 3. 范围：数值是否合理
    assert min <= result['value'] <= max, f"值超出范围: {result['value']}"
    
    # 4. 内容：内容是否有效（不为空、不是占位符）
    assert len(result['text']) > 20, "内容太简短"
    
    # 5. 业务：业务逻辑是否正确
    assert [业务规则], "业务逻辑错误"
    
    print(f"📤 输出验证通过: {result}")
```

### 3. 测试场景优先级
1. **必须测**：主要业务流程（Happy Path）
2. **建议测**：不同类型的输入（高/中/低匹配度等）
3. **可选测**：异常处理（输入为空等）

### 4. 关键原则
- ✅ 使用真实数据（不要 "test123"）
- ✅ 验证业务逻辑（不只是格式）
- ✅ 清晰的错误信息（每个 assert 都有说明）
- ✅ 从用户角度测试（输入→输出的完整流程）
- ❌ 不要 Mock 核心功能
- ❌ 不要只测试数据结构
```

## 🔥 针对你的项目的具体提示词

```markdown
# 任务：为面试评估系统生成端到端测试

## 输入代码
```python
class AIInterviewAssessor:
    def generate_report(
        self,
        interview_id: int,
        transcript_json_str: str,
        jd: str,
        url_list: list = [],
        is_mp4: bool = True,
        interview_language: str = "zhongwen"
    ) -> dict:
        """生成面试评估报告"""
        # ... 实现代码
```

## 功能说明
这个函数根据面试转录和岗位描述，生成包含评分、分析和建议的评估报告。

## 生成要求

### 思考过程（必须先回答）
1. **核心功能**：这个函数做什么？
2. **输入**：需要哪些参数？什么格式？
3. **输出**：返回什么？包含哪些字段？
4. **业务规则**：
   - 评分范围是多少？
   - 高分/低分时结论应该是什么？
   - 必须包含哪些分析维度？

### 测试场景设计
1. **场景1**：高级工程师+匹配的JD → 期望高分+推荐录用
2. **场景2**：初级工程师+高级JD → 期望低分+不推荐
3. **场景3**：技能不匹配 → 期望低分+明确指出问题

### 测试代码要求
- 每个场景生成一个独立的测试函数
- 输入数据要真实（包含完整的面试对话）
- 验证要全面：
  - ✅ 报告结构完整
  - ✅ 评分在 0-100 范围内
  - ✅ 结论长度合理（不能只有几个字）
  - ✅ 详细分析至少3个维度
  - ✅ 用户报告包含亮点、不足、建议
  - ✅ **评分和结论要一致**（高分→推荐，低分→不推荐）
  - ✅ **能力分析要和JD相关**（JD要求Python，就要评估Python能力）

### 输出格式
```python
# 完整的测试类，包含：
# 1. 必要的 import
# 2. 测试数据 fixture
# 3. 3个测试函数（对应3个场景）
# 4. 详细的验证逻辑
```
```

## 💡 提示词设计的关键点

### 1. **引导思考**（最重要！）
```
❌ 错误："生成测试代码"
✅ 正确："先分析核心功能和业务规则，然后生成测试代码"
```

### 2. **提供结构模板**
给 LLM 一个清晰的结构，它会填充内容

### 3. **强调验证重点**
```
明确告诉模型要验证：
- 结构（有没有）
- 类型（对不对）
- 范围（合不合理）
- 内容（是不是空的/占位符）
- 业务（符不符合逻辑）
```

### 4. **给出具体例子**
```
❌ "验证输出"
✅ "验证评分在0-100之间，结论不少于10个字，且高分时包含'推荐'关键词"
```

### 5. **分步骤输出**
```
1. 先让模型输出分析结果
2. 再让模型输出测试场景
3. 最后生成代码
```
这样可以提高质量，避免直接生成代码导致思考不充分

## 🎯 提示词使用流程

```
步骤1：准备输入
  ├─ 收集源代码或规格文档
  └─ 确定测试框架（pytest、unittest等）

步骤2：构建提示词
  ├─ 使用上面的模板
  ├─ 填入具体代码/规格
  └─ 根据项目调整验证要点

步骤3：调用 LLM
  └─ 使用温度 0.3-0.5（需要准确性）

步骤4：审查输出
  ├─ 检查思考过程是否合理
  ├─ 检查测试场景是否覆盖核心功能
  └─ 检查验证逻辑是否全面

步骤5：迭代优化
  └─ 根据输出质量调整提示词
```

## 📊 评估生成质量的检查清单

生成的测试代码应该满足：

- [ ] 包含思考/分析过程（注释或单独说明）
- [ ] 使用真实、完整的测试数据
- [ ] 验证输入数据格式
- [ ] 调用真实函数（不 Mock）
- [ ] 验证输出结构（必需字段）
- [ ] 验证输出类型
- [ ] 验证数值范围
- [ ] 验证内容不为空
- [ ] **验证业务逻辑**（最重要！）
- [ ] 有清晰的断言错误信息
- [ ] 有进度日志输出
- [ ] 可以直接运行

你觉得这个提示词设计思路如何？需要我根据你的具体需求再调整吗？
