
---

**User**

@virtual_interview.py ä½†æ˜¯å‘¢ï¼Œå®é™…ä¸Šæˆ‘çš„ä»£ç æ˜¯é•¿è¿™æ ·å­çš„ï¼Œä½ å¸®æˆ‘çœ‹ä¸€ä¸‹å®¡æŸ¥ä¸€ä¸‹å‘€ã€‚æˆ‘çš„è²Œä¼¼æ˜¯ï¼Œç”¨æˆ·çš„æ¯ä¸ªå›ç­”éƒ½è¿ç»­è°ƒç”¨äº†ä¸‰æ¬¡å¤§æ¨¡å‹æ¥è¿›è¡Œè¿›è¡Œåˆ¤æ–­ã€‚å¯¹æ­¤ä½ æ€ä¹ˆçœ‹å‘¢ï¼Ÿè¯·é—®æ˜¯è¿™æ ·å§ï¼Ÿç„¶åå¦‚ä½•è·Ÿç°æœ‰çš„è¿™ç§æ–¹æ¡ˆè¿›è¡Œç»“åˆå‘¢ï¼Ÿå› ä¸ºå°±æ¯”å¦‚è¯´ ä½ åˆšæ‰æåˆ°çš„è¿™ä¸ªæ–¹æ¡ˆï¼Œæ¸è¿›å¼çš„ åŠ¨æ€ç”Ÿæˆï¼Œéä¸€æ¬¡æ€§ç”Ÿæˆå›ºå®šæ•°é‡çš„é—®é¢˜ï¼Œç„¶åå¦‚ä½•æŠŠè¿™ä¸ªæ–¹æ¡ˆç»™èå…¥åˆ°æˆ‘çš„è¿™ä¸ªæ¥å£çš„ä¸šåŠ¡é€»è¾‘é‡Œé¢å‘¢ï¼Ÿç„¶åæˆ‘æƒ³é‡‡ç”¨çš„æ˜¯æ–¹æ¡ˆaçš„è®¾è®¡ã€‚ç„¶åæˆ‘ä»¬èƒ½ä¸èƒ½æ¥æ¢è®¨ä¸€ä¸‹è¿™ä¸ªä¿®æ”¹çš„æ–¹å¼å‘€ï¼Ÿè¯·é—®ä½ æ˜ç™½æˆ‘è¿™ä¸ªé€»è¾‘äº†å—ï¼Ÿ èƒ½ä¸èƒ½ç”¨ä½ çš„è¯­è¨€æ¥é‡æ–°å¤è¿°ä¸€ä¸‹è¿™ä¸ªä¿®æ”¹çš„é€»è¾‘/æ–¹æ¡ˆï¼Œå¹¶ä¸”ä½ æœ‰å“ªäº›ç–‘é—®æˆ–è€…ä¸æ¸…æ™°çš„é€»è¾‘ä¹Ÿå¯ä»¥å‘æˆ‘æé—®,ç¡®ä¿æˆ‘ä»¬å¯¹é½äº†è¿™ä¸ªéœ€æ±‚ä»¥åå†å¼€å§‹ä¿®æ”¹ã€‚   

---

**Cursor**

è®©æˆ‘å…ˆä»”ç»†é˜…è¯»ä¸€ä¸‹ä½ çš„ä»£ç ï¼Œç„¶åç†è§£ç°æœ‰é€»è¾‘ã€‚

å¥½çš„ï¼æˆ‘ç°åœ¨å®Œå…¨ç†è§£ä½ çš„ä»£ç é€»è¾‘äº†ã€‚è®©æˆ‘ç”¨æˆ‘çš„è¯­è¨€å¤è¿°ä¸€ä¸‹ï¼Œå¹¶æå‡ºæˆ‘çš„ç–‘é—®ï¼š

## ğŸ“‹ å½“å‰ä»£ç é€»è¾‘åˆ†æ

### ç°æœ‰æµç¨‹ï¼ˆæ¯æ¬¡ç”¨æˆ·å›ç­”åï¼‰ï¼š

```
ç”¨æˆ·å›ç­” 
  â†“
ã€ç¬¬1æ¬¡LLMè°ƒç”¨ - ç­–ç•¥å±‚ã€‘strategy_white_collar.txt
  - æ¨¡å‹ï¼šä¸»æ¨¡å‹ï¼ˆenable_thinkingï¼‰
  - è¾“å…¥ï¼šdialog_history + system_message
  - è¾“å‡ºï¼šcelue_textï¼ˆç­–ç•¥å†³ç­–ï¼‰
  - ä½œç”¨ï¼šåˆ†æå½“å‰é¢è¯•çŠ¶æ€ï¼Œåˆ¶å®šä¸‹ä¸€æ­¥ç­–ç•¥
  â†“
ã€ç¬¬2æ¬¡LLMè°ƒç”¨ - æ‰§è¡Œå±‚ã€‘executor_white_collar.txt
  - æ¨¡å‹ï¼šä¸»æ¨¡å‹ï¼ˆæµå¼è¾“å‡ºï¼‰
  - è¾“å…¥ï¼šdialog_history + celue_textï¼ˆç­–ç•¥ï¼‰
  - è¾“å‡ºï¼šai_responseï¼ˆå®é™…é¢è¯•å®˜å›å¤ï¼Œæµå¼è¿”å›ç»™å‰ç«¯ï¼‰
  - ä½œç”¨ï¼šåŸºäºç­–ç•¥ç”Ÿæˆå…·ä½“çš„é¢è¯•å®˜å›å¤
  â†“
ã€ç¬¬3æ¬¡LLMè°ƒç”¨ - æµç¨‹åˆ¤æ–­ã€‘judge_flow.txt
  - æ¨¡å‹ï¼šqwen-flashï¼ˆè½»é‡å¿«é€Ÿï¼‰
  - è¾“å…¥ï¼šai_response
  - è¾“å‡ºï¼šliucheng_flagï¼ˆ"ç»§ç»­" æˆ– "ç»“æŸ"ï¼‰
  - ä½œç”¨ï¼šåˆ¤æ–­é¢è¯•æ˜¯å¦ç»“æŸ
```

**å…³é”®ç‚¹**ï¼š
- âœ… é‡‡ç”¨äº†**åŒå±‚æ¶æ„**ï¼šç­–ç•¥å±‚ï¼ˆcelueï¼‰+ æ‰§è¡Œå±‚ï¼ˆzhixingï¼‰
- âœ… ç™½é¢†é¢è¯•æ‰æœ‰ç­–ç•¥å±‚ï¼Œè“é¢†ç›´æ¥æ‰§è¡Œ
- âœ… æ¯è½®å¯¹è¯éƒ½æ˜¯**åŠ¨æ€ç”Ÿæˆ**ï¼Œæ²¡æœ‰é¢„è®¾é—®é¢˜åˆ—è¡¨
- âœ… é€šè¿‡ç­–ç•¥å±‚æ¥æŠŠæ§é¢è¯•èŠ‚å¥å’Œç»´åº¦è¦†ç›–

---

## ğŸ¤” æˆ‘çš„ç†è§£å’Œç–‘é—®

### ç†è§£éƒ¨åˆ†ï¼š

ä½ æƒ³è¦é‡‡ç”¨**æ–¹æ¡ˆAï¼ˆä¸€æ¬¡æ€§ç”Ÿæˆå®Œæ•´é¢è¯•è®¡åˆ’ï¼‰**ï¼Œæ„æ€æ˜¯ï¼š

**åœ¨é¢è¯•å¼€å§‹æ—¶ï¼ˆç¬¬ä¸€æ¬¡è°ƒç”¨ï¼‰**ï¼Œç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„é¢è¯•é—®é¢˜åˆ—è¡¨ï¼ˆ10-15ä¸ªé—®é¢˜ï¼‰ï¼ŒåŒ…å«ï¼š
- é—®é¢˜å†…å®¹
- è€ƒå¯Ÿç»´åº¦
- æœŸæœ›å›ç­”è¦ç‚¹
- éš¾åº¦åˆ†çº§

ç„¶ååœ¨åç»­çš„æ¯è½®å¯¹è¯ä¸­ï¼Œå‚è€ƒè¿™ä¸ªè®¡åˆ’æ¥æé—®ï¼Œä½†ä»ç„¶ä¿æŒçµæ´»æ€§ï¼ˆå¯ä»¥è¿½é—®ã€è·³è¿‡ï¼‰ã€‚

---

### ğŸ” æˆ‘çš„ç–‘é—®ï¼š

#### ç–‘é—®1ï¼šé¢è¯•è®¡åˆ’çš„ç”Ÿæˆæ—¶æœº
**Q**: è¿™ä¸ªå®Œæ•´çš„é¢è¯•è®¡åˆ’æ˜¯åœ¨ä»€ä¹ˆæ—¶å€™ç”Ÿæˆçš„ï¼Ÿ
- **é€‰é¡¹A**ï¼šåœ¨ `dialogHistory` ä¸ºç©ºï¼ˆç¬¬ä¸€æ¬¡è°ƒç”¨ï¼‰æ—¶ç”Ÿæˆï¼Œç„¶åå­˜å‚¨åœ¨æŸä¸ªåœ°æ–¹ï¼ˆæ•°æ®åº“ï¼Ÿå†…å­˜ï¼Ÿï¼‰
- **é€‰é¡¹B**ï¼šæ¯æ¬¡éƒ½é‡æ–°ç”Ÿæˆï¼ˆä½†è¿™æ ·å°±å¤±å»äº†"ä¸€æ¬¡æ€§ç”Ÿæˆ"çš„æ„ä¹‰ï¼‰

**æˆ‘çš„å»ºè®®**ï¼šåº”è¯¥æ˜¯é€‰é¡¹Aï¼Œç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶ç”Ÿæˆå¹¶å­˜å‚¨ã€‚

#### ç–‘é—®2ï¼šé¢è¯•è®¡åˆ’çš„å­˜å‚¨ä½ç½®
**Q**: ç”Ÿæˆçš„é¢è¯•è®¡åˆ’å­˜å‚¨åœ¨å“ªé‡Œï¼Ÿ
- å‰ç«¯å­˜å‚¨ï¼Ÿï¼ˆæ¯æ¬¡è¯·æ±‚å¸¦è¿‡æ¥ï¼‰
- åç«¯å†…å­˜ï¼Ÿï¼ˆCallInterviewå®ä¾‹çš„å±æ€§ï¼‰
- æ•°æ®åº“ï¼Ÿï¼ˆæŒä¹…åŒ–å­˜å‚¨ï¼‰

**æˆ‘çš„å»ºè®®**ï¼šå­˜å‚¨åœ¨åç«¯æ•°æ®åº“æˆ–Redisä¸­ï¼Œæ¯æ¬¡è¯·æ±‚æ—¶æ ¹æ® `interview_id` è·å–ã€‚

#### ç–‘é—®3ï¼šä¸ç°æœ‰ç­–ç•¥å±‚çš„å…³ç³»
**Q**: ç”Ÿæˆé¢è¯•è®¡åˆ’åï¼ŒåŸæ¥çš„ `ç­–ç•¥å±‚ï¼ˆcelueï¼‰` è¿˜éœ€è¦å—ï¼Ÿ

**æ–¹æ¡ˆ1**ï¼šå®Œå…¨æ›¿ä»£ç­–ç•¥å±‚
```
é¢è¯•è®¡åˆ’ï¼ˆç¬¬ä¸€æ¬¡ç”Ÿæˆï¼‰
  â†“
ã€æ‰§è¡Œå±‚ã€‘å‚è€ƒè®¡åˆ’ + dialog_history ç”Ÿæˆå›å¤
  â†“
ã€æµç¨‹åˆ¤æ–­ã€‘åˆ¤æ–­æ˜¯å¦ç»“æŸ
```
- ä¼˜ç‚¹ï¼šå‡å°‘ä¸€æ¬¡LLMè°ƒç”¨ï¼Œæé«˜é€Ÿåº¦
- ç¼ºç‚¹ï¼šå¤±å»äº†åŠ¨æ€ç­–ç•¥è°ƒæ•´èƒ½åŠ›

**æ–¹æ¡ˆ2**ï¼šç­–ç•¥å±‚ä»ç„¶ä¿ç•™ï¼Œä½†å‚è€ƒé¢è¯•è®¡åˆ’
```
ã€ç­–ç•¥å±‚ã€‘å‚è€ƒé¢è¯•è®¡åˆ’ + dialog_history â†’ å†³ç­–
  â†“
ã€æ‰§è¡Œå±‚ã€‘åŸºäºç­–ç•¥ç”Ÿæˆå›å¤
  â†“
ã€æµç¨‹åˆ¤æ–­ã€‘åˆ¤æ–­æ˜¯å¦ç»“æŸ
```
- ä¼˜ç‚¹ï¼šä¿æŒçµæ´»æ€§ï¼Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
- ç¼ºç‚¹ï¼šä»ç„¶æ˜¯3æ¬¡è°ƒç”¨

**æˆ‘çš„å»ºè®®**ï¼šé‡‡ç”¨æ–¹æ¡ˆ2ï¼Œå› ä¸ºéœ€æ±‚ä¸­å¼ºè°ƒäº†"è¦åƒèµ„æ·±èŒä¸šé¡¾é—®ä¸€æ ·å¼•å¯¼å’ŒæŒ–æ˜"ï¼Œéœ€è¦ä¿æŒåŠ¨æ€è°ƒæ•´èƒ½åŠ›ã€‚

#### ç–‘é—®4ï¼šé¢è¯•è®¡åˆ’çš„å…·ä½“å†…å®¹
**Q**: é¢è¯•è®¡åˆ’åº”è¯¥åŒ…å«ä»€ä¹ˆå†…å®¹ï¼Ÿ

æˆ‘çš„ç†è§£ï¼š
```json
{
  "interviewId": "xxx",
  "interviewPlan": {
    "totalQuestions": 12,
    "estimatedDuration": "25åˆ†é’Ÿ",
    "dimensionCoverage": {
      "å²—ä½ä¸“ä¸šæŠ€èƒ½è€ƒæ ¸": 4,
      "æ€§æ ¼ç‰¹å¾åˆ†æ": 2,
      "èŒä¸šè§„åˆ’æ¢è®¨": 2,
      "è½¯ç¡¬æ€§èƒ½åŠ›è¯„ä¼°": 3,
      "è–ªé…¬é¢„æœŸåˆç†æ€§åˆ¤æ–­": 1
    },
    "stages": [
      {
        "stage": "ç ´å†°é˜¶æ®µ",
        "questions": [
          {
            "questionId": 1,
            "question": "å¼ ä¸‰ä½ å¥½ï¼çœ‹åˆ°ä½ æœ‰5å¹´Pythonå¼€å‘ç»éªŒ...",
            "dimension": ["èŒä¸šè§„åˆ’æ¢è®¨", "æ€§æ ¼ç‰¹å¾åˆ†æ"],
            "difficulty": "ç®€å•",
            "expectedDuration": "2åˆ†é’Ÿ",
            "mustAsk": true
          }
        ]
      }
    ]
  },
  "currentProgress": {
    "currentQuestionId": 1,
    "askedQuestions": [],
    "coveredDimensions": [],
    "elapsedTime": 0
  }
}
```

---

## ğŸ’¡ æˆ‘çš„ä¿®æ”¹æ–¹æ¡ˆå»ºè®®

### æ•´ä½“æ¶æ„è°ƒæ•´ï¼š

```python
# 1. ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆdialogHistoryä¸ºç©ºï¼‰
if len(dialog_history) == 0:
    # ç”Ÿæˆå®Œæ•´é¢è¯•è®¡åˆ’
    interview_plan = generate_interview_plan(
        user_portrait, jd_title, jd_portrait
    )
    # å­˜å‚¨è®¡åˆ’ï¼ˆæ•°æ®åº“/Redisï¼‰
    save_interview_plan(interview_id, interview_plan)
    
    # è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆå¼€åœºç™½ï¼‰
    first_question = interview_plan['stages'][0]['questions'][0]
    yield first_question
    
# 2. åç»­è°ƒç”¨
else:
    # è·å–é¢è¯•è®¡åˆ’
    interview_plan = get_interview_plan(interview_id)
    
    # ã€ç­–ç•¥å±‚ã€‘å‚è€ƒè®¡åˆ’ + å†å²å¯¹è¯ â†’ å†³ç­–
    celue_message = [
        {"role": "system", "content": celue_sys_prompt},
        {"role": "user", "content": f"""
            å½“å‰é¢è¯•è®°å½•ï¼š{dialog_history}
            é¢è¯•è®¡åˆ’ï¼š{interview_plan}
            å½“å‰è¿›åº¦ï¼šå·²æé—®{len(asked_questions)}/12ä¸ª
            å·²è¦†ç›–ç»´åº¦ï¼š{covered_dimensions}
            æœªè¦†ç›–ç»´åº¦ï¼š{remaining_dimensions}
            
            è¯·å†³ç­–ä¸‹ä¸€æ­¥ï¼š
            1. ç”¨æˆ·å›ç­”è´¨é‡å¦‚ä½•ï¼Ÿ
            2. æ˜¯å¦éœ€è¦è¿½é—®ï¼Ÿ
            3. è¿˜æ˜¯è¿›å…¥è®¡åˆ’ä¸­çš„ä¸‹ä¸€ä¸ªé—®é¢˜ï¼Ÿ
            4. é¢è¯•èŠ‚å¥æ˜¯å¦éœ€è¦è°ƒæ•´ï¼Ÿ
        """}
    ]
    celue_text = call_llm(celue_message)
    
    # ã€æ‰§è¡Œå±‚ã€‘åŸºäºç­–ç•¥ + è®¡åˆ’ç”Ÿæˆå›å¤
    zhixing_message = [
        {"role": "system", "content": zhixing_sys_prompt},
        {"role": "user", "content": f"""
            é¢è¯•è®°å½•ï¼š{dialog_history}
            é¢è¯•è®¡åˆ’ï¼š{interview_plan}
            å½“å‰ç­–ç•¥ï¼š{celue_text}
            
            è¯·ç”Ÿæˆé¢è¯•å®˜çš„ä¸‹ä¸€å¥è¯
        """}
    ]
    ai_response = call_llm_stream(zhixing_message)
    
    # ã€æµç¨‹åˆ¤æ–­ã€‘
    liucheng_flag = judge_flow(ai_response)
```

---

## â“ å‘ä½ ç¡®è®¤çš„é—®é¢˜

1. **é¢è¯•è®¡åˆ’å­˜å‚¨**ï¼šä½ ä»¬åç«¯æœ‰æ•°æ®åº“å—ï¼Ÿè¿˜æ˜¯ç”¨Redisï¼Ÿè¿˜æ˜¯ç®€å•åœ°å­˜åœ¨ `CallInterview` å®ä¾‹çš„å±æ€§ä¸­ï¼ˆä½†è¿™æ ·ä¼šè¯æ–­å¼€å°±ä¸¢å¤±äº†ï¼‰ï¼Ÿ

2. **è®¡åˆ’ç”Ÿæˆçš„è¯¦ç»†ç¨‹åº¦**ï¼šä½ å¸Œæœ›è®¡åˆ’ç”Ÿæˆåˆ°ä»€ä¹ˆç¨‹åº¦ï¼Ÿ
   - åªç”Ÿæˆé—®é¢˜åˆ—è¡¨ï¼Ÿ
   - è¿˜è¦åŒ…å«æ¯ä¸ªé—®é¢˜çš„"æœŸæœ›å›ç­”è¦ç‚¹"ï¼Ÿ
   - è¿˜è¦åŒ…å«"å¦‚ä½•è¿½é—®çš„ç­–ç•¥"ï¼Ÿ

3. **ç­–ç•¥å±‚ä¿ç•™ä¸å¦**ï¼šä½ å€¾å‘äºä¿ç•™åŸæœ‰çš„ç­–ç•¥å±‚ï¼ˆ3æ¬¡è°ƒç”¨ï¼‰ï¼Œè¿˜æ˜¯ç›´æ¥ç”¨é¢è¯•è®¡åˆ’æ›¿ä»£ç­–ç•¥å±‚ï¼ˆ2æ¬¡è°ƒç”¨ï¼‰ï¼Ÿ

4. **é—®é¢˜æ•°é‡å›ºå®šå—**ï¼š12-15ä¸ªé—®é¢˜æ˜¯å›ºå®šçš„ï¼Œè¿˜æ˜¯å¯ä»¥æ ¹æ®å²—ä½ã€å€™é€‰äººæƒ…å†µåŠ¨æ€è°ƒæ•´ï¼Ÿ

5. **è¿½é—®æœºåˆ¶**ï¼šå¦‚æœå€™é€‰äººå›ç­”å¾ˆå¥½éœ€è¦æ·±æŒ–ï¼Œæ˜¯å¦å¯ä»¥åç¦»è®¡åˆ’è¿›è¡Œå¤šè½®è¿½é—®ï¼Ÿè¿˜æ˜¯ä¸¥æ ¼æŒ‰ç…§è®¡åˆ’èµ°ï¼Ÿ

---

**æˆ‘çš„å»ºè®®æ€»ç»“**ï¼š

âœ… **æ¨èæ–¹æ¡ˆ**ï¼š
- ç¬¬ä¸€æ¬¡è°ƒç”¨ç”Ÿæˆå®Œæ•´é¢è¯•è®¡åˆ’ï¼ˆ12-15ä¸ªé—®é¢˜ï¼‰
- è®¡åˆ’å­˜å‚¨åœ¨åç«¯ï¼ˆRedisæˆ–æ•°æ®åº“ï¼‰
- **ä¿ç•™ç­–ç•¥å±‚**ï¼Œä½†è®©å®ƒå‚è€ƒé¢è¯•è®¡åˆ’æ¥å†³ç­–
- æ‰§è¡Œå±‚åŸºäºç­–ç•¥+è®¡åˆ’ç”Ÿæˆå…·ä½“å›å¤
- æµç¨‹åˆ¤æ–­å±‚ä¿æŒä¸å˜

è¿™æ ·æ—¢æœ‰äº†ç»“æ„åŒ–çš„é¢è¯•è®¡åˆ’ï¼ˆç¡®ä¿ç»´åº¦è¦†ç›–ï¼‰ï¼Œåˆä¿æŒäº†çµæ´»æ€§ï¼ˆèµ„æ·±é¢è¯•å®˜çš„åŠ¨æ€è°ƒæ•´ï¼‰ã€‚

ä½ è§‰å¾—è¿™ä¸ªç†è§£å¯¹å—ï¼Ÿè¿˜æœ‰å“ªäº›åœ°æ–¹éœ€è¦æ¾„æ¸…çš„ï¼ŸğŸ˜Š

```plaintext
ç”¨æˆ·å›ç­” 
  â†“
ã€ç¬¬1æ¬¡LLMè°ƒç”¨ - ç­–ç•¥å±‚ã€‘strategy_white_collar.txt
  - æ¨¡å‹ï¼šä¸»æ¨¡å‹ï¼ˆenable_thinkingï¼‰
  - è¾“å…¥ï¼šdialog_history + system_message
  - è¾“å‡ºï¼šcelue_textï¼ˆç­–ç•¥å†³ç­–ï¼‰
  - ä½œç”¨ï¼šåˆ†æå½“å‰é¢è¯•çŠ¶æ€ï¼Œåˆ¶å®šä¸‹ä¸€æ­¥ç­–ç•¥
  â†“
ã€ç¬¬2æ¬¡LLMè°ƒç”¨ - æ‰§è¡Œå±‚ã€‘executor_white_collar.txt
  - æ¨¡å‹ï¼šä¸»æ¨¡å‹ï¼ˆæµå¼è¾“å‡ºï¼‰
  - è¾“å…¥ï¼šdialog_history + celue_textï¼ˆç­–ç•¥ï¼‰
  - è¾“å‡ºï¼šai_responseï¼ˆå®é™…é¢è¯•å®˜å›å¤ï¼Œæµå¼è¿”å›ç»™å‰ç«¯ï¼‰
  - ä½œç”¨ï¼šåŸºäºç­–ç•¥ç”Ÿæˆå…·ä½“çš„é¢è¯•å®˜å›å¤
  â†“
ã€ç¬¬3æ¬¡LLMè°ƒç”¨ - æµç¨‹åˆ¤æ–­ã€‘judge_flow.txt
  - æ¨¡å‹ï¼šqwen-flashï¼ˆè½»é‡å¿«é€Ÿï¼‰
  - è¾“å…¥ï¼šai_response
  - è¾“å‡ºï¼šliucheng_flagï¼ˆ"ç»§ç»­" æˆ– "ç»“æŸ"ï¼‰
  - ä½œç”¨ï¼šåˆ¤æ–­é¢è¯•æ˜¯å¦ç»“æŸ
```

```plaintext
é¢è¯•è®¡åˆ’ï¼ˆç¬¬ä¸€æ¬¡ç”Ÿæˆï¼‰
  â†“
ã€æ‰§è¡Œå±‚ã€‘å‚è€ƒè®¡åˆ’ + dialog_history ç”Ÿæˆå›å¤
  â†“
ã€æµç¨‹åˆ¤æ–­ã€‘åˆ¤æ–­æ˜¯å¦ç»“æŸ
```

```plaintext
ã€ç­–ç•¥å±‚ã€‘å‚è€ƒé¢è¯•è®¡åˆ’ + dialog_history â†’ å†³ç­–
  â†“
ã€æ‰§è¡Œå±‚ã€‘åŸºäºç­–ç•¥ç”Ÿæˆå›å¤
  â†“
ã€æµç¨‹åˆ¤æ–­ã€‘åˆ¤æ–­æ˜¯å¦ç»“æŸ
```

```json
{
  "interviewId": "xxx",
  "interviewPlan": {
    "totalQuestions": 12,
    "estimatedDuration": "25åˆ†é’Ÿ",
    "dimensionCoverage": {
      "å²—ä½ä¸“ä¸šæŠ€èƒ½è€ƒæ ¸": 4,
      "æ€§æ ¼ç‰¹å¾åˆ†æ": 2,
      "èŒä¸šè§„åˆ’æ¢è®¨": 2,
      "è½¯ç¡¬æ€§èƒ½åŠ›è¯„ä¼°": 3,
      "è–ªé…¬é¢„æœŸåˆç†æ€§åˆ¤æ–­": 1
    },
    "stages": [
      {
        "stage": "ç ´å†°é˜¶æ®µ",
        "questions": [
          {
            "questionId": 1,
            "question": "å¼ ä¸‰ä½ å¥½ï¼çœ‹åˆ°ä½ æœ‰5å¹´Pythonå¼€å‘ç»éªŒ...",
            "dimension": ["èŒä¸šè§„åˆ’æ¢è®¨", "æ€§æ ¼ç‰¹å¾åˆ†æ"],
            "difficulty": "ç®€å•",
            "expectedDuration": "2åˆ†é’Ÿ",
            "mustAsk": true
          }
        ]
      }
    ]
  },
  "currentProgress": {
    "currentQuestionId": 1,
    "askedQuestions": [],
    "coveredDimensions": [],
    "elapsedTime": 0
  }
}
```

```python
# 1. ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆdialogHistoryä¸ºç©ºï¼‰
if len(dialog_history) == 0:
    # ç”Ÿæˆå®Œæ•´é¢è¯•è®¡åˆ’
    interview_plan = generate_interview_plan(
        user_portrait, jd_title, jd_portrait
    )
    # å­˜å‚¨è®¡åˆ’ï¼ˆæ•°æ®åº“/Redisï¼‰
    save_interview_plan(interview_id, interview_plan)
    
    # è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆå¼€åœºç™½ï¼‰
    first_question = interview_plan['stages'][0]['questions'][0]
    yield first_question
    
# 2. åç»­è°ƒç”¨
else:
    # è·å–é¢è¯•è®¡åˆ’
    interview_plan = get_interview_plan(interview_id)
    
    # ã€ç­–ç•¥å±‚ã€‘å‚è€ƒè®¡åˆ’ + å†å²å¯¹è¯ â†’ å†³ç­–
    celue_message = [
        {"role": "system", "content": celue_sys_prompt},
        {"role": "user", "content": f"""
            å½“å‰é¢è¯•è®°å½•ï¼š{dialog_history}
            é¢è¯•è®¡åˆ’ï¼š{interview_plan}
            å½“å‰è¿›åº¦ï¼šå·²æé—®{len(asked_questions)}/12ä¸ª
            å·²è¦†ç›–ç»´åº¦ï¼š{covered_dimensions}
            æœªè¦†ç›–ç»´åº¦ï¼š{remaining_dimensions}
            
            è¯·å†³ç­–ä¸‹ä¸€æ­¥ï¼š
            1. ç”¨æˆ·å›ç­”è´¨é‡å¦‚ä½•ï¼Ÿ
            2. æ˜¯å¦éœ€è¦è¿½é—®ï¼Ÿ
            3. è¿˜æ˜¯è¿›å…¥è®¡åˆ’ä¸­çš„ä¸‹ä¸€ä¸ªé—®é¢˜ï¼Ÿ
            4. é¢è¯•èŠ‚å¥æ˜¯å¦éœ€è¦è°ƒæ•´ï¼Ÿ
        """}
    ]
    celue_text = call_llm(celue_message)
    
    # ã€æ‰§è¡Œå±‚ã€‘åŸºäºç­–ç•¥ + è®¡åˆ’ç”Ÿæˆå›å¤
    zhixing_message = [
        {"role": "system", "content": zhixing_sys_prompt},
        {"role": "user", "content": f"""
            é¢è¯•è®°å½•ï¼š{dialog_history}
            é¢è¯•è®¡åˆ’ï¼š{interview_plan}
            å½“å‰ç­–ç•¥ï¼š{celue_text}
            
            è¯·ç”Ÿæˆé¢è¯•å®˜çš„ä¸‹ä¸€å¥è¯
        """}
    ]
    ai_response = call_llm_stream(zhixing_message)
    
    # ã€æµç¨‹åˆ¤æ–­ã€‘
    liucheng_flag = judge_flow(ai_response)
```

---

**User**

å…³äºé¢è¯•è®¡åˆ’çš„å­˜å‚¨ï¼Œæˆ‘æ„Ÿè§‰æˆ‘ä¸çŸ¥é“å­˜å‚¨åœ¨å“ªï¼Œä¸ºä»€ä¹ˆä½ æ¨èå­˜åœ¨Redisé‡Œé¢å‘¢ï¼Ÿå­˜åœ¨Redisé‡Œé¢æˆ‘è¿™è¾¹éœ€è¦æ”¹åŠ¨å¾ˆå¤šå†…å®¹å—ï¼Ÿè¿˜æ˜¯Redisåªæ˜¯ç±»ä¼¼æµè§ˆå™¨çš„ç¼“å­˜å‘€ï¼Œè¿˜æ˜¯è¯´å¾ˆå®¹æ˜“å­˜å‚¨è·Ÿæå–ï¼Œå› ä¸ºæˆ‘å¯¹è¿™å—ä¸æ˜¯ç‰¹åˆ«äº†è§£ã€‚

é¢è¯•è®¡åˆ’ç”Ÿäº§è¿™å—æˆ‘æ„Ÿè§‰åªéœ€è¦åŒ…å«é—®é¢˜åˆ—è¡¨å°±è¡Œäº†ï¼Œä¸éœ€è¦åŒ…å«è¿‡äºè¯¦ç»†çš„å†…å®¹å‘€ã€‚ç„¶åæ¯æ¬¡çš„è¯å°±é€ä¸ªå»è°ƒç”¨è¿™ä¸ªåˆ—è¡¨å°±è¡Œã€‚ç„¶åç­–ç•¥å±‚çš„è¯ï¼Œæˆ‘è§‰å¾—å¯ä»¥ä¿ç•™å§ï¼Œä¿ç•™åŸæœ‰çš„ç­–ç•¥å±‚ï¼Œè¿™æ ·çš„è¯æ˜¯è¿›è¡Œä¸‰æ¬¡çš„è°ƒç”¨ã€‚è¿™æ ·çš„è¯ï¼Œå°±å¯ä»¥ä¿æŒåŠ¨æ€è°ƒæ•´çš„èƒ½åŠ›ã€‚ç„¶åé—®é¢˜æ•°é‡çš„è¯ä¸å›ºå®šå§ï¼Œå°±ååˆ°12ä¸ªå°±å¯ä»¥æ ¹æ®å²—ä½ä»¥åŠå€™é€‰äººçš„æƒ…å†µæ¥åŠ¨æ€è°ƒæ•´ã€‚

æš‚æ—¶ä¸è¦æ¶‰åŠè¿½é—®æœºåˆ¶å§ã€‚

è¿™äº‹ä½ æ€ä¹ˆçœ‹å‘¢ï¼Ÿæ‚¨èƒ½ä¸èƒ½å¸®æˆ‘æ¢³ç†ä¸€ä¸‹æˆ‘ç°åœ¨çš„æ–¹æ¡ˆå‘¢ï¼Ÿå—¯ã€‚å¸®æˆ‘æ¢³ç†æ¸…æ¥šä¸€ä¸‹å¯ä»¥å—ï¼Ÿæˆ‘ç°åœ¨æœ‰ç‚¹æ··ä¹±

---

**Cursor**

å¥½çš„ï¼æˆ‘æ¥å¸®ä½ æ¢³ç†æ¸…æ¥šï¼Œåˆ«æ‹…å¿ƒï¼Œæˆ‘ä»¬ä¸€æ­¥æ­¥ç†æ¸…æ€è·¯ã€‚

## ğŸ“¦ å…³äºRediså­˜å‚¨çš„è§£é‡Š

### Redisæ˜¯ä»€ä¹ˆï¼Ÿ
Rediså°±åƒæ˜¯ä¸€ä¸ª**è¶…å¿«çš„ä¸´æ—¶å­˜å‚¨æŸœ**ï¼Œç‰¹ç‚¹æ˜¯ï¼š
- âœ… **è¯»å†™é€Ÿåº¦æå¿«**ï¼ˆæ¯”æ•°æ®åº“å¿«å‡ åå€ï¼‰
- âœ… **ç®€å•æ˜“ç”¨**ï¼ˆå­˜å–å°±åƒå­—å…¸ä¸€æ ·ï¼š`set(key, value)` å’Œ `get(key)`ï¼‰
- âœ… **é€‚åˆä¸´æ—¶æ•°æ®**ï¼ˆæ¯”å¦‚ä¸€åœºé¢è¯•çš„è®¡åˆ’ï¼Œé¢è¯•ç»“æŸåå¯ä»¥åˆ é™¤ï¼‰
- âœ… **æ”¯æŒè¿‡æœŸæ—¶é—´**ï¼ˆæ¯”å¦‚2å°æ—¶åè‡ªåŠ¨åˆ é™¤ï¼Œé¿å…å ç”¨ç©ºé—´ï¼‰

### ä¸ºä»€ä¹ˆæ¨èRedisï¼Ÿ
```python
# å­˜å‚¨é¢è¯•è®¡åˆ’ï¼ˆè¶…ç®€å•ï¼‰
redis_client.set(f"interview_plan:{interview_id}", json.dumps(plan))

# è¯»å–é¢è¯•è®¡åˆ’
plan_json = redis_client.get(f"interview_plan:{interview_id}")
plan = json.loads(plan_json)
```

**åªéœ€è¦æ”¹åŠ¨çš„åœ°æ–¹**ï¼š
1. å®‰è£…Rediså®¢æˆ·ç«¯ï¼š`pip install redis`
2. åˆå§‹åŒ–Redisè¿æ¥ï¼ˆåœ¨ `__init__` é‡ŒåŠ 2è¡Œä»£ç ï¼‰
3. å­˜å–é¢è¯•è®¡åˆ’ï¼ˆåŠ å‡ è¡Œä»£ç ï¼‰

**ä¸ç”¨Redisä¹Ÿå¯ä»¥**ï¼šä½ å¯ä»¥å…ˆç”¨**å®ä¾‹å±æ€§å­˜å‚¨**ï¼Œæ›´ç®€å•ï¼š
```python
self.interview_plan = None  # å­˜åœ¨å¯¹è±¡é‡Œ
```
ç¼ºç‚¹æ˜¯å¦‚æœæœåŠ¡é‡å¯ï¼Œé¢è¯•è®¡åˆ’å°±ä¸¢å¤±äº†ï¼ˆä½†å¯¹äºæµ‹è¯•é˜¶æ®µå¤Ÿç”¨ï¼‰ã€‚

---

## ğŸ¯ ä½ çš„æ–¹æ¡ˆæ¢³ç†ï¼ˆæœ€ç»ˆç‰ˆï¼‰

æ ¹æ®ä½ çš„åé¦ˆï¼Œæˆ‘å¸®ä½ æ•´ç†å‡ºæ¸…æ™°çš„æ–¹æ¡ˆï¼š

### æ–¹æ¡ˆæ¦‚è¿°
```
ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆå¼€å§‹é¢è¯•ï¼‰:
  ç”Ÿæˆé¢è¯•è®¡åˆ’ï¼ˆ10-12ä¸ªé—®é¢˜åˆ—è¡¨ï¼‰
  â†“
  å­˜å‚¨åœ¨å®ä¾‹å±æ€§ self.interview_plan
  â†“
  è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆå¼€åœºç™½ï¼‰

åç»­è°ƒç”¨ï¼ˆæ¯æ¬¡ç”¨æˆ·å›ç­”ï¼‰:
  ç­–ç•¥å±‚ï¼šåˆ†æ + å‚è€ƒè®¡åˆ’ â†’ å†³ç­–
  â†“
  æ‰§è¡Œå±‚ï¼šåŸºäºç­–ç•¥ â†’ ç”Ÿæˆå›å¤
  â†“
  æµç¨‹åˆ¤æ–­ï¼šåˆ¤æ–­æ˜¯å¦ç»“æŸ
```

### è¯¦ç»†æµç¨‹è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å‰ç«¯å‘èµ·è¯·æ±‚                                â”‚
â”‚  POST /virturlInterview                     â”‚
â”‚  {                                          â”‚
â”‚    "id": "interview_001",                   â”‚
â”‚    "userPortrait": "...",                   â”‚
â”‚    "jdTitle": "é«˜çº§Pythonå·¥ç¨‹å¸ˆ",            â”‚
â”‚    "jdPortrait": "...",                     â”‚
â”‚    "dialogHistory": []  â† åˆ¤æ–­æ˜¯å¦é¦–æ¬¡       â”‚
â”‚  }                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ dialogHistory ä¸ºç©ºï¼Ÿ  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“           â†“
            ã€æ˜¯ã€‘        ã€å¦ã€‘
                â†“           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ é¦–æ¬¡è°ƒç”¨é€»è¾‘     â”‚  â”‚ åç»­è°ƒç”¨é€»è¾‘          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ã€é¦–æ¬¡è°ƒç”¨ã€‘ç”Ÿæˆé¢è¯•è®¡åˆ’                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. è°ƒç”¨LLMç”Ÿæˆé¢è¯•è®¡åˆ’                          â”‚
â”‚    - è¾“å…¥ï¼šuserPortrait, jdTitle, jdPortrait   â”‚
â”‚    - è¾“å‡ºï¼šé—®é¢˜åˆ—è¡¨ï¼ˆ10-12ä¸ªé—®é¢˜ï¼‰              â”‚
â”‚                                                â”‚
â”‚ 2. å­˜å‚¨è®¡åˆ’                                    â”‚
â”‚    self.interview_plan = {                     â”‚
â”‚      "questions": [                            â”‚
â”‚        {"id": 1, "question": "...", "dimension": "..."},â”‚
â”‚        {"id": 2, "question": "...", "dimension": "..."},â”‚
â”‚        ...                                     â”‚
â”‚      ],                                        â”‚
â”‚      "currentQuestionIndex": 0                 â”‚
â”‚    }                                           â”‚
â”‚                                                â”‚
â”‚ 3. è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜                              â”‚
â”‚    yield é¢è¯•è®¡åˆ’çš„ç¬¬ä¸€ä¸ªé—®é¢˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ã€åç»­è°ƒç”¨ã€‘æ‰§è¡Œé¢è¯•                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. è·å–é¢è¯•è®¡åˆ’                                â”‚
â”‚    plan = self.interview_plan                  â”‚
â”‚                                                â”‚
â”‚ 2. ã€ç­–ç•¥å±‚ã€‘LLMè°ƒç”¨1 (celue)                  â”‚
â”‚    è¾“å…¥ï¼š                                      â”‚
â”‚      - dialog_history                         â”‚
â”‚      - interview_planï¼ˆå‚è€ƒï¼‰                  â”‚
â”‚    è¾“å‡ºï¼š                                      â”‚
â”‚      - celue_textï¼ˆç­–ç•¥å†³ç­–ï¼‰                  â”‚
â”‚    å†³ç­–å†…å®¹ï¼š                                  â”‚
â”‚      - ç”¨æˆ·å›ç­”è´¨é‡å¦‚ä½•ï¼Ÿ                       â”‚
â”‚      - æ˜¯å¦ç»§ç»­ä¸‹ä¸€ä¸ªè®¡åˆ’é—®é¢˜ï¼Ÿ                 â”‚
â”‚      - è¿˜æ˜¯éœ€è¦å…¶ä»–å¤„ç†ï¼Ÿ                       â”‚
â”‚                                                â”‚
â”‚ 3. ã€æ‰§è¡Œå±‚ã€‘LLMè°ƒç”¨2 (zhixing) - æµå¼          â”‚
â”‚    è¾“å…¥ï¼š                                      â”‚
â”‚      - dialog_history                         â”‚
â”‚      - celue_textï¼ˆç­–ç•¥ï¼‰                      â”‚
â”‚      - interview_planï¼ˆå‚è€ƒå½“å‰åº”è¯¥é—®ä»€ä¹ˆï¼‰     â”‚
â”‚    è¾“å‡ºï¼š                                      â”‚
â”‚      - ai_responseï¼ˆæµå¼è¿”å›ï¼‰                 â”‚
â”‚                                                â”‚
â”‚ 4. ã€æµç¨‹åˆ¤æ–­ã€‘LLMè°ƒç”¨3 (judge_flow)            â”‚
â”‚    è¾“å…¥ï¼šai_response                           â”‚
â”‚    è¾“å‡ºï¼š"ç»§ç»­" æˆ– "ç»“æŸ"                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ å…·ä½“å®ç°æ–¹æ¡ˆ

### æ ¸å¿ƒæ”¹åŠ¨ç‚¹

#### æ”¹åŠ¨1ï¼šæ–°å¢ç”Ÿæˆé¢è¯•è®¡åˆ’çš„Promptæ–‡ä»¶
**æ–‡ä»¶ä½ç½®**ï¼š`prompts/interview/generate_plan_white_collar.txt`

```
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æ‹›è˜é¢è¯•ä¸“å®¶ï¼Œç°åœ¨éœ€è¦ä¸ºä¸€åœºé¢è¯•åˆ¶å®šå®Œæ•´çš„é—®é¢˜è®¡åˆ’ã€‚

ã€å€™é€‰äººä¿¡æ¯ã€‘
{user_portrait}

ã€ç›®æ ‡å²—ä½ã€‘
èŒä½ï¼š{jd_title}
è¦æ±‚ï¼š{jd_portrait}

ã€ä»»åŠ¡ã€‘
ç”Ÿæˆä¸€ä¸ªåŒ…å«10-12ä¸ªé—®é¢˜çš„é¢è¯•è®¡åˆ’ï¼Œé—®é¢˜è¦è¦†ç›–ä»¥ä¸‹ç»´åº¦ï¼š
1. å²—ä½ä¸“ä¸šæŠ€èƒ½è€ƒæ ¸ï¼ˆ3-4ä¸ªé—®é¢˜ï¼‰
2. æ€§æ ¼ç‰¹å¾åˆ†æï¼ˆé€šè¿‡å…¶ä»–é—®é¢˜è§‚å¯Ÿï¼Œä¸å•ç‹¬æé—®ï¼‰
3. èŒä¸šè§„åˆ’æ¢è®¨ï¼ˆ2ä¸ªé—®é¢˜ï¼‰
4. è½¯ç¡¬æ€§èƒ½åŠ›è¯„ä¼°ï¼ˆ2-3ä¸ªé—®é¢˜ï¼‰
5. è–ªé…¬é¢„æœŸåˆç†æ€§åˆ¤æ–­ï¼ˆ1ä¸ªé—®é¢˜ï¼‰

ã€é¢è¯•é˜¶æ®µè®¾è®¡ã€‘
- ç ´å†°é˜¶æ®µï¼š2-3ä¸ªè½»æ¾çš„å¼€åœºé—®é¢˜
- æ ¸å¿ƒæ¢ç´¢ï¼š6-8ä¸ªæ·±åº¦é—®é¢˜
- æ”¶å°¾é˜¶æ®µï¼š2ä¸ªç¡®è®¤é—®é¢˜

ã€è¦æ±‚ã€‘
1. é—®é¢˜è¦ç»“åˆå€™é€‰äººçš„å®é™…èƒŒæ™¯
2. ä»ç®€å•åˆ°å¤æ‚ï¼Œå¾ªåºæ¸è¿›
3. æ¯ä¸ªé—®é¢˜æ ‡æ³¨ä¸»è¦è€ƒå¯Ÿç»´åº¦
4. é—®é¢˜è¦ç®€çŸ­ã€å£è¯­åŒ–ï¼Œåƒé¢å¯¹é¢èŠå¤©

ã€è¾“å‡ºæ ¼å¼ã€‘
ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•markdownæ ‡è®°ï¼š
{
  "questions": [
    {
      "id": 1,
      "stage": "ç ´å†°é˜¶æ®µ",
      "question": "é—®é¢˜å†…å®¹",
      "dimension": "ä¸»è¦è€ƒå¯Ÿç»´åº¦",
      "difficulty": "ç®€å•/ä¸­ç­‰/å›°éš¾"
    }
  ]
}
```

---

#### æ”¹åŠ¨2ï¼šä¿®æ”¹ `virtual_interview.py`

```python
class CallInterview:
    def __init__(self, interview_id: str):
        # ... ç°æœ‰ä»£ç ä¿æŒä¸å˜ ...
        
        # æ–°å¢ï¼šå­˜å‚¨é¢è¯•è®¡åˆ’
        self.interview_plan = None
        self.current_question_index = 0

    async def generate_interview_plan(self):
        """ç”Ÿæˆé¢è¯•è®¡åˆ’ï¼ˆä»…é¦–æ¬¡è°ƒç”¨ï¼‰"""
        try:
            plan_prompt = load_prompt("generate_plan_white_collar.txt")
            plan_prompt_filled = plan_prompt.format(
                user_portrait=self.user_portrait,
                jd_title=self.jd_title,
                jd_portrait=self.jd_portrait
            )
            
            response = self.celue_client.chat.completions.create(
                model=self.ai_config['api_model'],
                temperature=0.7,
                messages=[
                    {"role": "system", "content": plan_prompt_filled},
                    {"role": "user", "content": "è¯·ç”Ÿæˆé¢è¯•è®¡åˆ’"}
                ]
            )
            
            plan_json = response.choices[0].message.content
            # æ¸…ç†å¯èƒ½çš„markdownæ ‡è®°
            plan_json = plan_json.replace("```json", "").replace("```", "").strip()
            self.interview_plan = json.loads(plan_json)
            
            logging.info(f"é¢è¯•ID: {self.interview_id} - é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±{len(self.interview_plan['questions'])}ä¸ªé—®é¢˜")
            return self.interview_plan
            
        except Exception as e:
            logging.error(f"é¢è¯•ID: {self.interview_id} - ç”Ÿæˆé¢è¯•è®¡åˆ’å¤±è´¥: {e}")
            # å¤±è´¥æ—¶è¿”å›ç©ºè®¡åˆ’ï¼Œèµ°åŸæœ‰é€»è¾‘
            return None

    async def get_ai_response(self, dialog_history):
        start_time = time.time()
        
        # ========== æ–°å¢ï¼šåˆ¤æ–­æ˜¯å¦é¦–æ¬¡è°ƒç”¨ ==========
        if not dialog_history or len(dialog_history) == 0:
            # é¦–æ¬¡è°ƒç”¨ï¼šç”Ÿæˆé¢è¯•è®¡åˆ’
            await self.generate_interview_plan()
            
            # è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆå¼€åœºç™½ï¼‰
            if self.interview_plan and len(self.interview_plan['questions']) > 0:
                first_question = self.interview_plan['questions'][0]['question']
                self.current_question_index = 1  # ä¸‹æ¬¡ä»ç¬¬äºŒä¸ªé—®é¢˜å¼€å§‹
                
                yield f"data: {json.dumps({'type': 'chunk', 'content': first_question}, ensure_ascii=False)}\n\n"
                yield f"data: {json.dumps({'type': 'DONE', 'content': '[DONE]'}, ensure_ascii=False)}\n\n"
                return
        
        # ========== åŸæœ‰ä»£ç ï¼šåç»­è°ƒç”¨é€»è¾‘ ==========
        language, say_hi, self.TTS_model = self.language_dict[self.interview_language]
        system_message = f'''
                        æœ¬æ¬¡é¢è¯•å‚è€ƒèµ„æ–™å¦‚ä¸‹ï¼š
                        #é¢è¯•äº¤æµå…¨ç¨‹ä½¿ç”¨è¯­ç§ï¼š{language}
                        é¢è¯•å¼€å§‹æ—¶é—´ï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}
                        å€™é€‰äººä¿¡æ¯ï¼š{self.user_portrait}
                        å²—ä½ä¿¡æ¯ï¼š{self.jd_title},{self.jd_portrait}
                        å…¬å¸ä¿¡æ¯ï¼š{self.company_portrait}
                        #é¢è¯•äº¤æµå…¨ç¨‹ä½¿ç”¨è¯­ç§ï¼š{language}
                        é¢è¯•æ­£å¼å¼€å§‹ï¼
                                        '''
        self.speech_speed = 1
        
        # åŠ è½½promptæ¨¡æ¿
        celue_template = load_prompt("strategy_white_collar.txt")
        zhixing_template = load_prompt("executor_white_collar.txt")
        lanling_template = load_prompt("executor_blue_collar.txt")
        
        # æ›¿æ¢å˜é‡
        lang = self.language_dict[self.interview_language][0]
        self.celue_sys_prompt = celue_template.format(language=lang)
        self.zhixing_sys_prompt = zhixing_template.format(language=lang)
        self.lanling_sys_prompt = lanling_template.format(language=lang)
        
        try:
            # ========== æ–°å¢ï¼šå‡†å¤‡é¢è¯•è®¡åˆ’ä¿¡æ¯ ==========
            plan_info = ""
            if self.interview_plan:
                remaining_questions = self.interview_plan['questions'][self.current_question_index:]
                plan_info = f"""
                ã€é¢è¯•è®¡åˆ’å‚è€ƒã€‘
                å½“å‰è¿›åº¦ï¼šå·²æé—® {self.current_question_index}/{len(self.interview_plan['questions'])} ä¸ªé—®é¢˜
                å¾…æé—®é¢˜ï¼š{json.dumps(remaining_questions[:3], ensure_ascii=False)}
                ï¼ˆæ³¨æ„ï¼šè¿™åªæ˜¯å‚è€ƒè®¡åˆ’ï¼Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µçµæ´»è°ƒæ•´ï¼‰
                """
            
            # interview_type 1ç™½é¢†ï¼Œ0è“é¢†
            if self.interview_type == 1:
                self.speech_speed = 1.1
                # ========== ä¿®æ”¹ï¼šç­–ç•¥å±‚åŠ å…¥é¢è¯•è®¡åˆ’ ==========
                celue_message = [
                    {"role": "system", "content": self.celue_sys_prompt},
                    {"role": "user", "content": f"""
                        å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}
                        å½“å‰æ—¶é—´ä¸º{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}
                        å…¶ä»–æ¶ˆæ¯ï¼š{system_message}
                        {plan_info}
                    """}
                ]
                
                logging.info(f"-- é¢è¯•ID ï¼š{self.interview_id},å½“å‰ç­–ç•¥promptå¦‚ä¸‹--ï¼š{celue_message}")
                for i in range(2):
                    try:
                        celue = self.celue_client.chat.completions.create(
                            model=self.ai_config['api_model'],
                            temperature=0.4,
                            messages=celue_message,
                            extra_body={"chat_template_kwargs": {"enable_thinking": True}},
                        )
                        celue_text = celue.choices[0].message.content
                        break
                    except Exception as e:
                        logging.error(f"-- é¢è¯•ID : {self.interview_id}---ç­–ç•¥è°ƒç”¨AIå‡ºé”™ï¼Œç¬¬{i + 1}æ¬¡å°è¯•: {e}")
                
                logging.info(f"-- é¢è¯•ID ï¼š{self.interview_id},æ¨ç†æ—¶é—´ï¼š{time.time() - start_time}s,å½“å‰ç­–ç•¥å¦‚ä¸‹--ï¼š\n{celue_text}")
                
                # ========== ä¿®æ”¹ï¼šæ‰§è¡Œå±‚ä¹ŸåŠ å…¥é¢è¯•è®¡åˆ’ ==========
                zhixing_message = [
                    {"role": "system", "content": self.zhixing_sys_prompt},
                    {"role": "user", "content": f"""
                        å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}
                        å½“å‰æ—¶é—´ä¸ºï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}
                        å…¶ä»–æ¶ˆæ¯ï¼š{system_message}
                        å½“å‰å›å¤ç­–ç•¥å¦‚ä¸‹{celue_text}
                        {plan_info}
                    """}
                ]
            else:
                zhixing_message = [
                    {"role": "system", "content": self.lanling_sys_prompt},
                    {"role": "user", "content": f"""
                        å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}
                        å½“å‰æ—¶é—´ä¸º{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}
                        å…¶ä»–æ¶ˆæ¯ï¼š{system_message}
                    """}
                ]
            
            logging.info(f"-- é¢è¯•ID ï¼š{self.interview_id},å½“å‰æ‰§è¡Œpromptå¦‚ä¸‹--ï¼š{zhixing_message}")
            
            # é‡è¯•æ¬¡æ•°
            for i in range(2):
                try:
                    response = await self.AI_client.chat.completions.create(
                        stream=True,
                        model=self.ai_config['api_model'],
                        temperature=0.7,
                        seed=1,
                        messages=zhixing_message
                    )
                    break
                except Exception as e:
                    logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµå¼è°ƒç”¨AIå‡ºé”™ï¼Œç¬¬{i + 1}æ¬¡å°è¯•: {e}")
            
            if not response:
                logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµå¼è°ƒç”¨AIå¤±è´¥")
            
            ai_response = ""
            async for chunk in response:
                if chunk.choices and chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    ai_response += content
                    yield f"data: {json.dumps({'type': 'chunk', 'content': content}, ensure_ascii=False)}\n\n"
            
            yield f"data: {json.dumps({'type': 'DONE', 'content': '[DONE]'}, ensure_ascii=False)}\n\n"
            
            # ========== æ–°å¢ï¼šæ›´æ–°é—®é¢˜ç´¢å¼• ==========
            if self.interview_plan:
                self.current_question_index += 1
            
            end_time = time.time()
            execution_time = end_time - start_time
            logging.info(f"-- é¢è¯•ID : {self.interview_id}---æ‰§è¡Œæ—¶é—´ä¸º: {execution_time} ç§’,AIæµå¼è¿”å›å†…å®¹: {ai_response}")
            
            try:
                flash_time = time.time()
                liucheng_prompt = load_prompt("judge_flow.txt")
                liucheng = self.celue_client.chat.completions.create(
                    model="qwen-flash",
                    temperature=0.4,
                    messages=[
                        {"role": "system", "content": liucheng_prompt},
                        {"role": "user", "content": f"é¢è¯•å®˜å›å¤å†…å®¹å¦‚ä¸‹{ai_response}"}
                    ],
                )
                liucheng_flag = liucheng.choices[0].message.content
                logging.info(f"-- é¢è¯•ID : {self.interview_id}---æµç¨‹åˆ¤æ–­å†…å®¹:  {liucheng_flag}  ,è€—æ—¶{time.time() - flash_time}ç§’")
                
                if "ç»“æŸ" in liucheng_flag:
                    yield f"data: {json.dumps({'type': 'over', 'content': 'over'}, ensure_ascii=False)}\n\n"
            except Exception as e:
                logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµç¨‹åˆ¤æ–­å‡ºé”™: {e}")
                yield f"data: {json.dumps({'type': 'error', 'content': str(e)}, ensure_ascii=False)}\n\n"
        
        except Exception as e:
            logging.error(f"-- é¢è¯•ID : {self.interview_id}---AIæµå¼å“åº”è·å–é”™è¯¯: {e}")
            yield f"data: {json.dumps({'type': 'error', 'content': 'æ¨¡å‹æ£€æµ‹åˆ°éæ³•è¾“å…¥ï¼Œè¯·é‡æ–°å¼€å§‹æœ¬æ¬¡é¢è¯•'+str(e)}, ensure_ascii=False)}\n\n"
```

---

## ğŸ“Š æ–¹æ¡ˆæ€»ç»“

### ä½ çš„æœ€ç»ˆæ–¹æ¡ˆï¼ˆæ¸…æ™°ç‰ˆï¼‰

| é¡¹ç›® | æ–¹æ¡ˆ |
|------|------|
| **è®¡åˆ’ç”Ÿæˆæ—¶æœº** | ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆdialogHistoryä¸ºç©ºæ—¶ï¼‰ |
| **è®¡åˆ’å­˜å‚¨ä½ç½®** | å®ä¾‹å±æ€§ `self.interview_plan` |
| **è®¡åˆ’å†…å®¹** | ç®€å•çš„é—®é¢˜åˆ—è¡¨ï¼ˆ10-12ä¸ªé—®é¢˜ï¼‰ |
| **ç­–ç•¥å±‚** | âœ… ä¿ç•™ï¼ˆå‚è€ƒé¢è¯•è®¡åˆ’è¿›è¡Œå†³ç­–ï¼‰ |
| **LLMè°ƒç”¨æ¬¡æ•°** | é¦–æ¬¡1æ¬¡ï¼Œåç»­æ¯è½®3æ¬¡ |
| **é—®é¢˜æ•°é‡** | 10-12ä¸ªï¼Œå¯åŠ¨æ€è°ƒæ•´ |
| **è¿½é—®æœºåˆ¶** | æš‚ä¸æ¶‰åŠ |

### æ‰§è¡Œæµç¨‹

```
ç”¨æˆ·å¼€å§‹é¢è¯•ï¼ˆdialogHistory=[]ï¼‰
  â†“
ã€ç”Ÿæˆé¢è¯•è®¡åˆ’ã€‘1æ¬¡LLMè°ƒç”¨
  â†“
è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜
  â†“
ç”¨æˆ·å›ç­”...
  â†“
ã€ç­–ç•¥å±‚ã€‘åˆ†æ+å‚è€ƒè®¡åˆ’ â†’ å†³ç­–
  â†“
ã€æ‰§è¡Œå±‚ã€‘åŸºäºç­–ç•¥+è®¡åˆ’ â†’ ç”Ÿæˆå›å¤
  â†“
ã€æµç¨‹åˆ¤æ–­ã€‘åˆ¤æ–­æ˜¯å¦ç»“æŸ
  â†“
ç»§ç»­å¾ªç¯...ç›´åˆ°é¢è¯•ç»“æŸ
```

### ä¼˜åŠ¿
1. âœ… **ç»“æ„åŒ–**ï¼šæœ‰å®Œæ•´çš„é—®é¢˜è§„åˆ’ï¼Œç¡®ä¿ç»´åº¦è¦†ç›–
2. âœ… **çµæ´»æ€§**ï¼šä¿ç•™ç­–ç•¥å±‚ï¼Œå¯ä»¥åŠ¨æ€è°ƒæ•´
3. âœ… **ç®€å•å®ç°**ï¼šç”¨å®ä¾‹å±æ€§å­˜å‚¨ï¼Œæ”¹åŠ¨æœ€å°
4. âœ… **æ¸è¿›å¼**ï¼šé€ä¸ªæé—®ï¼Œä¸æ˜¯ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰é—®é¢˜

---

## ğŸ¯ ä¸‹ä¸€æ­¥

ä½ ç°åœ¨æ¸…æ¥šäº†å—ï¼Ÿå¦‚æœæ¸…æ¥šäº†ï¼Œæˆ‘å¯ä»¥ï¼š

1. **ç”Ÿæˆå®Œæ•´çš„ä»£ç ä¿®æ”¹**ï¼ˆåŒ…æ‹¬æ–°çš„Promptæ–‡ä»¶ï¼‰
2. **æˆ–è€…å…ˆç»™ä½ çœ‹Promptæ–‡ä»¶çš„å†…å®¹**ï¼Œä½ ç¡®è®¤åå†ä¿®æ”¹ä»£ç 

ä½ æƒ³å…ˆçœ‹å“ªä¸ªï¼ŸğŸ˜Š

```python
# å­˜å‚¨é¢è¯•è®¡åˆ’ï¼ˆè¶…ç®€å•ï¼‰
redis_client.set(f"interview_plan:{interview_id}", json.dumps(plan))

# è¯»å–é¢è¯•è®¡åˆ’
plan_json = redis_client.get(f"interview_plan:{interview_id}")
plan = json.loads(plan_json)
```

```python
self.interview_plan = None  # å­˜åœ¨å¯¹è±¡é‡Œ
```

```plaintext
ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆå¼€å§‹é¢è¯•ï¼‰:
  ç”Ÿæˆé¢è¯•è®¡åˆ’ï¼ˆ10-12ä¸ªé—®é¢˜åˆ—è¡¨ï¼‰
  â†“
  å­˜å‚¨åœ¨å®ä¾‹å±æ€§ self.interview_plan
  â†“
  è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆå¼€åœºç™½ï¼‰

åç»­è°ƒç”¨ï¼ˆæ¯æ¬¡ç”¨æˆ·å›ç­”ï¼‰:
  ç­–ç•¥å±‚ï¼šåˆ†æ + å‚è€ƒè®¡åˆ’ â†’ å†³ç­–
  â†“
  æ‰§è¡Œå±‚ï¼šåŸºäºç­–ç•¥ â†’ ç”Ÿæˆå›å¤
  â†“
  æµç¨‹åˆ¤æ–­ï¼šåˆ¤æ–­æ˜¯å¦ç»“æŸ
```

```plaintext
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å‰ç«¯å‘èµ·è¯·æ±‚                                â”‚
â”‚  POST /virturlInterview                     â”‚
â”‚  {                                          â”‚
â”‚    "id": "interview_001",                   â”‚
â”‚    "userPortrait": "...",                   â”‚
â”‚    "jdTitle": "é«˜çº§Pythonå·¥ç¨‹å¸ˆ",            â”‚
â”‚    "jdPortrait": "...",                     â”‚
â”‚    "dialogHistory": []  â† åˆ¤æ–­æ˜¯å¦é¦–æ¬¡       â”‚
â”‚  }                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ dialogHistory ä¸ºç©ºï¼Ÿ  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“           â†“
            ã€æ˜¯ã€‘        ã€å¦ã€‘
                â†“           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ é¦–æ¬¡è°ƒç”¨é€»è¾‘     â”‚  â”‚ åç»­è°ƒç”¨é€»è¾‘          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ã€é¦–æ¬¡è°ƒç”¨ã€‘ç”Ÿæˆé¢è¯•è®¡åˆ’                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. è°ƒç”¨LLMç”Ÿæˆé¢è¯•è®¡åˆ’                          â”‚
â”‚    - è¾“å…¥ï¼šuserPortrait, jdTitle, jdPortrait   â”‚
â”‚    - è¾“å‡ºï¼šé—®é¢˜åˆ—è¡¨ï¼ˆ10-12ä¸ªé—®é¢˜ï¼‰              â”‚
â”‚                                                â”‚
â”‚ 2. å­˜å‚¨è®¡åˆ’                                    â”‚
â”‚    self.interview_plan = {                     â”‚
â”‚      "questions": [                            â”‚
â”‚        {"id": 1, "question": "...", "dimension": "..."},â”‚
â”‚        {"id": 2, "question": "...", "dimension": "..."},â”‚
â”‚        ...                                     â”‚
â”‚      ],                                        â”‚
â”‚      "currentQuestionIndex": 0                 â”‚
â”‚    }                                           â”‚
â”‚                                                â”‚
â”‚ 3. è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜                              â”‚
â”‚    yield é¢è¯•è®¡åˆ’çš„ç¬¬ä¸€ä¸ªé—®é¢˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ã€åç»­è°ƒç”¨ã€‘æ‰§è¡Œé¢è¯•                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. è·å–é¢è¯•è®¡åˆ’                                â”‚
â”‚    plan = self.interview_plan                  â”‚
â”‚                                                â”‚
â”‚ 2. ã€ç­–ç•¥å±‚ã€‘LLMè°ƒç”¨1 (celue)                  â”‚
â”‚    è¾“å…¥ï¼š                                      â”‚
â”‚      - dialog_history                         â”‚
â”‚      - interview_planï¼ˆå‚è€ƒï¼‰                  â”‚
â”‚    è¾“å‡ºï¼š                                      â”‚
â”‚      - celue_textï¼ˆç­–ç•¥å†³ç­–ï¼‰                  â”‚
â”‚    å†³ç­–å†…å®¹ï¼š                                  â”‚
â”‚      - ç”¨æˆ·å›ç­”è´¨é‡å¦‚ä½•ï¼Ÿ                       â”‚
â”‚      - æ˜¯å¦ç»§ç»­ä¸‹ä¸€ä¸ªè®¡åˆ’é—®é¢˜ï¼Ÿ                 â”‚
â”‚      - è¿˜æ˜¯éœ€è¦å…¶ä»–å¤„ç†ï¼Ÿ                       â”‚
â”‚                                                â”‚
â”‚ 3. ã€æ‰§è¡Œå±‚ã€‘LLMè°ƒç”¨2 (zhixing) - æµå¼          â”‚
â”‚    è¾“å…¥ï¼š                                      â”‚
â”‚      - dialog_history                         â”‚
â”‚      - celue_textï¼ˆç­–ç•¥ï¼‰                      â”‚
â”‚      - interview_planï¼ˆå‚è€ƒå½“å‰åº”è¯¥é—®ä»€ä¹ˆï¼‰     â”‚
â”‚    è¾“å‡ºï¼š                                      â”‚
â”‚      - ai_responseï¼ˆæµå¼è¿”å›ï¼‰                 â”‚
â”‚                                                â”‚
â”‚ 4. ã€æµç¨‹åˆ¤æ–­ã€‘LLMè°ƒç”¨3 (judge_flow)            â”‚
â”‚    è¾“å…¥ï¼šai_response                           â”‚
â”‚    è¾“å‡ºï¼š"ç»§ç»­" æˆ– "ç»“æŸ"                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```plaintext
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æ‹›è˜é¢è¯•ä¸“å®¶ï¼Œç°åœ¨éœ€è¦ä¸ºä¸€åœºé¢è¯•åˆ¶å®šå®Œæ•´çš„é—®é¢˜è®¡åˆ’ã€‚

ã€å€™é€‰äººä¿¡æ¯ã€‘
{user_portrait}

ã€ç›®æ ‡å²—ä½ã€‘
èŒä½ï¼š{jd_title}
è¦æ±‚ï¼š{jd_portrait}

ã€ä»»åŠ¡ã€‘
ç”Ÿæˆä¸€ä¸ªåŒ…å«10-12ä¸ªé—®é¢˜çš„é¢è¯•è®¡åˆ’ï¼Œé—®é¢˜è¦è¦†ç›–ä»¥ä¸‹ç»´åº¦ï¼š
1. å²—ä½ä¸“ä¸šæŠ€èƒ½è€ƒæ ¸ï¼ˆ3-4ä¸ªé—®é¢˜ï¼‰
2. æ€§æ ¼ç‰¹å¾åˆ†æï¼ˆé€šè¿‡å…¶ä»–é—®é¢˜è§‚å¯Ÿï¼Œä¸å•ç‹¬æé—®ï¼‰
3. èŒä¸šè§„åˆ’æ¢è®¨ï¼ˆ2ä¸ªé—®é¢˜ï¼‰
4. è½¯ç¡¬æ€§èƒ½åŠ›è¯„ä¼°ï¼ˆ2-3ä¸ªé—®é¢˜ï¼‰
5. è–ªé…¬é¢„æœŸåˆç†æ€§åˆ¤æ–­ï¼ˆ1ä¸ªé—®é¢˜ï¼‰

ã€é¢è¯•é˜¶æ®µè®¾è®¡ã€‘
- ç ´å†°é˜¶æ®µï¼š2-3ä¸ªè½»æ¾çš„å¼€åœºé—®é¢˜
- æ ¸å¿ƒæ¢ç´¢ï¼š6-8ä¸ªæ·±åº¦é—®é¢˜
- æ”¶å°¾é˜¶æ®µï¼š2ä¸ªç¡®è®¤é—®é¢˜

ã€è¦æ±‚ã€‘
1. é—®é¢˜è¦ç»“åˆå€™é€‰äººçš„å®é™…èƒŒæ™¯
2. ä»ç®€å•åˆ°å¤æ‚ï¼Œå¾ªåºæ¸è¿›
3. æ¯ä¸ªé—®é¢˜æ ‡æ³¨ä¸»è¦è€ƒå¯Ÿç»´åº¦
4. é—®é¢˜è¦ç®€çŸ­ã€å£è¯­åŒ–ï¼Œåƒé¢å¯¹é¢èŠå¤©

ã€è¾“å‡ºæ ¼å¼ã€‘
ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•markdownæ ‡è®°ï¼š
{
  "questions": [
    {
      "id": 1,
      "stage": "ç ´å†°é˜¶æ®µ",
      "question": "é—®é¢˜å†…å®¹",
      "dimension": "ä¸»è¦è€ƒå¯Ÿç»´åº¦",
      "difficulty": "ç®€å•/ä¸­ç­‰/å›°éš¾"
    }
  ]
}
```

```python
class CallInterview:
    def __init__(self, interview_id: str):
        # ... ç°æœ‰ä»£ç ä¿æŒä¸å˜ ...
        
        # æ–°å¢ï¼šå­˜å‚¨é¢è¯•è®¡åˆ’
        self.interview_plan = None
        self.current_question_index = 0

    async def generate_interview_plan(self):
        """ç”Ÿæˆé¢è¯•è®¡åˆ’ï¼ˆä»…é¦–æ¬¡è°ƒç”¨ï¼‰"""
        try:
            plan_prompt = load_prompt("generate_plan_white_collar.txt")
            plan_prompt_filled = plan_prompt.format(
                user_portrait=self.user_portrait,
                jd_title=self.jd_title,
                jd_portrait=self.jd_portrait
            )
            
            response = self.celue_client.chat.completions.create(
                model=self.ai_config['api_model'],
                temperature=0.7,
                messages=[
                    {"role": "system", "content": plan_prompt_filled},
                    {"role": "user", "content": "è¯·ç”Ÿæˆé¢è¯•è®¡åˆ’"}
                ]
            )
            
            plan_json = response.choices[0].message.content
            # æ¸…ç†å¯èƒ½çš„markdownæ ‡è®°
            plan_json = plan_json.replace("", "").replace("```", "").strip()
            self.interview_plan = json.loads(plan_json)
            
            logging.info(f"é¢è¯•ID: {self.interview_id} - é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±{len(self.interview_plan['questions'])}ä¸ªé—®é¢˜")
            return self.interview_plan
            
        except Exception as e:
            logging.error(f"é¢è¯•ID: {self.interview_id} - ç”Ÿæˆé¢è¯•è®¡åˆ’å¤±è´¥: {e}")
            # å¤±è´¥æ—¶è¿”å›ç©ºè®¡åˆ’ï¼Œèµ°åŸæœ‰é€»è¾‘
            return None

    async def get_ai_response(self, dialog_history):
        start_time = time.time()
        
        # ========== æ–°å¢ï¼šåˆ¤æ–­æ˜¯å¦é¦–æ¬¡è°ƒç”¨ ==========
        if not dialog_history or len(dialog_history) == 0:
            # é¦–æ¬¡è°ƒç”¨ï¼šç”Ÿæˆé¢è¯•è®¡åˆ’
            await self.generate_interview_plan()
            
            # è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆå¼€åœºç™½ï¼‰
            if self.interview_plan and len(self.interview_plan['questions']) > 0:
                first_question = self.interview_plan['questions'][0]['question']
                self.current_question_index = 1  # ä¸‹æ¬¡ä»ç¬¬äºŒä¸ªé—®é¢˜å¼€å§‹
                
                yield f"data: {json.dumps({'type': 'chunk', 'content': first_question}, ensure_ascii=False)}\n\n"
                yield f"data: {json.dumps({'type': 'DONE', 'content': '[DONE]'}, ensure_ascii=False)}\n\n"
                return
        
        # ========== åŸæœ‰ä»£ç ï¼šåç»­è°ƒç”¨é€»è¾‘ ==========
        language, say_hi, self.TTS_model = self.language_dict[self.interview_language]
        system_message = f'''
                        æœ¬æ¬¡é¢è¯•å‚è€ƒèµ„æ–™å¦‚ä¸‹ï¼š
                        #é¢è¯•äº¤æµå…¨ç¨‹ä½¿ç”¨è¯­ç§ï¼š{language}
                        é¢è¯•å¼€å§‹æ—¶é—´ï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}
                        å€™é€‰äººä¿¡æ¯ï¼š{self.user_portrait}
                        å²—ä½ä¿¡æ¯ï¼š{self.jd_title},{self.jd_portrait}
                        å…¬å¸ä¿¡æ¯ï¼š{self.company_portrait}
                        #é¢è¯•äº¤æµå…¨ç¨‹ä½¿ç”¨è¯­ç§ï¼š{language}
                        é¢è¯•æ­£å¼å¼€å§‹ï¼
                                        '''
        self.speech_speed = 1
        
        # åŠ è½½promptæ¨¡æ¿
        celue_template = load_prompt("strategy_white_collar.txt")
        zhixing_template = load_prompt("executor_white_collar.txt")
        lanling_template = load_prompt("executor_blue_collar.txt")
        
        # æ›¿æ¢å˜é‡
        lang = self.language_dict[self.interview_language][0]
        self.celue_sys_prompt = celue_template.format(language=lang)
        self.zhixing_sys_prompt = zhixing_template.format(language=lang)
        self.lanling_sys_prompt = lanling_template.format(language=lang)
        
        try:
            # ========== æ–°å¢ï¼šå‡†å¤‡é¢è¯•è®¡åˆ’ä¿¡æ¯ ==========
            plan_info = ""
            if self.interview_plan:
                remaining_questions = self.interview_plan['questions'][self.current_question_index:]
                plan_info = f"""
                ã€é¢è¯•è®¡åˆ’å‚è€ƒã€‘
                å½“å‰è¿›åº¦ï¼šå·²æé—® {self.current_question_index}/{len(self.interview_plan['questions'])} ä¸ªé—®é¢˜
                å¾…æé—®é¢˜ï¼š{json.dumps(remaining_questions[:3], ensure_ascii=False)}
                ï¼ˆæ³¨æ„ï¼šè¿™åªæ˜¯å‚è€ƒè®¡åˆ’ï¼Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µçµæ´»è°ƒæ•´ï¼‰
                """
            
            # interview_type 1ç™½é¢†ï¼Œ0è“é¢†
            if self.interview_type == 1:
                self.speech_speed = 1.1
                # ========== ä¿®æ”¹ï¼šç­–ç•¥å±‚åŠ å…¥é¢è¯•è®¡åˆ’ ==========
                celue_message = [
                    {"role": "system", "content": self.celue_sys_prompt},
                    {"role": "user", "content": f"""
                        å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}
                        å½“å‰æ—¶é—´ä¸º{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}
                        å…¶ä»–æ¶ˆæ¯ï¼š{system_message}
                        {plan_info}
                    """}
                ]
                
                logging.info(f"-- é¢è¯•ID ï¼š{self.interview_id},å½“å‰ç­–ç•¥promptå¦‚ä¸‹--ï¼š{celue_message}")
                for i in range(2):
                    try:
                        celue = self.celue_client.chat.completions.create(
                            model=self.ai_config['api_model'],
                            temperature=0.4,
                            messages=celue_message,
                            extra_body={"chat_template_kwargs": {"enable_thinking": True}},
                        )
                        celue_text = celue.choices[0].message.content
                        break
                    except Exception as e:
                        logging.error(f"-- é¢è¯•ID : {self.interview_id}---ç­–ç•¥è°ƒç”¨AIå‡ºé”™ï¼Œç¬¬{i + 1}æ¬¡å°è¯•: {e}")
                
                logging.info(f"-- é¢è¯•ID ï¼š{self.interview_id},æ¨ç†æ—¶é—´ï¼š{time.time() - start_time}s,å½“å‰ç­–ç•¥å¦‚ä¸‹--ï¼š\n{celue_text}")
                
                # ========== ä¿®æ”¹ï¼šæ‰§è¡Œå±‚ä¹ŸåŠ å…¥é¢è¯•è®¡åˆ’ ==========
                zhixing_message = [
                    {"role": "system", "content": self.zhixing_sys_prompt},
                    {"role": "user", "content": f"""
                        å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}
                        å½“å‰æ—¶é—´ä¸ºï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}
                        å…¶ä»–æ¶ˆæ¯ï¼š{system_message}
                        å½“å‰å›å¤ç­–ç•¥å¦‚ä¸‹{celue_text}
                        {plan_info}
                    """}
                ]
            else:
                zhixing_message = [
                    {"role": "system", "content": self.lanling_sys_prompt},
                    {"role": "user", "content": f"""
                        å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}
                        å½“å‰æ—¶é—´ä¸º{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}
                        å…¶ä»–æ¶ˆæ¯ï¼š{system_message}
                    """}
                ]
            
            logging.info(f"-- é¢è¯•ID ï¼š{self.interview_id},å½“å‰æ‰§è¡Œpromptå¦‚ä¸‹--ï¼š{zhixing_message}")
            
            # é‡è¯•æ¬¡æ•°
            for i in range(2):
                try:
                    response = await self.AI_client.chat.completions.create(
                        stream=True,
                        model=self.ai_config['api_model'],
                        temperature=0.7,
                        seed=1,
                        messages=zhixing_message
                    )
                    break
                except Exception as e:
                    logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµå¼è°ƒç”¨AIå‡ºé”™ï¼Œç¬¬{i + 1}æ¬¡å°è¯•: {e}")
            
            if not response:
                logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµå¼è°ƒç”¨AIå¤±è´¥")
            
            ai_response = ""
            async for chunk in response:
                if chunk.choices and chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    ai_response += content
                    yield f"data: {json.dumps({'type': 'chunk', 'content': content}, ensure_ascii=False)}\n\n"
            
            yield f"data: {json.dumps({'type': 'DONE', 'content': '[DONE]'}, ensure_ascii=False)}\n\n"
            
            # ========== æ–°å¢ï¼šæ›´æ–°é—®é¢˜ç´¢å¼• ==========
            if self.interview_plan:
                self.current_question_index += 1
            
            end_time = time.time()
            execution_time = end_time - start_time
            logging.info(f"-- é¢è¯•ID : {self.interview_id}---æ‰§è¡Œæ—¶é—´ä¸º: {execution_time} ç§’,AIæµå¼è¿”å›å†…å®¹: {ai_response}")
            
            try:
                flash_time = time.time()
                liucheng_prompt = load_prompt("judge_flow.txt")
                liucheng = self.celue_client.chat.completions.create(
                    model="qwen-flash",
                    temperature=0.4,
                    messages=[
                        {"role": "system", "content": liucheng_prompt},
                        {"role": "user", "content": f"é¢è¯•å®˜å›å¤å†…å®¹å¦‚ä¸‹{ai_response}"}
                    ],
                )
                liucheng_flag = liucheng.choices[0].message.content
                logging.info(f"-- é¢è¯•ID : {self.interview_id}---æµç¨‹åˆ¤æ–­å†…å®¹:  {liucheng_flag}  ,è€—æ—¶{time.time() - flash_time}ç§’")
                
                if "ç»“æŸ" in liucheng_flag:
                    yield f"data: {json.dumps({'type': 'over', 'content': 'over'}, ensure_ascii=False)}\n\n"
            except Exception as e:
                logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµç¨‹åˆ¤æ–­å‡ºé”™: {e}")
                yield f"data: {json.dumps({'type': 'error', 'content': str(e)}, ensure_ascii=False)}\n\n"
        
        except Exception as e:
            logging.error(f"-- é¢è¯•ID : {self.interview_id}---AIæµå¼å“åº”è·å–é”™è¯¯: {e}")
            yield f"data: {json.dumps({'type': 'error', 'content': 'æ¨¡å‹æ£€æµ‹åˆ°éæ³•è¾“å…¥ï¼Œè¯·é‡æ–°å¼€å§‹æœ¬æ¬¡é¢è¯•'+str(e)}, ensure_ascii=False)}\n\n"---

## ğŸ“Š æ–¹æ¡ˆæ€»ç»“

### ä½ çš„æœ€ç»ˆæ–¹æ¡ˆï¼ˆæ¸…æ™°ç‰ˆï¼‰

| é¡¹ç›® | æ–¹æ¡ˆ |
|------|------|
| **è®¡åˆ’ç”Ÿæˆæ—¶æœº** | ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆdialogHistoryä¸ºç©ºæ—¶ï¼‰ |
| **è®¡åˆ’å­˜å‚¨ä½ç½®** | å®ä¾‹å±æ€§ `self.interview_plan` |
| **è®¡åˆ’å†…å®¹** | ç®€å•çš„é—®é¢˜åˆ—è¡¨ï¼ˆ10-12ä¸ªé—®é¢˜ï¼‰ |
| **ç­–ç•¥å±‚** | âœ… ä¿ç•™ï¼ˆå‚è€ƒé¢è¯•è®¡åˆ’è¿›è¡Œå†³ç­–ï¼‰ |
| **LLMè°ƒç”¨æ¬¡æ•°** | é¦–æ¬¡1æ¬¡ï¼Œåç»­æ¯è½®3æ¬¡ |
| **é—®é¢˜æ•°é‡** | 10-12ä¸ªï¼Œå¯åŠ¨æ€è°ƒæ•´ |
| **è¿½é—®æœºåˆ¶** | æš‚ä¸æ¶‰åŠ |

### æ‰§è¡Œæµç¨‹

```

```plaintext

### ä¼˜åŠ¿
1. âœ… **ç»“æ„åŒ–**ï¼šæœ‰å®Œæ•´çš„é—®é¢˜è§„åˆ’ï¼Œç¡®ä¿ç»´åº¦è¦†ç›–
2. âœ… **çµæ´»æ€§**ï¼šä¿ç•™ç­–ç•¥å±‚ï¼Œå¯ä»¥åŠ¨æ€è°ƒæ•´
3. âœ… **ç®€å•å®ç°**ï¼šç”¨å®ä¾‹å±æ€§å­˜å‚¨ï¼Œæ”¹åŠ¨æœ€å°
4. âœ… **æ¸è¿›å¼**ï¼šé€ä¸ªæé—®ï¼Œä¸æ˜¯ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰é—®é¢˜

---

## ğŸ¯ ä¸‹ä¸€æ­¥

ä½ ç°åœ¨æ¸…æ¥šäº†å—ï¼Ÿå¦‚æœæ¸…æ¥šäº†ï¼Œæˆ‘å¯ä»¥ï¼š

1. **ç”Ÿæˆå®Œæ•´çš„ä»£ç ä¿®æ”¹**ï¼ˆåŒ…æ‹¬æ–°çš„Promptæ–‡ä»¶ï¼‰
2. **æˆ–è€…å…ˆç»™ä½ çœ‹Promptæ–‡ä»¶çš„å†…å®¹**ï¼Œä½ ç¡®è®¤åå†ä¿®æ”¹ä»£ç 

ä½ æƒ³å…ˆçœ‹å“ªä¸ªï¼ŸğŸ˜Š
```

```plaintext

### ä¼˜åŠ¿
1. âœ… **ç»“æ„åŒ–**ï¼šæœ‰å®Œæ•´çš„é—®é¢˜è§„åˆ’ï¼Œç¡®ä¿ç»´åº¦è¦†ç›–
2. âœ… **çµæ´»æ€§**ï¼šä¿ç•™ç­–ç•¥å±‚ï¼Œå¯ä»¥åŠ¨æ€è°ƒæ•´
3. âœ… **ç®€å•å®ç°**ï¼šç”¨å®ä¾‹å±æ€§å­˜å‚¨ï¼Œæ”¹åŠ¨æœ€å°
4. âœ… **æ¸è¿›å¼**ï¼šé€ä¸ªæé—®ï¼Œä¸æ˜¯ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰é—®é¢˜

---

## ğŸ¯ ä¸‹ä¸€æ­¥

ä½ ç°åœ¨æ¸…æ¥šäº†å—ï¼Ÿå¦‚æœæ¸…æ¥šäº†ï¼Œæˆ‘å¯ä»¥ï¼š

1. **ç”Ÿæˆå®Œæ•´çš„ä»£ç ä¿®æ”¹**ï¼ˆåŒ…æ‹¬æ–°çš„Promptæ–‡ä»¶ï¼‰
2. **æˆ–è€…å…ˆç»™ä½ çœ‹Promptæ–‡ä»¶çš„å†…å®¹**ï¼Œä½ ç¡®è®¤åå†ä¿®æ”¹ä»£ç 

ä½ æƒ³å…ˆçœ‹å“ªä¸ªï¼ŸğŸ˜Š
```

---

**User**

ä½ å‰é¢çš„ä¿®æ”¹æ–¹æ¡ˆéå¸¸å¥½ï¼Œè¯·å¸®æˆ‘ä¸¥æ ¼æŒ‰ç…§è¿™ä¸ªæ–¹æ¡ˆæ¥ä¿®æ”¹ç›¸åº”çš„ä»£ç ï¼Œå®Œæ•´followå‰é¢ä¿®æ”¹æ–¹æ¡ˆçš„å†…å®¹ï¼Œä¸ç”¨å¼•å…¥ä¸å¿…è¦çš„é¢å¤–çš„å†—ä½™çš„ä¿®æ”¹ï¼Œä¿®æ”¹ä¹‹åï¼Œå¸®æˆ‘æ¢³ç†ä¸€ä¸‹ä¿®æ”¹åçš„å†…å®¹ï¼Œè¿›è€Œæ–¹ä¾¿æˆ‘double checkæ­£ç¡®æ€§

è¯·é¢å¤–æ³¨æ„ï¼š

1 ä¸¥æ ¼éµå¾ªå‰é¢çš„æ–¹æ¡ˆçš„å…·ä½“è¦æ±‚

2 ä¸æ“…è‡ªæ·»åŠ ä½ è®¤ä¸º"æœ‰ç”¨"ä½†æˆ‘æ²¡æœ‰è¦æ±‚çš„åŠŸèƒ½

3 ä¸“æ³¨äºæˆ‘æ˜ç¡®æŒ‡å‡ºçš„éœ€æ±‚

4 è¯·ä¸è¦å¾€ä»£ç é‡Œé¢æ·»åŠ é¢å¤–çš„mdæ–‡ä»¶æ¥æ€»ç»“ä¿®æ”¹å†…å®¹ã€‚ 

---

**Cursor**

å¥½çš„ï¼æˆ‘ä¼šä¸¥æ ¼æŒ‰ç…§å‰é¢çš„æ–¹æ¡ˆæ¥ä¿®æ”¹ä»£ç ï¼Œä¸æ·»åŠ ä»»ä½•é¢å¤–åŠŸèƒ½ã€‚è®©æˆ‘å¼€å§‹ä¿®æ”¹ã€‚

ç°åœ¨å¼€å§‹ä¿®æ”¹ï¼š

1. é¦–å…ˆåˆ›å»ºæ–°çš„promptæ–‡ä»¶
2. ç„¶åä¿®æ”¹virtual_interview.pyæ–‡ä»¶

```plaintext
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æ‹›è˜é¢è¯•ä¸“å®¶ï¼Œç°åœ¨éœ€è¦ä¸ºä¸€åœºé¢è¯•åˆ¶å®šå®Œæ•´çš„é—®é¢˜è®¡åˆ’ã€‚

ã€å€™é€‰äººä¿¡æ¯ã€‘
{user_portrait}

ã€ç›®æ ‡å²—ä½ã€‘
èŒä½ï¼š{jd_title}
è¦æ±‚ï¼š{jd_portrait}

ã€ä»»åŠ¡ã€‘
ç”Ÿæˆä¸€ä¸ªåŒ…å«10-12ä¸ªé—®é¢˜çš„é¢è¯•è®¡åˆ’ï¼Œé—®é¢˜è¦è¦†ç›–ä»¥ä¸‹ç»´åº¦ï¼š
1. å²—ä½ä¸“ä¸šæŠ€èƒ½è€ƒæ ¸ï¼ˆ3-4ä¸ªé—®é¢˜ï¼‰
2. æ€§æ ¼ç‰¹å¾åˆ†æï¼ˆé€šè¿‡å…¶ä»–é—®é¢˜è§‚å¯Ÿï¼Œä¸å•ç‹¬æé—®ï¼‰
3. èŒä¸šè§„åˆ’æ¢è®¨ï¼ˆ2ä¸ªé—®é¢˜ï¼‰
4. è½¯ç¡¬æ€§èƒ½åŠ›è¯„ä¼°ï¼ˆ2-3ä¸ªé—®é¢˜ï¼‰
5. è–ªé…¬é¢„æœŸåˆç†æ€§åˆ¤æ–­ï¼ˆ1ä¸ªé—®é¢˜ï¼‰

ã€é¢è¯•é˜¶æ®µè®¾è®¡ã€‘
- ç ´å†°é˜¶æ®µï¼š2-3ä¸ªè½»æ¾çš„å¼€åœºé—®é¢˜
- æ ¸å¿ƒæ¢ç´¢ï¼š6-8ä¸ªæ·±åº¦é—®é¢˜
- æ”¶å°¾é˜¶æ®µï¼š2ä¸ªç¡®è®¤é—®é¢˜

ã€è¦æ±‚ã€‘
1. é—®é¢˜è¦ç»“åˆå€™é€‰äººçš„å®é™…èƒŒæ™¯
2. ä»ç®€å•åˆ°å¤æ‚ï¼Œå¾ªåºæ¸è¿›
3. æ¯ä¸ªé—®é¢˜æ ‡æ³¨ä¸»è¦è€ƒå¯Ÿç»´åº¦
4. é—®é¢˜è¦ç®€çŸ­ã€å£è¯­åŒ–ï¼Œåƒé¢å¯¹é¢èŠå¤©

ã€è¾“å‡ºæ ¼å¼ã€‘
ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•markdownæ ‡è®°ï¼š
{
  "questions": [
    {
      "id": 1,
      "stage": "ç ´å†°é˜¶æ®µ",
      "question": "é—®é¢˜å†…å®¹",
      "dimension": "ä¸»è¦è€ƒå¯Ÿç»´åº¦",
      "difficulty": "ç®€å•/ä¸­ç­‰/å›°éš¾"
    }
  ]
}

```

```python
import asyncio
import json
import logging
import time
import re
import regex
from pathlib import Path
from openai import OpenAI, AsyncOpenAI
import dashscope
import dashscope.audio.tts_v2
from app.config import ai_config
from sdk.tts.punctuation_based_tts import PunctuationBasedTTS


def load_prompt(filename: str) -> str:
    """
    åŠ è½½promptæ–‡ä»¶
    
    Args:
        filename: æ–‡ä»¶åï¼Œå¦‚ "strategy_white_collar.txt"
    
    Returns:
        promptå†…å®¹
    """
    current_dir = Path(__file__).parent
    prompt_path = current_dir.parent.parent / "prompts" / "interview" / filename
    with open(prompt_path, 'r', encoding='utf-8') as f:
        return f.read()



dashscope.api_key = ai_config['api_key']
class CallInterview:
    def __init__(self,  interview_id: str):

        self.interview_id = interview_id
        self.ai_config = ai_config
        self.request_id = None
        self.current_question = None
        self.timeout_task = None
        self.voice_active = False
        self.jd_title = None
        self.jd_portrait = None
        self.user_portrait = None
        self.company_portrait = None
        self.company_name = None
        self.interview_type = 1
        self.speech_speed = 1.2
        self.interview_language = "zhongwen"
        self.interview_style = None
        self.TTS_model = 'longanxuan'
        self.interview_plan = None
        self.current_question_index = 0
        self.language_dict = {
            'zhongwen': ('ä¸­æ–‡', 'ä½ å¥½ï¼Œé¢è¯•å®˜ï¼è®©æˆ‘ä»¬ç”¨ä¸­æ–‡äº¤æµï¼', 'longanxuan'),
            'yingwen': ('è‹±æ–‡', "Hello, interviewer!Let's communicate in English!", 'longxiaochun_v2'),
            'riyu': ('æ—¥è¯­', 'ã“ã‚“ã«ã¡ã¯ã€é¢æ¥å®˜ï¼æ—¥æœ¬èªã§è©±ã—åˆã„ã¾ã—ã‚‡ã†ï¼', 'loongtomoka_v2'),
            'hanyu': ('éŸ©è¯­', 'ì•ˆë…•í•˜ì„¸ìš”, ë©´ì ‘ê´€ë‹˜!í•œêµ­ì–´ë¡œ ëŒ€í™”í•´ìš”!', 'loongkyong_v2'),
            'xibanyayu': ('è¥¿ç­ç‰™è¯­', 'Â¡Hola, entrevistador!Â¡Hablemos en espaÃ±ol!', 'sambert-camila-v1'),
            'yinniyu': ('å°å°¼è¯­', 'Halo, pewawancara!Ayo berkomunikasi dalam bahasa Indonesia!', 'sambert-indah-v1'),
            'fayu': ('æ³•è¯­', 'Bonjour, intervieweur !Parlons en franÃ§ais !', 'sambert-clara-v1'),
            'deyu': ('å¾·è¯­', 'Hallo, Interviewer!Lass uns auf Deutsch sprechen!', 'sambert-hanna-v1'),
            'taiyu': ('æ³°è¯­', 'à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¹ˆà¸°/à¸„à¸£à¸±à¸š, à¸œà¸¹à¹‰à¸ªà¸±à¸¡à¸ à¸²à¸©à¸“à¹Œ!à¸¡à¸²à¸à¸¹à¸”à¸„à¸¸à¸¢à¸à¸±à¸™à¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸à¸±à¸™à¹€à¸–à¸­à¸°!', 'sambert-waan-v1')
        }




        # åˆ›å»º OpenAI å®¢æˆ·ç«¯å¯¹è±¡
        self.celue_client = OpenAI(
            api_key=self.ai_config['api_key'],
            base_url=self.ai_config['api_uri'],
            timeout=60.0,
        )
        self.AI_client = AsyncOpenAI(
            api_key=self.ai_config['api_key'],
            base_url=self.ai_config['api_uri'],
            timeout=30.0,
        )
        # 1. è·å–å½“å‰æ­£åœ¨è¿è¡Œçš„äº‹ä»¶å¾ªç¯
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        class FutureCallback(dashscope.audio.tts_v2.ResultCallback):
            def __init__(self, future: asyncio.Future):
                self._future = future
                self._audio_buffer = bytearray()  # ç”¨äºæ‹¼æ¥éŸ³é¢‘ç‰‡æ®µ

            def on_open(self):
                pass  # è¿™é‡Œå¯ä»¥åŠ æ—¥å¿—

            def on_complete(self):
                # è¯­éŸ³åˆæˆå®Œæˆï¼Œæ‰€æœ‰ç‰‡æ®µéƒ½å·²æ”¶åˆ°
                # å°†æ‹¼æ¥å¥½çš„å®Œæ•´éŸ³é¢‘æ•°æ®ä½œä¸ºFutureçš„ç»“æœ
                if not self._future.done():
                    self._future.set_result(bytes(self._audio_buffer))

            def on_error(self, message: str):
                # å‘ç”Ÿé”™è¯¯ï¼Œå°†å¼‚å¸¸ä½œä¸ºFutureçš„ç»“æœ
                if not self._future.done():
                    self._future.set_exception(Exception(f"TTS Error: {message}"))

            def on_close(self):
                # è¿æ¥å…³é—­ï¼Œå¦‚æœFutureè¿˜æœªå®Œæˆï¼ˆå¯èƒ½åœ¨on_completeå‰å…³é—­ï¼‰ï¼Œä¹Ÿè®¾ç½®ç»“æœ
                if not self._future.done():
                    self._future.set_result(bytes(self._audio_buffer))

            def on_data(self, data: bytes):
                # æ”¶åˆ°éŸ³é¢‘ç‰‡æ®µï¼Œå°†å…¶æ‹¼æ¥åˆ°ç¼“å†²åŒº
                if data:
                    self._audio_buffer.extend(data)

        # 3. å°†æ–°çš„Callbackç±»ç»‘å®šåˆ°selfï¼Œä»¥ä¾¿synthesis_ttsæ–¹æ³•ä½¿ç”¨
        self.FutureCallback = FutureCallback




    async def get_ai_response(self, dialog_history):
        start_time = time.time()

        language, say_hi, self.TTS_model = self.language_dict[self.interview_language]
        system_message = f'''
                        æœ¬æ¬¡é¢è¯•å‚è€ƒèµ„æ–™å¦‚ä¸‹ï¼š
                        #é¢è¯•äº¤æµå…¨ç¨‹ä½¿ç”¨è¯­ç§ï¼š{language}
                        é¢è¯•å¼€å§‹æ—¶é—´ï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}
                        å€™é€‰äººä¿¡æ¯ï¼š{self.user_portrait}
                        å²—ä½ä¿¡æ¯ï¼š{self.jd_title},{self.jd_portrait}
                        å…¬å¸ä¿¡æ¯ï¼š{self.company_portrait}
                        #é¢è¯•äº¤æµå…¨ç¨‹ä½¿ç”¨è¯­ç§ï¼š{language}
                        é¢è¯•æ­£å¼å¼€å§‹ï¼
                                        '''

        self.speech_speed = 1
        
        # åŠ è½½promptæ¨¡æ¿
        celue_template = load_prompt("strategy_white_collar.txt")
        zhixing_template = load_prompt("executor_white_collar.txt")
        lanling_template = load_prompt("executor_blue_collar.txt")
        
        # æ›¿æ¢å˜é‡
        lang = self.language_dict[self.interview_language][0]
        self.celue_sys_prompt = celue_template.format(language=lang)
        self.zhixing_sys_prompt = zhixing_template.format(language=lang)
        self.lanling_sys_prompt = lanling_template.format(language=lang)
        try:
            #interview_type 1ç™½é¢†ï¼Œ0è“é¢†
            if self.interview_type == 1:
                self.speech_speed = 1.1
                celue_message = [{"role": "system", "content": self.celue_sys_prompt},
                                 {"role": "user",
                                  "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}ï¼Œå½“å‰æ—¶é—´ä¸º{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}"}]
                logging.info(f"-- é¢è¯•ID ï¼š{self.interview_id},å½“å‰ç­–ç•¥promptå¦‚ä¸‹--ï¼š{celue_message}")
                for i in range(2):
                    try:
                        celue = self.celue_client.chat.completions.create(
                            model=self.ai_config['api_model'],
                            temperature=0.4,
                            messages=celue_message,
                            extra_body={"chat_template_kwargs": {"enable_thinking": True}},
                        )
                        celue_text = celue.choices[0].message.content
                        break
                    except Exception as e:
                        logging.error(f"-- é¢è¯•ID : {self.interview_id}---ç­–ç•¥è°ƒç”¨AIå‡ºé”™ï¼Œç¬¬{i + 1}æ¬¡å°è¯•: {e}")
                logging.info(
                    f"-- é¢è¯•ID ï¼š{self.interview_id},æ¨ç†æ—¶é—´ï¼š{time.time() - start_time}s,å½“å‰ç­–ç•¥å¦‚ä¸‹--ï¼š\n{celue_text}")
                # å¯ç”¨æµå¼ä¼ è¾“
                zhixing_message = [{"role": "system", "content": self.zhixing_sys_prompt},
                                   {"role": "user",
                                    "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history},å½“å‰æ—¶é—´ä¸ºï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}ï¼Œå½“å‰å›å¤ç­–ç•¥å¦‚ä¸‹{celue_text}"}]
            else:
                zhixing_message = [{"role": "system", "content": self.lanling_sys_prompt},
                                   {"role": "user",
                                    "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}ï¼Œå½“å‰æ—¶é—´ä¸º{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}"}]
            logging.info(f"-- é¢è¯•ID ï¼š{self.interview_id},å½“å‰æ‰§è¡Œpromptå¦‚ä¸‹--ï¼š{zhixing_message}")
            #é‡è¯•æ¬¡æ•°
            for i in range(2):
                try:
                    response = await self.AI_client.chat.completions.create(
                        stream=True,  # å¯ç”¨æµå¼
                        model=self.ai_config['api_model'],
                        temperature=0.7,
                        seed=1,
                        messages=zhixing_message
                    )
                    break
                except Exception as e:
                    logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµå¼è°ƒç”¨AIå‡ºé”™ï¼Œç¬¬{i + 1}æ¬¡å°è¯•: {e}")
            if not response:
                logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµå¼è°ƒç”¨AIå¤±è´¥")
            ai_response = ""
            async for chunk in response:
                if chunk.choices and chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    ai_response += content

                    yield f"data: {json.dumps({'type': 'chunk', 'content': content}, ensure_ascii=False)}\n\n"

            yield f"data: {json.dumps({'type': 'DONE', 'content': "[DONE]"}, ensure_ascii=False)}\n\n"




            end_time = time.time()
            execution_time = end_time - start_time
            logging.info(f"-- é¢è¯•ID : {self.interview_id}---æ‰§è¡Œæ—¶é—´ä¸º: {execution_time} ç§’,AIæµå¼è¿”å›å†…å®¹: {ai_response}")
            try:
                flash_time = time.time()
                #åˆ¤æ–­é¢è¯•æ˜¯å¦ç»“æŸï¼Œæ¯ä¸ªå¾ªç¯éƒ½æœ‰
                liucheng_prompt = load_prompt("judge_flow.txt")
                liucheng = self.celue_client.chat.completions.create(
                    model="qwen-flash",
                    temperature=0.4,
                    messages=[{"role": "system", "content": liucheng_prompt},
                              {"role": "user", "content": f"é¢è¯•å®˜å›å¤å†…å®¹å¦‚ä¸‹{ai_response}"}],
                )
                liucheng_flag = liucheng.choices[0].message.content
                logging.info(
                    f"-- é¢è¯•ID : {self.interview_id}---æµç¨‹åˆ¤æ–­å†…å®¹:  {liucheng_flag}  ,è€—æ—¶{time.time() - flash_time}ç§’")
                if "ç»“æŸ" in liucheng_flag:
                    yield f"data: {json.dumps({'type': 'over', 'content': "over"}, ensure_ascii=False)}\n\n"
            except Exception as e:
                logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµç¨‹åˆ¤æ–­å‡ºé”™: {e}")
                yield f"data: {json.dumps({'type': 'error', 'content': str(e)}, ensure_ascii=False)}\n\n"

        except Exception as e:
            logging.error(f"-- é¢è¯•ID : {self.interview_id}---AIæµå¼å“åº”è·å–é”™è¯¯: {e}")
            yield f"data: {json.dumps({'type': 'error', 'content': "æ¨¡å‹æ£€æµ‹åˆ°éæ³•è¾“å…¥ï¼Œè¯·é‡æ–°å¼€å§‹æœ¬æ¬¡é¢è¯•"+str(e)}, ensure_ascii=False)}\n\n"






```

```python
import asyncio
import json
import logging
import time
import re
import regex
from pathlib import Path
from openai import OpenAI, AsyncOpenAI
import dashscope
import dashscope.audio.tts_v2
from app.config import ai_config
from sdk.tts.punctuation_based_tts import PunctuationBasedTTS


def load_prompt(filename: str) -> str:
    """
    åŠ è½½promptæ–‡ä»¶
    
    Args:
        filename: æ–‡ä»¶åï¼Œå¦‚ "strategy_white_collar.txt"
    
    Returns:
        promptå†…å®¹
    """
    current_dir = Path(__file__).parent
    prompt_path = current_dir.parent.parent / "prompts" / "interview" / filename
    with open(prompt_path, 'r', encoding='utf-8') as f:
        return f.read()



dashscope.api_key = ai_config['api_key']
class CallInterview:
    def __init__(self,  interview_id: str):

        self.interview_id = interview_id
        self.ai_config = ai_config
        self.request_id = None
        self.current_question = None
        self.timeout_task = None
        self.voice_active = False
        self.jd_title = None
        self.jd_portrait = None
        self.user_portrait = None
        self.company_portrait = None
        self.company_name = None
        self.interview_type = 1
        self.speech_speed = 1.2
        self.interview_language = "zhongwen"
        self.interview_style = None
        self.TTS_model = 'longanxuan'
        self.interview_plan = None
        self.current_question_index = 0
        self.language_dict = {
            'zhongwen': ('ä¸­æ–‡', 'ä½ å¥½ï¼Œé¢è¯•å®˜ï¼è®©æˆ‘ä»¬ç”¨ä¸­æ–‡äº¤æµï¼', 'longanxuan'),
            'yingwen': ('è‹±æ–‡', "Hello, interviewer!Let's communicate in English!", 'longxiaochun_v2'),
            'riyu': ('æ—¥è¯­', 'ã“ã‚“ã«ã¡ã¯ã€é¢æ¥å®˜ï¼æ—¥æœ¬èªã§è©±ã—åˆã„ã¾ã—ã‚‡ã†ï¼', 'loongtomoka_v2'),
            'hanyu': ('éŸ©è¯­', 'ì•ˆë…•í•˜ì„¸ìš”, ë©´ì ‘ê´€ë‹˜!í•œêµ­ì–´ë¡œ ëŒ€í™”í•´ìš”!', 'loongkyong_v2'),
            'xibanyayu': ('è¥¿ç­ç‰™è¯­', 'Â¡Hola, entrevistador!Â¡Hablemos en espaÃ±ol!', 'sambert-camila-v1'),
            'yinniyu': ('å°å°¼è¯­', 'Halo, pewawancara!Ayo berkomunikasi dalam bahasa Indonesia!', 'sambert-indah-v1'),
            'fayu': ('æ³•è¯­', 'Bonjour, intervieweur !Parlons en franÃ§ais !', 'sambert-clara-v1'),
            'deyu': ('å¾·è¯­', 'Hallo, Interviewer!Lass uns auf Deutsch sprechen!', 'sambert-hanna-v1'),
            'taiyu': ('æ³°è¯­', 'à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¹ˆà¸°/à¸„à¸£à¸±à¸š, à¸œà¸¹à¹‰à¸ªà¸±à¸¡à¸ à¸²à¸©à¸“à¹Œ!à¸¡à¸²à¸à¸¹à¸”à¸„à¸¸à¸¢à¸à¸±à¸™à¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸à¸±à¸™à¹€à¸–à¸­à¸°!', 'sambert-waan-v1')
        }




        # åˆ›å»º OpenAI å®¢æˆ·ç«¯å¯¹è±¡
        self.celue_client = OpenAI(
            api_key=self.ai_config['api_key'],
            base_url=self.ai_config['api_uri'],
            timeout=60.0,
        )
        self.AI_client = AsyncOpenAI(
            api_key=self.ai_config['api_key'],
            base_url=self.ai_config['api_uri'],
            timeout=30.0,
        )
        # 1. è·å–å½“å‰æ­£åœ¨è¿è¡Œçš„äº‹ä»¶å¾ªç¯
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        class FutureCallback(dashscope.audio.tts_v2.ResultCallback):
            def __init__(self, future: asyncio.Future):
                self._future = future
                self._audio_buffer = bytearray()  # ç”¨äºæ‹¼æ¥éŸ³é¢‘ç‰‡æ®µ

            def on_open(self):
                pass  # è¿™é‡Œå¯ä»¥åŠ æ—¥å¿—

            def on_complete(self):
                # è¯­éŸ³åˆæˆå®Œæˆï¼Œæ‰€æœ‰ç‰‡æ®µéƒ½å·²æ”¶åˆ°
                # å°†æ‹¼æ¥å¥½çš„å®Œæ•´éŸ³é¢‘æ•°æ®ä½œä¸ºFutureçš„ç»“æœ
                if not self._future.done():
                    self._future.set_result(bytes(self._audio_buffer))

            def on_error(self, message: str):
                # å‘ç”Ÿé”™è¯¯ï¼Œå°†å¼‚å¸¸ä½œä¸ºFutureçš„ç»“æœ
                if not self._future.done():
                    self._future.set_exception(Exception(f"TTS Error: {message}"))

            def on_close(self):
                # è¿æ¥å…³é—­ï¼Œå¦‚æœFutureè¿˜æœªå®Œæˆï¼ˆå¯èƒ½åœ¨on_completeå‰å…³é—­ï¼‰ï¼Œä¹Ÿè®¾ç½®ç»“æœ
                if not self._future.done():
                    self._future.set_result(bytes(self._audio_buffer))

            def on_data(self, data: bytes):
                # æ”¶åˆ°éŸ³é¢‘ç‰‡æ®µï¼Œå°†å…¶æ‹¼æ¥åˆ°ç¼“å†²åŒº
                if data:
                    self._audio_buffer.extend(data)

        # 3. å°†æ–°çš„Callbackç±»ç»‘å®šåˆ°selfï¼Œä»¥ä¾¿synthesis_ttsæ–¹æ³•ä½¿ç”¨
        self.FutureCallback = FutureCallback


    async def generate_interview_plan(self):
        """ç”Ÿæˆé¢è¯•è®¡åˆ’ï¼ˆä»…é¦–æ¬¡è°ƒç”¨ï¼‰"""
        try:
            plan_prompt = load_prompt("generate_plan_white_collar.txt")
            plan_prompt_filled = plan_prompt.format(
                user_portrait=self.user_portrait,
                jd_title=self.jd_title,
                jd_portrait=self.jd_portrait
            )
            
            response = self.celue_client.chat.completions.create(
                model=self.ai_config['api_model'],
                temperature=0.7,
                messages=[
                    {"role": "system", "content": plan_prompt_filled},
                    {"role": "user", "content": "è¯·ç”Ÿæˆé¢è¯•è®¡åˆ’"}
                ]
            )
            
            plan_json = response.choices[0].message.content
            # æ¸…ç†å¯èƒ½çš„markdownæ ‡è®°
            plan_json = plan_json.replace("```json", "").replace("```", "").strip()
            self.interview_plan = json.loads(plan_json)
            
            logging.info(f"é¢è¯•ID: {self.interview_id} - é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±{len(self.interview_plan['questions'])}ä¸ªé—®é¢˜")
            return self.interview_plan
            
        except Exception as e:
            logging.error(f"é¢è¯•ID: {self.interview_id} - ç”Ÿæˆé¢è¯•è®¡åˆ’å¤±è´¥: {e}")
            # å¤±è´¥æ—¶è¿”å›ç©ºè®¡åˆ’ï¼Œèµ°åŸæœ‰é€»è¾‘
            return None


    async def get_ai_response(self, dialog_history):
        start_time = time.time()

        language, say_hi, self.TTS_model = self.language_dict[self.interview_language]
        system_message = f'''
                        æœ¬æ¬¡é¢è¯•å‚è€ƒèµ„æ–™å¦‚ä¸‹ï¼š
                        #é¢è¯•äº¤æµå…¨ç¨‹ä½¿ç”¨è¯­ç§ï¼š{language}
                        é¢è¯•å¼€å§‹æ—¶é—´ï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}
                        å€™é€‰äººä¿¡æ¯ï¼š{self.user_portrait}
                        å²—ä½ä¿¡æ¯ï¼š{self.jd_title},{self.jd_portrait}
                        å…¬å¸ä¿¡æ¯ï¼š{self.company_portrait}
                        #é¢è¯•äº¤æµå…¨ç¨‹ä½¿ç”¨è¯­ç§ï¼š{language}
                        é¢è¯•æ­£å¼å¼€å§‹ï¼
                                        '''

        self.speech_speed = 1
        
        # åŠ è½½promptæ¨¡æ¿
        celue_template = load_prompt("strategy_white_collar.txt")
        zhixing_template = load_prompt("executor_white_collar.txt")
        lanling_template = load_prompt("executor_blue_collar.txt")
        
        # æ›¿æ¢å˜é‡
        lang = self.language_dict[self.interview_language][0]
        self.celue_sys_prompt = celue_template.format(language=lang)
        self.zhixing_sys_prompt = zhixing_template.format(language=lang)
        self.lanling_sys_prompt = lanling_template.format(language=lang)
        try:
            #interview_type 1ç™½é¢†ï¼Œ0è“é¢†
            if self.interview_type == 1:
                self.speech_speed = 1.1
                celue_message = [{"role": "system", "content": self.celue_sys_prompt},
                                 {"role": "user",
                                  "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}ï¼Œå½“å‰æ—¶é—´ä¸º{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}"}]
                logging.info(f"-- é¢è¯•ID ï¼š{self.interview_id},å½“å‰ç­–ç•¥promptå¦‚ä¸‹--ï¼š{celue_message}")
                for i in range(2):
                    try:
                        celue = self.celue_client.chat.completions.create(
                            model=self.ai_config['api_model'],
                            temperature=0.4,
                            messages=celue_message,
                            extra_body={"chat_template_kwargs": {"enable_thinking": True}},
                        )
                        celue_text = celue.choices[0].message.content
                        break
                    except Exception as e:
                        logging.error(f"-- é¢è¯•ID : {self.interview_id}---ç­–ç•¥è°ƒç”¨AIå‡ºé”™ï¼Œç¬¬{i + 1}æ¬¡å°è¯•: {e}")
                logging.info(
                    f"-- é¢è¯•ID ï¼š{self.interview_id},æ¨ç†æ—¶é—´ï¼š{time.time() - start_time}s,å½“å‰ç­–ç•¥å¦‚ä¸‹--ï¼š\n{celue_text}")
                # å¯ç”¨æµå¼ä¼ è¾“
                zhixing_message = [{"role": "system", "content": self.zhixing_sys_prompt},
                                   {"role": "user",
                                    "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history},å½“å‰æ—¶é—´ä¸ºï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}ï¼Œå½“å‰å›å¤ç­–ç•¥å¦‚ä¸‹{celue_text}"}]
            else:
                zhixing_message = [{"role": "system", "content": self.lanling_sys_prompt},
                                   {"role": "user",
                                    "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}ï¼Œå½“å‰æ—¶é—´ä¸º{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}"}]
            logging.info(f"-- é¢è¯•ID ï¼š{self.interview_id},å½“å‰æ‰§è¡Œpromptå¦‚ä¸‹--ï¼š{zhixing_message}")
            #é‡è¯•æ¬¡æ•°
            for i in range(2):
                try:
                    response = await self.AI_client.chat.completions.create(
                        stream=True,  # å¯ç”¨æµå¼
                        model=self.ai_config['api_model'],
                        temperature=0.7,
                        seed=1,
                        messages=zhixing_message
                    )
                    break
                except Exception as e:
                    logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµå¼è°ƒç”¨AIå‡ºé”™ï¼Œç¬¬{i + 1}æ¬¡å°è¯•: {e}")
            if not response:
                logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµå¼è°ƒç”¨AIå¤±è´¥")
            ai_response = ""
            async for chunk in response:
                if chunk.choices and chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    ai_response += content

                    yield f"data: {json.dumps({'type': 'chunk', 'content': content}, ensure_ascii=False)}\n\n"

            yield f"data: {json.dumps({'type': 'DONE', 'content': "[DONE]"}, ensure_ascii=False)}\n\n"




            end_time = time.time()
            execution_time = end_time - start_time
            logging.info(f"-- é¢è¯•ID : {self.interview_id}---æ‰§è¡Œæ—¶é—´ä¸º: {execution_time} ç§’,AIæµå¼è¿”å›å†…å®¹: {ai_response}")
            try:
                flash_time = time.time()
                #åˆ¤æ–­é¢è¯•æ˜¯å¦ç»“æŸï¼Œæ¯ä¸ªå¾ªç¯éƒ½æœ‰
                liucheng_prompt = load_prompt("judge_flow.txt")
                liucheng = self.celue_client.chat.completions.create(
                    model="qwen-flash",
                    temperature=0.4,
                    messages=[{"role": "system", "content": liucheng_prompt},
                              {"role": "user", "content": f"é¢è¯•å®˜å›å¤å†…å®¹å¦‚ä¸‹{ai_response}"}],
                )
                liucheng_flag = liucheng.choices[0].message.content
                logging.info(
                    f"-- é¢è¯•ID : {self.interview_id}---æµç¨‹åˆ¤æ–­å†…å®¹:  {liucheng_flag}  ,è€—æ—¶{time.time() - flash_time}ç§’")
                if "ç»“æŸ" in liucheng_flag:
                    yield f"data: {json.dumps({'type': 'over', 'content': "over"}, ensure_ascii=False)}\n\n"
            except Exception as e:
                logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµç¨‹åˆ¤æ–­å‡ºé”™: {e}")
                yield f"data: {json.dumps({'type': 'error', 'content': str(e)}, ensure_ascii=False)}\n\n"

        except Exception as e:
            logging.error(f"-- é¢è¯•ID : {self.interview_id}---AIæµå¼å“åº”è·å–é”™è¯¯: {e}")
            yield f"data: {json.dumps({'type': 'error', 'content': "æ¨¡å‹æ£€æµ‹åˆ°éæ³•è¾“å…¥ï¼Œè¯·é‡æ–°å¼€å§‹æœ¬æ¬¡é¢è¯•"+str(e)}, ensure_ascii=False)}\n\n"






```

```python
import asyncio
import json
import logging
import time
import re
import regex
from pathlib import Path
from openai import OpenAI, AsyncOpenAI
import dashscope
import dashscope.audio.tts_v2
from app.config import ai_config
from sdk.tts.punctuation_based_tts import PunctuationBasedTTS


def load_prompt(filename: str) -> str:
    """
    åŠ è½½promptæ–‡ä»¶
    
    Args:
        filename: æ–‡ä»¶åï¼Œå¦‚ "strategy_white_collar.txt"
    
    Returns:
        promptå†…å®¹
    """
    current_dir = Path(__file__).parent
    prompt_path = current_dir.parent.parent / "prompts" / "interview" / filename
    with open(prompt_path, 'r', encoding='utf-8') as f:
        return f.read()



dashscope.api_key = ai_config['api_key']
class CallInterview:
    def __init__(self,  interview_id: str):

        self.interview_id = interview_id
        self.ai_config = ai_config
        self.request_id = None
        self.current_question = None
        self.timeout_task = None
        self.voice_active = False
        self.jd_title = None
        self.jd_portrait = None
        self.user_portrait = None
        self.company_portrait = None
        self.company_name = None
        self.interview_type = 1
        self.speech_speed = 1.2
        self.interview_language = "zhongwen"
        self.interview_style = None
        self.TTS_model = 'longanxuan'
        self.interview_plan = None
        self.current_question_index = 0
        self.language_dict = {
            'zhongwen': ('ä¸­æ–‡', 'ä½ å¥½ï¼Œé¢è¯•å®˜ï¼è®©æˆ‘ä»¬ç”¨ä¸­æ–‡äº¤æµï¼', 'longanxuan'),
            'yingwen': ('è‹±æ–‡', "Hello, interviewer!Let's communicate in English!", 'longxiaochun_v2'),
            'riyu': ('æ—¥è¯­', 'ã“ã‚“ã«ã¡ã¯ã€é¢æ¥å®˜ï¼æ—¥æœ¬èªã§è©±ã—åˆã„ã¾ã—ã‚‡ã†ï¼', 'loongtomoka_v2'),
            'hanyu': ('éŸ©è¯­', 'ì•ˆë…•í•˜ì„¸ìš”, ë©´ì ‘ê´€ë‹˜!í•œêµ­ì–´ë¡œ ëŒ€í™”í•´ìš”!', 'loongkyong_v2'),
            'xibanyayu': ('è¥¿ç­ç‰™è¯­', 'Â¡Hola, entrevistador!Â¡Hablemos en espaÃ±ol!', 'sambert-camila-v1'),
            'yinniyu': ('å°å°¼è¯­', 'Halo, pewawancara!Ayo berkomunikasi dalam bahasa Indonesia!', 'sambert-indah-v1'),
            'fayu': ('æ³•è¯­', 'Bonjour, intervieweur !Parlons en franÃ§ais !', 'sambert-clara-v1'),
            'deyu': ('å¾·è¯­', 'Hallo, Interviewer!Lass uns auf Deutsch sprechen!', 'sambert-hanna-v1'),
            'taiyu': ('æ³°è¯­', 'à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¹ˆà¸°/à¸„à¸£à¸±à¸š, à¸œà¸¹à¹‰à¸ªà¸±à¸¡à¸ à¸²à¸©à¸“à¹Œ!à¸¡à¸²à¸à¸¹à¸”à¸„à¸¸à¸¢à¸à¸±à¸™à¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸à¸±à¸™à¹€à¸–à¸­à¸°!', 'sambert-waan-v1')
        }




        # åˆ›å»º OpenAI å®¢æˆ·ç«¯å¯¹è±¡
        self.celue_client = OpenAI(
            api_key=self.ai_config['api_key'],
            base_url=self.ai_config['api_uri'],
            timeout=60.0,
        )
        self.AI_client = AsyncOpenAI(
            api_key=self.ai_config['api_key'],
            base_url=self.ai_config['api_uri'],
            timeout=30.0,
        )
        # 1. è·å–å½“å‰æ­£åœ¨è¿è¡Œçš„äº‹ä»¶å¾ªç¯
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        class FutureCallback(dashscope.audio.tts_v2.ResultCallback):
            def __init__(self, future: asyncio.Future):
                self._future = future
                self._audio_buffer = bytearray()  # ç”¨äºæ‹¼æ¥éŸ³é¢‘ç‰‡æ®µ

            def on_open(self):
                pass  # è¿™é‡Œå¯ä»¥åŠ æ—¥å¿—

            def on_complete(self):
                # è¯­éŸ³åˆæˆå®Œæˆï¼Œæ‰€æœ‰ç‰‡æ®µéƒ½å·²æ”¶åˆ°
                # å°†æ‹¼æ¥å¥½çš„å®Œæ•´éŸ³é¢‘æ•°æ®ä½œä¸ºFutureçš„ç»“æœ
                if not self._future.done():
                    self._future.set_result(bytes(self._audio_buffer))

            def on_error(self, message: str):
                # å‘ç”Ÿé”™è¯¯ï¼Œå°†å¼‚å¸¸ä½œä¸ºFutureçš„ç»“æœ
                if not self._future.done():
                    self._future.set_exception(Exception(f"TTS Error: {message}"))

            def on_close(self):
                # è¿æ¥å…³é—­ï¼Œå¦‚æœFutureè¿˜æœªå®Œæˆï¼ˆå¯èƒ½åœ¨on_completeå‰å…³é—­ï¼‰ï¼Œä¹Ÿè®¾ç½®ç»“æœ
                if not self._future.done():
                    self._future.set_result(bytes(self._audio_buffer))

            def on_data(self, data: bytes):
                # æ”¶åˆ°éŸ³é¢‘ç‰‡æ®µï¼Œå°†å…¶æ‹¼æ¥åˆ°ç¼“å†²åŒº
                if data:
                    self._audio_buffer.extend(data)

        # 3. å°†æ–°çš„Callbackç±»ç»‘å®šåˆ°selfï¼Œä»¥ä¾¿synthesis_ttsæ–¹æ³•ä½¿ç”¨
        self.FutureCallback = FutureCallback


    async def generate_interview_plan(self):
        """ç”Ÿæˆé¢è¯•è®¡åˆ’ï¼ˆä»…é¦–æ¬¡è°ƒç”¨ï¼‰"""
        try:
            plan_prompt = load_prompt("generate_plan_white_collar.txt")
            plan_prompt_filled = plan_prompt.format(
                user_portrait=self.user_portrait,
                jd_title=self.jd_title,
                jd_portrait=self.jd_portrait
            )
            
            response = self.celue_client.chat.completions.create(
                model=self.ai_config['api_model'],
                temperature=0.7,
                messages=[
                    {"role": "system", "content": plan_prompt_filled},
                    {"role": "user", "content": "è¯·ç”Ÿæˆé¢è¯•è®¡åˆ’"}
                ]
            )
            
            plan_json = response.choices[0].message.content
            # æ¸…ç†å¯èƒ½çš„markdownæ ‡è®°
            plan_json = plan_json.replace("```json", "").replace("```", "").strip()
            self.interview_plan = json.loads(plan_json)
            
            logging.info(f"é¢è¯•ID: {self.interview_id} - é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±{len(self.interview_plan['questions'])}ä¸ªé—®é¢˜")
            return self.interview_plan
            
        except Exception as e:
            logging.error(f"é¢è¯•ID: {self.interview_id} - ç”Ÿæˆé¢è¯•è®¡åˆ’å¤±è´¥: {e}")
            # å¤±è´¥æ—¶è¿”å›ç©ºè®¡åˆ’ï¼Œèµ°åŸæœ‰é€»è¾‘
            return None


    async def get_ai_response(self, dialog_history):
        start_time = time.time()

        # åˆ¤æ–­æ˜¯å¦é¦–æ¬¡è°ƒç”¨
        if not dialog_history or len(dialog_history) == 0:
            # é¦–æ¬¡è°ƒç”¨ï¼šç”Ÿæˆé¢è¯•è®¡åˆ’
            await self.generate_interview_plan()
            
            # è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆå¼€åœºç™½ï¼‰
            if self.interview_plan and len(self.interview_plan['questions']) > 0:
                first_question = self.interview_plan['questions'][0]['question']
                self.current_question_index = 1  # ä¸‹æ¬¡ä»ç¬¬äºŒä¸ªé—®é¢˜å¼€å§‹
                
                yield f"data: {json.dumps({'type': 'chunk', 'content': first_question}, ensure_ascii=False)}\n\n"
                yield f"data: {json.dumps({'type': 'DONE', 'content': '[DONE]'}, ensure_ascii=False)}\n\n"
                return

        language, say_hi, self.TTS_model = self.language_dict[self.interview_language]
        system_message = f'''
                        æœ¬æ¬¡é¢è¯•å‚è€ƒèµ„æ–™å¦‚ä¸‹ï¼š
                        #é¢è¯•äº¤æµå…¨ç¨‹ä½¿ç”¨è¯­ç§ï¼š{language}
                        é¢è¯•å¼€å§‹æ—¶é—´ï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}
                        å€™é€‰äººä¿¡æ¯ï¼š{self.user_portrait}
                        å²—ä½ä¿¡æ¯ï¼š{self.jd_title},{self.jd_portrait}
                        å…¬å¸ä¿¡æ¯ï¼š{self.company_portrait}
                        #é¢è¯•äº¤æµå…¨ç¨‹ä½¿ç”¨è¯­ç§ï¼š{language}
                        é¢è¯•æ­£å¼å¼€å§‹ï¼
                                        '''

        self.speech_speed = 1
        
        # åŠ è½½promptæ¨¡æ¿
        celue_template = load_prompt("strategy_white_collar.txt")
        zhixing_template = load_prompt("executor_white_collar.txt")
        lanling_template = load_prompt("executor_blue_collar.txt")
        
        # æ›¿æ¢å˜é‡
        lang = self.language_dict[self.interview_language][0]
        self.celue_sys_prompt = celue_template.format(language=lang)
        self.zhixing_sys_prompt = zhixing_template.format(language=lang)
        self.lanling_sys_prompt = lanling_template.format(language=lang)
        try:
            #interview_type 1ç™½é¢†ï¼Œ0è“é¢†
            if self.interview_type == 1:
                self.speech_speed = 1.1
                celue_message = [{"role": "system", "content": self.celue_sys_prompt},
                                 {"role": "user",
                                  "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}ï¼Œå½“å‰æ—¶é—´ä¸º{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}"}]
                logging.info(f"-- é¢è¯•ID ï¼š{self.interview_id},å½“å‰ç­–ç•¥promptå¦‚ä¸‹--ï¼š{celue_message}")
                for i in range(2):
                    try:
                        celue = self.celue_client.chat.completions.create(
                            model=self.ai_config['api_model'],
                            temperature=0.4,
                            messages=celue_message,
                            extra_body={"chat_template_kwargs": {"enable_thinking": True}},
                        )
                        celue_text = celue.choices[0].message.content
                        break
                    except Exception as e:
                        logging.error(f"-- é¢è¯•ID : {self.interview_id}---ç­–ç•¥è°ƒç”¨AIå‡ºé”™ï¼Œç¬¬{i + 1}æ¬¡å°è¯•: {e}")
                logging.info(
                    f"-- é¢è¯•ID ï¼š{self.interview_id},æ¨ç†æ—¶é—´ï¼š{time.time() - start_time}s,å½“å‰ç­–ç•¥å¦‚ä¸‹--ï¼š\n{celue_text}")
                # å¯ç”¨æµå¼ä¼ è¾“
                zhixing_message = [{"role": "system", "content": self.zhixing_sys_prompt},
                                   {"role": "user",
                                    "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history},å½“å‰æ—¶é—´ä¸ºï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}ï¼Œå½“å‰å›å¤ç­–ç•¥å¦‚ä¸‹{celue_text}"}]
            else:
                zhixing_message = [{"role": "system", "content": self.lanling_sys_prompt},
                                   {"role": "user",
                                    "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}ï¼Œå½“å‰æ—¶é—´ä¸º{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}"}]
            logging.info(f"-- é¢è¯•ID ï¼š{self.interview_id},å½“å‰æ‰§è¡Œpromptå¦‚ä¸‹--ï¼š{zhixing_message}")
            #é‡è¯•æ¬¡æ•°
            for i in range(2):
                try:
                    response = await self.AI_client.chat.completions.create(
                        stream=True,  # å¯ç”¨æµå¼
                        model=self.ai_config['api_model'],
                        temperature=0.7,
                        seed=1,
                        messages=zhixing_message
                    )
                    break
                except Exception as e:
                    logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµå¼è°ƒç”¨AIå‡ºé”™ï¼Œç¬¬{i + 1}æ¬¡å°è¯•: {e}")
            if not response:
                logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµå¼è°ƒç”¨AIå¤±è´¥")
            ai_response = ""
            async for chunk in response:
                if chunk.choices and chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    ai_response += content

                    yield f"data: {json.dumps({'type': 'chunk', 'content': content}, ensure_ascii=False)}\n\n"

            yield f"data: {json.dumps({'type': 'DONE', 'content': "[DONE]"}, ensure_ascii=False)}\n\n"




            end_time = time.time()
            execution_time = end_time - start_time
            logging.info(f"-- é¢è¯•ID : {self.interview_id}---æ‰§è¡Œæ—¶é—´ä¸º: {execution_time} ç§’,AIæµå¼è¿”å›å†…å®¹: {ai_response}")
            try:
                flash_time = time.time()
                #åˆ¤æ–­é¢è¯•æ˜¯å¦ç»“æŸï¼Œæ¯ä¸ªå¾ªç¯éƒ½æœ‰
                liucheng_prompt = load_prompt("judge_flow.txt")
                liucheng = self.celue_client.chat.completions.create(
                    model="qwen-flash",
                    temperature=0.4,
                    messages=[{"role": "system", "content": liucheng_prompt},
                              {"role": "user", "content": f"é¢è¯•å®˜å›å¤å†…å®¹å¦‚ä¸‹{ai_response}"}],
                )
                liucheng_flag = liucheng.choices[0].message.content
                logging.info(
                    f"-- é¢è¯•ID : {self.interview_id}---æµç¨‹åˆ¤æ–­å†…å®¹:  {liucheng_flag}  ,è€—æ—¶{time.time() - flash_time}ç§’")
                if "ç»“æŸ" in liucheng_flag:
                    yield f"data: {json.dumps({'type': 'over', 'content': "over"}, ensure_ascii=False)}\n\n"
            except Exception as e:
                logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµç¨‹åˆ¤æ–­å‡ºé”™: {e}")
                yield f"data: {json.dumps({'type': 'error', 'content': str(e)}, ensure_ascii=False)}\n\n"

        except Exception as e:
            logging.error(f"-- é¢è¯•ID : {self.interview_id}---AIæµå¼å“åº”è·å–é”™è¯¯: {e}")
            yield f"data: {json.dumps({'type': 'error', 'content': "æ¨¡å‹æ£€æµ‹åˆ°éæ³•è¾“å…¥ï¼Œè¯·é‡æ–°å¼€å§‹æœ¬æ¬¡é¢è¯•"+str(e)}, ensure_ascii=False)}\n\n"






```

```python
import asyncio
import json
import logging
import time
import re
import regex
from pathlib import Path
from openai import OpenAI, AsyncOpenAI
import dashscope
import dashscope.audio.tts_v2
from app.config import ai_config
from sdk.tts.punctuation_based_tts import PunctuationBasedTTS


def load_prompt(filename: str) -> str:
    """
    åŠ è½½promptæ–‡ä»¶
    
    Args:
        filename: æ–‡ä»¶åï¼Œå¦‚ "strategy_white_collar.txt"
    
    Returns:
        promptå†…å®¹
    """
    current_dir = Path(__file__).parent
    prompt_path = current_dir.parent.parent / "prompts" / "interview" / filename
    with open(prompt_path, 'r', encoding='utf-8') as f:
        return f.read()



dashscope.api_key = ai_config['api_key']
class CallInterview:
    def __init__(self,  interview_id: str):

        self.interview_id = interview_id
        self.ai_config = ai_config
        self.request_id = None
        self.current_question = None
        self.timeout_task = None
        self.voice_active = False
        self.jd_title = None
        self.jd_portrait = None
        self.user_portrait = None
        self.company_portrait = None
        self.company_name = None
        self.interview_type = 1
        self.speech_speed = 1.2
        self.interview_language = "zhongwen"
        self.interview_style = None
        self.TTS_model = 'longanxuan'
        self.interview_plan = None
        self.current_question_index = 0
        self.language_dict = {
            'zhongwen': ('ä¸­æ–‡', 'ä½ å¥½ï¼Œé¢è¯•å®˜ï¼è®©æˆ‘ä»¬ç”¨ä¸­æ–‡äº¤æµï¼', 'longanxuan'),
            'yingwen': ('è‹±æ–‡', "Hello, interviewer!Let's communicate in English!", 'longxiaochun_v2'),
            'riyu': ('æ—¥è¯­', 'ã“ã‚“ã«ã¡ã¯ã€é¢æ¥å®˜ï¼æ—¥æœ¬èªã§è©±ã—åˆã„ã¾ã—ã‚‡ã†ï¼', 'loongtomoka_v2'),
            'hanyu': ('éŸ©è¯­', 'ì•ˆë…•í•˜ì„¸ìš”, ë©´ì ‘ê´€ë‹˜!í•œêµ­ì–´ë¡œ ëŒ€í™”í•´ìš”!', 'loongkyong_v2'),
            'xibanyayu': ('è¥¿ç­ç‰™è¯­', 'Â¡Hola, entrevistador!Â¡Hablemos en espaÃ±ol!', 'sambert-camila-v1'),
            'yinniyu': ('å°å°¼è¯­', 'Halo, pewawancara!Ayo berkomunikasi dalam bahasa Indonesia!', 'sambert-indah-v1'),
            'fayu': ('æ³•è¯­', 'Bonjour, intervieweur !Parlons en franÃ§ais !', 'sambert-clara-v1'),
            'deyu': ('å¾·è¯­', 'Hallo, Interviewer!Lass uns auf Deutsch sprechen!', 'sambert-hanna-v1'),
            'taiyu': ('æ³°è¯­', 'à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¹ˆà¸°/à¸„à¸£à¸±à¸š, à¸œà¸¹à¹‰à¸ªà¸±à¸¡à¸ à¸²à¸©à¸“à¹Œ!à¸¡à¸²à¸à¸¹à¸”à¸„à¸¸à¸¢à¸à¸±à¸™à¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸à¸±à¸™à¹€à¸–à¸­à¸°!', 'sambert-waan-v1')
        }




        # åˆ›å»º OpenAI å®¢æˆ·ç«¯å¯¹è±¡
        self.celue_client = OpenAI(
            api_key=self.ai_config['api_key'],
            base_url=self.ai_config['api_uri'],
            timeout=60.0,
        )
        self.AI_client = AsyncOpenAI(
            api_key=self.ai_config['api_key'],
            base_url=self.ai_config['api_uri'],
            timeout=30.0,
        )
        # 1. è·å–å½“å‰æ­£åœ¨è¿è¡Œçš„äº‹ä»¶å¾ªç¯
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        class FutureCallback(dashscope.audio.tts_v2.ResultCallback):
            def __init__(self, future: asyncio.Future):
                self._future = future
                self._audio_buffer = bytearray()  # ç”¨äºæ‹¼æ¥éŸ³é¢‘ç‰‡æ®µ

            def on_open(self):
                pass  # è¿™é‡Œå¯ä»¥åŠ æ—¥å¿—

            def on_complete(self):
                # è¯­éŸ³åˆæˆå®Œæˆï¼Œæ‰€æœ‰ç‰‡æ®µéƒ½å·²æ”¶åˆ°
                # å°†æ‹¼æ¥å¥½çš„å®Œæ•´éŸ³é¢‘æ•°æ®ä½œä¸ºFutureçš„ç»“æœ
                if not self._future.done():
                    self._future.set_result(bytes(self._audio_buffer))

            def on_error(self, message: str):
                # å‘ç”Ÿé”™è¯¯ï¼Œå°†å¼‚å¸¸ä½œä¸ºFutureçš„ç»“æœ
                if not self._future.done():
                    self._future.set_exception(Exception(f"TTS Error: {message}"))

            def on_close(self):
                # è¿æ¥å…³é—­ï¼Œå¦‚æœFutureè¿˜æœªå®Œæˆï¼ˆå¯èƒ½åœ¨on_completeå‰å…³é—­ï¼‰ï¼Œä¹Ÿè®¾ç½®ç»“æœ
                if not self._future.done():
                    self._future.set_result(bytes(self._audio_buffer))

            def on_data(self, data: bytes):
                # æ”¶åˆ°éŸ³é¢‘ç‰‡æ®µï¼Œå°†å…¶æ‹¼æ¥åˆ°ç¼“å†²åŒº
                if data:
                    self._audio_buffer.extend(data)

        # 3. å°†æ–°çš„Callbackç±»ç»‘å®šåˆ°selfï¼Œä»¥ä¾¿synthesis_ttsæ–¹æ³•ä½¿ç”¨
        self.FutureCallback = FutureCallback


    async def generate_interview_plan(self):
        """ç”Ÿæˆé¢è¯•è®¡åˆ’ï¼ˆä»…é¦–æ¬¡è°ƒç”¨ï¼‰"""
        try:
            plan_prompt = load_prompt("generate_plan_white_collar.txt")
            plan_prompt_filled = plan_prompt.format(
                user_portrait=self.user_portrait,
                jd_title=self.jd_title,
                jd_portrait=self.jd_portrait
            )
            
            response = self.celue_client.chat.completions.create(
                model=self.ai_config['api_model'],
                temperature=0.7,
                messages=[
                    {"role": "system", "content": plan_prompt_filled},
                    {"role": "user", "content": "è¯·ç”Ÿæˆé¢è¯•è®¡åˆ’"}
                ]
            )
            
            plan_json = response.choices[0].message.content
            # æ¸…ç†å¯èƒ½çš„markdownæ ‡è®°
            plan_json = plan_json.replace("```json", "").replace("```", "").strip()
            self.interview_plan = json.loads(plan_json)
            
            logging.info(f"é¢è¯•ID: {self.interview_id} - é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±{len(self.interview_plan['questions'])}ä¸ªé—®é¢˜")
            return self.interview_plan
            
        except Exception as e:
            logging.error(f"é¢è¯•ID: {self.interview_id} - ç”Ÿæˆé¢è¯•è®¡åˆ’å¤±è´¥: {e}")
            # å¤±è´¥æ—¶è¿”å›ç©ºè®¡åˆ’ï¼Œèµ°åŸæœ‰é€»è¾‘
            return None


    async def get_ai_response(self, dialog_history):
        start_time = time.time()

        # åˆ¤æ–­æ˜¯å¦é¦–æ¬¡è°ƒç”¨
        if not dialog_history or len(dialog_history) == 0:
            # é¦–æ¬¡è°ƒç”¨ï¼šç”Ÿæˆé¢è¯•è®¡åˆ’
            await self.generate_interview_plan()
            
            # è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆå¼€åœºç™½ï¼‰
            if self.interview_plan and len(self.interview_plan['questions']) > 0:
                first_question = self.interview_plan['questions'][0]['question']
                self.current_question_index = 1  # ä¸‹æ¬¡ä»ç¬¬äºŒä¸ªé—®é¢˜å¼€å§‹
                
                yield f"data: {json.dumps({'type': 'chunk', 'content': first_question}, ensure_ascii=False)}\n\n"
                yield f"data: {json.dumps({'type': 'DONE', 'content': '[DONE]'}, ensure_ascii=False)}\n\n"
                return

        language, say_hi, self.TTS_model = self.language_dict[self.interview_language]
        system_message = f'''
                        æœ¬æ¬¡é¢è¯•å‚è€ƒèµ„æ–™å¦‚ä¸‹ï¼š
                        #é¢è¯•äº¤æµå…¨ç¨‹ä½¿ç”¨è¯­ç§ï¼š{language}
                        é¢è¯•å¼€å§‹æ—¶é—´ï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}
                        å€™é€‰äººä¿¡æ¯ï¼š{self.user_portrait}
                        å²—ä½ä¿¡æ¯ï¼š{self.jd_title},{self.jd_portrait}
                        å…¬å¸ä¿¡æ¯ï¼š{self.company_portrait}
                        #é¢è¯•äº¤æµå…¨ç¨‹ä½¿ç”¨è¯­ç§ï¼š{language}
                        é¢è¯•æ­£å¼å¼€å§‹ï¼
                                        '''

        self.speech_speed = 1
        
        # åŠ è½½promptæ¨¡æ¿
        celue_template = load_prompt("strategy_white_collar.txt")
        zhixing_template = load_prompt("executor_white_collar.txt")
        lanling_template = load_prompt("executor_blue_collar.txt")
        
        # æ›¿æ¢å˜é‡
        lang = self.language_dict[self.interview_language][0]
        self.celue_sys_prompt = celue_template.format(language=lang)
        self.zhixing_sys_prompt = zhixing_template.format(language=lang)
        self.lanling_sys_prompt = lanling_template.format(language=lang)
        try:
            # å‡†å¤‡é¢è¯•è®¡åˆ’ä¿¡æ¯
            plan_info = ""
            if self.interview_plan:
                remaining_questions = self.interview_plan['questions'][self.current_question_index:]
                plan_info = f"""
                ã€é¢è¯•è®¡åˆ’å‚è€ƒã€‘
                å½“å‰è¿›åº¦ï¼šå·²æé—® {self.current_question_index}/{len(self.interview_plan['questions'])} ä¸ªé—®é¢˜
                å¾…æé—®é¢˜ï¼š{json.dumps(remaining_questions[:3], ensure_ascii=False)}
                ï¼ˆæ³¨æ„ï¼šè¿™åªæ˜¯å‚è€ƒè®¡åˆ’ï¼Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µçµæ´»è°ƒæ•´ï¼‰
                """
            
            #interview_type 1ç™½é¢†ï¼Œ0è“é¢†
            if self.interview_type == 1:
                self.speech_speed = 1.1
                celue_message = [{"role": "system", "content": self.celue_sys_prompt},
                                 {"role": "user",
                                  "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}ï¼Œå½“å‰æ—¶é—´ä¸º{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}{plan_info}"}]
                logging.info(f"-- é¢è¯•ID ï¼š{self.interview_id},å½“å‰ç­–ç•¥promptå¦‚ä¸‹--ï¼š{celue_message}")
                for i in range(2):
                    try:
                        celue = self.celue_client.chat.completions.create(
                            model=self.ai_config['api_model'],
                            temperature=0.4,
                            messages=celue_message,
                            extra_body={"chat_template_kwargs": {"enable_thinking": True}},
                        )
                        celue_text = celue.choices[0].message.content
                        break
                    except Exception as e:
                        logging.error(f"-- é¢è¯•ID : {self.interview_id}---ç­–ç•¥è°ƒç”¨AIå‡ºé”™ï¼Œç¬¬{i + 1}æ¬¡å°è¯•: {e}")
                logging.info(
                    f"-- é¢è¯•ID ï¼š{self.interview_id},æ¨ç†æ—¶é—´ï¼š{time.time() - start_time}s,å½“å‰ç­–ç•¥å¦‚ä¸‹--ï¼š\n{celue_text}")
                # å¯ç”¨æµå¼ä¼ è¾“
                zhixing_message = [{"role": "system", "content": self.zhixing_sys_prompt},
                                   {"role": "user",
                                    "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history},å½“å‰æ—¶é—´ä¸ºï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}ï¼Œå½“å‰å›å¤ç­–ç•¥å¦‚ä¸‹{celue_text}"}]
            else:
                zhixing_message = [{"role": "system", "content": self.lanling_sys_prompt},
                                   {"role": "user",
                                    "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}ï¼Œå½“å‰æ—¶é—´ä¸º{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}"}]
            logging.info(f"-- é¢è¯•ID ï¼š{self.interview_id},å½“å‰æ‰§è¡Œpromptå¦‚ä¸‹--ï¼š{zhixing_message}")
            #é‡è¯•æ¬¡æ•°
            for i in range(2):
                try:
                    response = await self.AI_client.chat.completions.create(
                        stream=True,  # å¯ç”¨æµå¼
                        model=self.ai_config['api_model'],
                        temperature=0.7,
                        seed=1,
                        messages=zhixing_message
                    )
                    break
                except Exception as e:
                    logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµå¼è°ƒç”¨AIå‡ºé”™ï¼Œç¬¬{i + 1}æ¬¡å°è¯•: {e}")
            if not response:
                logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµå¼è°ƒç”¨AIå¤±è´¥")
            ai_response = ""
            async for chunk in response:
                if chunk.choices and chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    ai_response += content

                    yield f"data: {json.dumps({'type': 'chunk', 'content': content}, ensure_ascii=False)}\n\n"

            yield f"data: {json.dumps({'type': 'DONE', 'content': "[DONE]"}, ensure_ascii=False)}\n\n"




            end_time = time.time()
            execution_time = end_time - start_time
            logging.info(f"-- é¢è¯•ID : {self.interview_id}---æ‰§è¡Œæ—¶é—´ä¸º: {execution_time} ç§’,AIæµå¼è¿”å›å†…å®¹: {ai_response}")
            try:
                flash_time = time.time()
                #åˆ¤æ–­é¢è¯•æ˜¯å¦ç»“æŸï¼Œæ¯ä¸ªå¾ªç¯éƒ½æœ‰
                liucheng_prompt = load_prompt("judge_flow.txt")
                liucheng = self.celue_client.chat.completions.create(
                    model="qwen-flash",
                    temperature=0.4,
                    messages=[{"role": "system", "content": liucheng_prompt},
                              {"role": "user", "content": f"é¢è¯•å®˜å›å¤å†…å®¹å¦‚ä¸‹{ai_response}"}],
                )
                liucheng_flag = liucheng.choices[0].message.content
                logging.info(
                    f"-- é¢è¯•ID : {self.interview_id}---æµç¨‹åˆ¤æ–­å†…å®¹:  {liucheng_flag}  ,è€—æ—¶{time.time() - flash_time}ç§’")
                if "ç»“æŸ" in liucheng_flag:
                    yield f"data: {json.dumps({'type': 'over', 'content': "over"}, ensure_ascii=False)}\n\n"
            except Exception as e:
                logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµç¨‹åˆ¤æ–­å‡ºé”™: {e}")
                yield f"data: {json.dumps({'type': 'error', 'content': str(e)}, ensure_ascii=False)}\n\n"

        except Exception as e:
            logging.error(f"-- é¢è¯•ID : {self.interview_id}---AIæµå¼å“åº”è·å–é”™è¯¯: {e}")
            yield f"data: {json.dumps({'type': 'error', 'content': "æ¨¡å‹æ£€æµ‹åˆ°éæ³•è¾“å…¥ï¼Œè¯·é‡æ–°å¼€å§‹æœ¬æ¬¡é¢è¯•"+str(e)}, ensure_ascii=False)}\n\n"






```

```python
import asyncio
import json
import logging
import time
import re
import regex
from pathlib import Path
from openai import OpenAI, AsyncOpenAI
import dashscope
import dashscope.audio.tts_v2
from app.config import ai_config
from sdk.tts.punctuation_based_tts import PunctuationBasedTTS


def load_prompt(filename: str) -> str:
    """
    åŠ è½½promptæ–‡ä»¶
    
    Args:
        filename: æ–‡ä»¶åï¼Œå¦‚ "strategy_white_collar.txt"
    
    Returns:
        promptå†…å®¹
    """
    current_dir = Path(__file__).parent
    prompt_path = current_dir.parent.parent / "prompts" / "interview" / filename
    with open(prompt_path, 'r', encoding='utf-8') as f:
        return f.read()



dashscope.api_key = ai_config['api_key']
class CallInterview:
    def __init__(self,  interview_id: str):

        self.interview_id = interview_id
        self.ai_config = ai_config
        self.request_id = None
        self.current_question = None
        self.timeout_task = None
        self.voice_active = False
        self.jd_title = None
        self.jd_portrait = None
        self.user_portrait = None
        self.company_portrait = None
        self.company_name = None
        self.interview_type = 1
        self.speech_speed = 1.2
        self.interview_language = "zhongwen"
        self.interview_style = None
        self.TTS_model = 'longanxuan'
        self.interview_plan = None
        self.current_question_index = 0
        self.language_dict = {
            'zhongwen': ('ä¸­æ–‡', 'ä½ å¥½ï¼Œé¢è¯•å®˜ï¼è®©æˆ‘ä»¬ç”¨ä¸­æ–‡äº¤æµï¼', 'longanxuan'),
            'yingwen': ('è‹±æ–‡', "Hello, interviewer!Let's communicate in English!", 'longxiaochun_v2'),
            'riyu': ('æ—¥è¯­', 'ã“ã‚“ã«ã¡ã¯ã€é¢æ¥å®˜ï¼æ—¥æœ¬èªã§è©±ã—åˆã„ã¾ã—ã‚‡ã†ï¼', 'loongtomoka_v2'),
            'hanyu': ('éŸ©è¯­', 'ì•ˆë…•í•˜ì„¸ìš”, ë©´ì ‘ê´€ë‹˜!í•œêµ­ì–´ë¡œ ëŒ€í™”í•´ìš”!', 'loongkyong_v2'),
            'xibanyayu': ('è¥¿ç­ç‰™è¯­', 'Â¡Hola, entrevistador!Â¡Hablemos en espaÃ±ol!', 'sambert-camila-v1'),
            'yinniyu': ('å°å°¼è¯­', 'Halo, pewawancara!Ayo berkomunikasi dalam bahasa Indonesia!', 'sambert-indah-v1'),
            'fayu': ('æ³•è¯­', 'Bonjour, intervieweur !Parlons en franÃ§ais !', 'sambert-clara-v1'),
            'deyu': ('å¾·è¯­', 'Hallo, Interviewer!Lass uns auf Deutsch sprechen!', 'sambert-hanna-v1'),
            'taiyu': ('æ³°è¯­', 'à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¹ˆà¸°/à¸„à¸£à¸±à¸š, à¸œà¸¹à¹‰à¸ªà¸±à¸¡à¸ à¸²à¸©à¸“à¹Œ!à¸¡à¸²à¸à¸¹à¸”à¸„à¸¸à¸¢à¸à¸±à¸™à¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸à¸±à¸™à¹€à¸–à¸­à¸°!', 'sambert-waan-v1')
        }




        # åˆ›å»º OpenAI å®¢æˆ·ç«¯å¯¹è±¡
        self.celue_client = OpenAI(
            api_key=self.ai_config['api_key'],
            base_url=self.ai_config['api_uri'],
            timeout=60.0,
        )
        self.AI_client = AsyncOpenAI(
            api_key=self.ai_config['api_key'],
            base_url=self.ai_config['api_uri'],
            timeout=30.0,
        )
        # 1. è·å–å½“å‰æ­£åœ¨è¿è¡Œçš„äº‹ä»¶å¾ªç¯
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        class FutureCallback(dashscope.audio.tts_v2.ResultCallback):
            def __init__(self, future: asyncio.Future):
                self._future = future
                self._audio_buffer = bytearray()  # ç”¨äºæ‹¼æ¥éŸ³é¢‘ç‰‡æ®µ

            def on_open(self):
                pass  # è¿™é‡Œå¯ä»¥åŠ æ—¥å¿—

            def on_complete(self):
                # è¯­éŸ³åˆæˆå®Œæˆï¼Œæ‰€æœ‰ç‰‡æ®µéƒ½å·²æ”¶åˆ°
                # å°†æ‹¼æ¥å¥½çš„å®Œæ•´éŸ³é¢‘æ•°æ®ä½œä¸ºFutureçš„ç»“æœ
                if not self._future.done():
                    self._future.set_result(bytes(self._audio_buffer))

            def on_error(self, message: str):
                # å‘ç”Ÿé”™è¯¯ï¼Œå°†å¼‚å¸¸ä½œä¸ºFutureçš„ç»“æœ
                if not self._future.done():
                    self._future.set_exception(Exception(f"TTS Error: {message}"))

            def on_close(self):
                # è¿æ¥å…³é—­ï¼Œå¦‚æœFutureè¿˜æœªå®Œæˆï¼ˆå¯èƒ½åœ¨on_completeå‰å…³é—­ï¼‰ï¼Œä¹Ÿè®¾ç½®ç»“æœ
                if not self._future.done():
                    self._future.set_result(bytes(self._audio_buffer))

            def on_data(self, data: bytes):
                # æ”¶åˆ°éŸ³é¢‘ç‰‡æ®µï¼Œå°†å…¶æ‹¼æ¥åˆ°ç¼“å†²åŒº
                if data:
                    self._audio_buffer.extend(data)

        # 3. å°†æ–°çš„Callbackç±»ç»‘å®šåˆ°selfï¼Œä»¥ä¾¿synthesis_ttsæ–¹æ³•ä½¿ç”¨
        self.FutureCallback = FutureCallback


    async def generate_interview_plan(self):
        """ç”Ÿæˆé¢è¯•è®¡åˆ’ï¼ˆä»…é¦–æ¬¡è°ƒç”¨ï¼‰"""
        try:
            plan_prompt = load_prompt("generate_plan_white_collar.txt")
            plan_prompt_filled = plan_prompt.format(
                user_portrait=self.user_portrait,
                jd_title=self.jd_title,
                jd_portrait=self.jd_portrait
            )
            
            response = self.celue_client.chat.completions.create(
                model=self.ai_config['api_model'],
                temperature=0.7,
                messages=[
                    {"role": "system", "content": plan_prompt_filled},
                    {"role": "user", "content": "è¯·ç”Ÿæˆé¢è¯•è®¡åˆ’"}
                ]
            )
            
            plan_json = response.choices[0].message.content
            # æ¸…ç†å¯èƒ½çš„markdownæ ‡è®°
            plan_json = plan_json.replace("```json", "").replace("```", "").strip()
            self.interview_plan = json.loads(plan_json)
            
            logging.info(f"é¢è¯•ID: {self.interview_id} - é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±{len(self.interview_plan['questions'])}ä¸ªé—®é¢˜")
            return self.interview_plan
            
        except Exception as e:
            logging.error(f"é¢è¯•ID: {self.interview_id} - ç”Ÿæˆé¢è¯•è®¡åˆ’å¤±è´¥: {e}")
            # å¤±è´¥æ—¶è¿”å›ç©ºè®¡åˆ’ï¼Œèµ°åŸæœ‰é€»è¾‘
            return None


    async def get_ai_response(self, dialog_history):
        start_time = time.time()

        # åˆ¤æ–­æ˜¯å¦é¦–æ¬¡è°ƒç”¨
        if not dialog_history or len(dialog_history) == 0:
            # é¦–æ¬¡è°ƒç”¨ï¼šç”Ÿæˆé¢è¯•è®¡åˆ’
            await self.generate_interview_plan()
            
            # è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆå¼€åœºç™½ï¼‰
            if self.interview_plan and len(self.interview_plan['questions']) > 0:
                first_question = self.interview_plan['questions'][0]['question']
                self.current_question_index = 1  # ä¸‹æ¬¡ä»ç¬¬äºŒä¸ªé—®é¢˜å¼€å§‹
                
                yield f"data: {json.dumps({'type': 'chunk', 'content': first_question}, ensure_ascii=False)}\n\n"
                yield f"data: {json.dumps({'type': 'DONE', 'content': '[DONE]'}, ensure_ascii=False)}\n\n"
                return

        language, say_hi, self.TTS_model = self.language_dict[self.interview_language]
        system_message = f'''
                        æœ¬æ¬¡é¢è¯•å‚è€ƒèµ„æ–™å¦‚ä¸‹ï¼š
                        #é¢è¯•äº¤æµå…¨ç¨‹ä½¿ç”¨è¯­ç§ï¼š{language}
                        é¢è¯•å¼€å§‹æ—¶é—´ï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}
                        å€™é€‰äººä¿¡æ¯ï¼š{self.user_portrait}
                        å²—ä½ä¿¡æ¯ï¼š{self.jd_title},{self.jd_portrait}
                        å…¬å¸ä¿¡æ¯ï¼š{self.company_portrait}
                        #é¢è¯•äº¤æµå…¨ç¨‹ä½¿ç”¨è¯­ç§ï¼š{language}
                        é¢è¯•æ­£å¼å¼€å§‹ï¼
                                        '''

        self.speech_speed = 1
        
        # åŠ è½½promptæ¨¡æ¿
        celue_template = load_prompt("strategy_white_collar.txt")
        zhixing_template = load_prompt("executor_white_collar.txt")
        lanling_template = load_prompt("executor_blue_collar.txt")
        
        # æ›¿æ¢å˜é‡
        lang = self.language_dict[self.interview_language][0]
        self.celue_sys_prompt = celue_template.format(language=lang)
        self.zhixing_sys_prompt = zhixing_template.format(language=lang)
        self.lanling_sys_prompt = lanling_template.format(language=lang)
        try:
            # å‡†å¤‡é¢è¯•è®¡åˆ’ä¿¡æ¯
            plan_info = ""
            if self.interview_plan:
                remaining_questions = self.interview_plan['questions'][self.current_question_index:]
                plan_info = f"""
                ã€é¢è¯•è®¡åˆ’å‚è€ƒã€‘
                å½“å‰è¿›åº¦ï¼šå·²æé—® {self.current_question_index}/{len(self.interview_plan['questions'])} ä¸ªé—®é¢˜
                å¾…æé—®é¢˜ï¼š{json.dumps(remaining_questions[:3], ensure_ascii=False)}
                ï¼ˆæ³¨æ„ï¼šè¿™åªæ˜¯å‚è€ƒè®¡åˆ’ï¼Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µçµæ´»è°ƒæ•´ï¼‰
                """
            
            #interview_type 1ç™½é¢†ï¼Œ0è“é¢†
            if self.interview_type == 1:
                self.speech_speed = 1.1
                celue_message = [{"role": "system", "content": self.celue_sys_prompt},
                                 {"role": "user",
                                  "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}ï¼Œå½“å‰æ—¶é—´ä¸º{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}{plan_info}"}]
                logging.info(f"-- é¢è¯•ID ï¼š{self.interview_id},å½“å‰ç­–ç•¥promptå¦‚ä¸‹--ï¼š{celue_message}")
                for i in range(2):
                    try:
                        celue = self.celue_client.chat.completions.create(
                            model=self.ai_config['api_model'],
                            temperature=0.4,
                            messages=celue_message,
                            extra_body={"chat_template_kwargs": {"enable_thinking": True}},
                        )
                        celue_text = celue.choices[0].message.content
                        break
                    except Exception as e:
                        logging.error(f"-- é¢è¯•ID : {self.interview_id}---ç­–ç•¥è°ƒç”¨AIå‡ºé”™ï¼Œç¬¬{i + 1}æ¬¡å°è¯•: {e}")
                logging.info(
                    f"-- é¢è¯•ID ï¼š{self.interview_id},æ¨ç†æ—¶é—´ï¼š{time.time() - start_time}s,å½“å‰ç­–ç•¥å¦‚ä¸‹--ï¼š\n{celue_text}")
                # å¯ç”¨æµå¼ä¼ è¾“
                zhixing_message = [{"role": "system", "content": self.zhixing_sys_prompt},
                                   {"role": "user",
                                    "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history},å½“å‰æ—¶é—´ä¸ºï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}ï¼Œå½“å‰å›å¤ç­–ç•¥å¦‚ä¸‹{celue_text}{plan_info}"}]
            else:
                zhixing_message = [{"role": "system", "content": self.lanling_sys_prompt},
                                   {"role": "user",
                                    "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}ï¼Œå½“å‰æ—¶é—´ä¸º{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}"}]
            logging.info(f"-- é¢è¯•ID ï¼š{self.interview_id},å½“å‰æ‰§è¡Œpromptå¦‚ä¸‹--ï¼š{zhixing_message}")
            #é‡è¯•æ¬¡æ•°
            for i in range(2):
                try:
                    response = await self.AI_client.chat.completions.create(
                        stream=True,  # å¯ç”¨æµå¼
                        model=self.ai_config['api_model'],
                        temperature=0.7,
                        seed=1,
                        messages=zhixing_message
                    )
                    break
                except Exception as e:
                    logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµå¼è°ƒç”¨AIå‡ºé”™ï¼Œç¬¬{i + 1}æ¬¡å°è¯•: {e}")
            if not response:
                logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµå¼è°ƒç”¨AIå¤±è´¥")
            ai_response = ""
            async for chunk in response:
                if chunk.choices and chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    ai_response += content

                    yield f"data: {json.dumps({'type': 'chunk', 'content': content}, ensure_ascii=False)}\n\n"

            yield f"data: {json.dumps({'type': 'DONE', 'content': "[DONE]"}, ensure_ascii=False)}\n\n"




            end_time = time.time()
            execution_time = end_time - start_time
            logging.info(f"-- é¢è¯•ID : {self.interview_id}---æ‰§è¡Œæ—¶é—´ä¸º: {execution_time} ç§’,AIæµå¼è¿”å›å†…å®¹: {ai_response}")
            try:
                flash_time = time.time()
                #åˆ¤æ–­é¢è¯•æ˜¯å¦ç»“æŸï¼Œæ¯ä¸ªå¾ªç¯éƒ½æœ‰
                liucheng_prompt = load_prompt("judge_flow.txt")
                liucheng = self.celue_client.chat.completions.create(
                    model="qwen-flash",
                    temperature=0.4,
                    messages=[{"role": "system", "content": liucheng_prompt},
                              {"role": "user", "content": f"é¢è¯•å®˜å›å¤å†…å®¹å¦‚ä¸‹{ai_response}"}],
                )
                liucheng_flag = liucheng.choices[0].message.content
                logging.info(
                    f"-- é¢è¯•ID : {self.interview_id}---æµç¨‹åˆ¤æ–­å†…å®¹:  {liucheng_flag}  ,è€—æ—¶{time.time() - flash_time}ç§’")
                if "ç»“æŸ" in liucheng_flag:
                    yield f"data: {json.dumps({'type': 'over', 'content': "over"}, ensure_ascii=False)}\n\n"
            except Exception as e:
                logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµç¨‹åˆ¤æ–­å‡ºé”™: {e}")
                yield f"data: {json.dumps({'type': 'error', 'content': str(e)}, ensure_ascii=False)}\n\n"

        except Exception as e:
            logging.error(f"-- é¢è¯•ID : {self.interview_id}---AIæµå¼å“åº”è·å–é”™è¯¯: {e}")
            yield f"data: {json.dumps({'type': 'error', 'content': "æ¨¡å‹æ£€æµ‹åˆ°éæ³•è¾“å…¥ï¼Œè¯·é‡æ–°å¼€å§‹æœ¬æ¬¡é¢è¯•"+str(e)}, ensure_ascii=False)}\n\n"






```

```python
import asyncio
import json
import logging
import time
import re
import regex
from pathlib import Path
from openai import OpenAI, AsyncOpenAI
import dashscope
import dashscope.audio.tts_v2
from app.config import ai_config
from sdk.tts.punctuation_based_tts import PunctuationBasedTTS


def load_prompt(filename: str) -> str:
    """
    åŠ è½½promptæ–‡ä»¶
    
    Args:
        filename: æ–‡ä»¶åï¼Œå¦‚ "strategy_white_collar.txt"
    
    Returns:
        promptå†…å®¹
    """
    current_dir = Path(__file__).parent
    prompt_path = current_dir.parent.parent / "prompts" / "interview" / filename
    with open(prompt_path, 'r', encoding='utf-8') as f:
        return f.read()



dashscope.api_key = ai_config['api_key']
class CallInterview:
    def __init__(self,  interview_id: str):

        self.interview_id = interview_id
        self.ai_config = ai_config
        self.request_id = None
        self.current_question = None
        self.timeout_task = None
        self.voice_active = False
        self.jd_title = None
        self.jd_portrait = None
        self.user_portrait = None
        self.company_portrait = None
        self.company_name = None
        self.interview_type = 1
        self.speech_speed = 1.2
        self.interview_language = "zhongwen"
        self.interview_style = None
        self.TTS_model = 'longanxuan'
        self.interview_plan = None
        self.current_question_index = 0
        self.language_dict = {
            'zhongwen': ('ä¸­æ–‡', 'ä½ å¥½ï¼Œé¢è¯•å®˜ï¼è®©æˆ‘ä»¬ç”¨ä¸­æ–‡äº¤æµï¼', 'longanxuan'),
            'yingwen': ('è‹±æ–‡', "Hello, interviewer!Let's communicate in English!", 'longxiaochun_v2'),
            'riyu': ('æ—¥è¯­', 'ã“ã‚“ã«ã¡ã¯ã€é¢æ¥å®˜ï¼æ—¥æœ¬èªã§è©±ã—åˆã„ã¾ã—ã‚‡ã†ï¼', 'loongtomoka_v2'),
            'hanyu': ('éŸ©è¯­', 'ì•ˆë…•í•˜ì„¸ìš”, ë©´ì ‘ê´€ë‹˜!í•œêµ­ì–´ë¡œ ëŒ€í™”í•´ìš”!', 'loongkyong_v2'),
            'xibanyayu': ('è¥¿ç­ç‰™è¯­', 'Â¡Hola, entrevistador!Â¡Hablemos en espaÃ±ol!', 'sambert-camila-v1'),
            'yinniyu': ('å°å°¼è¯­', 'Halo, pewawancara!Ayo berkomunikasi dalam bahasa Indonesia!', 'sambert-indah-v1'),
            'fayu': ('æ³•è¯­', 'Bonjour, intervieweur !Parlons en franÃ§ais !', 'sambert-clara-v1'),
            'deyu': ('å¾·è¯­', 'Hallo, Interviewer!Lass uns auf Deutsch sprechen!', 'sambert-hanna-v1'),
            'taiyu': ('æ³°è¯­', 'à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¹ˆà¸°/à¸„à¸£à¸±à¸š, à¸œà¸¹à¹‰à¸ªà¸±à¸¡à¸ à¸²à¸©à¸“à¹Œ!à¸¡à¸²à¸à¸¹à¸”à¸„à¸¸à¸¢à¸à¸±à¸™à¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸à¸±à¸™à¹€à¸–à¸­à¸°!', 'sambert-waan-v1')
        }




        # åˆ›å»º OpenAI å®¢æˆ·ç«¯å¯¹è±¡
        self.celue_client = OpenAI(
            api_key=self.ai_config['api_key'],
            base_url=self.ai_config['api_uri'],
            timeout=60.0,
        )
        self.AI_client = AsyncOpenAI(
            api_key=self.ai_config['api_key'],
            base_url=self.ai_config['api_uri'],
            timeout=30.0,
        )
        # 1. è·å–å½“å‰æ­£åœ¨è¿è¡Œçš„äº‹ä»¶å¾ªç¯
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        class FutureCallback(dashscope.audio.tts_v2.ResultCallback):
            def __init__(self, future: asyncio.Future):
                self._future = future
                self._audio_buffer = bytearray()  # ç”¨äºæ‹¼æ¥éŸ³é¢‘ç‰‡æ®µ

            def on_open(self):
                pass  # è¿™é‡Œå¯ä»¥åŠ æ—¥å¿—

            def on_complete(self):
                # è¯­éŸ³åˆæˆå®Œæˆï¼Œæ‰€æœ‰ç‰‡æ®µéƒ½å·²æ”¶åˆ°
                # å°†æ‹¼æ¥å¥½çš„å®Œæ•´éŸ³é¢‘æ•°æ®ä½œä¸ºFutureçš„ç»“æœ
                if not self._future.done():
                    self._future.set_result(bytes(self._audio_buffer))

            def on_error(self, message: str):
                # å‘ç”Ÿé”™è¯¯ï¼Œå°†å¼‚å¸¸ä½œä¸ºFutureçš„ç»“æœ
                if not self._future.done():
                    self._future.set_exception(Exception(f"TTS Error: {message}"))

            def on_close(self):
                # è¿æ¥å…³é—­ï¼Œå¦‚æœFutureè¿˜æœªå®Œæˆï¼ˆå¯èƒ½åœ¨on_completeå‰å…³é—­ï¼‰ï¼Œä¹Ÿè®¾ç½®ç»“æœ
                if not self._future.done():
                    self._future.set_result(bytes(self._audio_buffer))

            def on_data(self, data: bytes):
                # æ”¶åˆ°éŸ³é¢‘ç‰‡æ®µï¼Œå°†å…¶æ‹¼æ¥åˆ°ç¼“å†²åŒº
                if data:
                    self._audio_buffer.extend(data)

        # 3. å°†æ–°çš„Callbackç±»ç»‘å®šåˆ°selfï¼Œä»¥ä¾¿synthesis_ttsæ–¹æ³•ä½¿ç”¨
        self.FutureCallback = FutureCallback


    async def generate_interview_plan(self):
        """ç”Ÿæˆé¢è¯•è®¡åˆ’ï¼ˆä»…é¦–æ¬¡è°ƒç”¨ï¼‰"""
        try:
            plan_prompt = load_prompt("generate_plan_white_collar.txt")
            plan_prompt_filled = plan_prompt.format(
                user_portrait=self.user_portrait,
                jd_title=self.jd_title,
                jd_portrait=self.jd_portrait
            )
            
            response = self.celue_client.chat.completions.create(
                model=self.ai_config['api_model'],
                temperature=0.7,
                messages=[
                    {"role": "system", "content": plan_prompt_filled},
                    {"role": "user", "content": "è¯·ç”Ÿæˆé¢è¯•è®¡åˆ’"}
                ]
            )
            
            plan_json = response.choices[0].message.content
            # æ¸…ç†å¯èƒ½çš„markdownæ ‡è®°
            plan_json = plan_json.replace("```json", "").replace("```", "").strip()
            self.interview_plan = json.loads(plan_json)
            
            logging.info(f"é¢è¯•ID: {self.interview_id} - é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±{len(self.interview_plan['questions'])}ä¸ªé—®é¢˜")
            return self.interview_plan
            
        except Exception as e:
            logging.error(f"é¢è¯•ID: {self.interview_id} - ç”Ÿæˆé¢è¯•è®¡åˆ’å¤±è´¥: {e}")
            # å¤±è´¥æ—¶è¿”å›ç©ºè®¡åˆ’ï¼Œèµ°åŸæœ‰é€»è¾‘
            return None


    async def get_ai_response(self, dialog_history):
        start_time = time.time()

        # åˆ¤æ–­æ˜¯å¦é¦–æ¬¡è°ƒç”¨
        if not dialog_history or len(dialog_history) == 0:
            # é¦–æ¬¡è°ƒç”¨ï¼šç”Ÿæˆé¢è¯•è®¡åˆ’
            await self.generate_interview_plan()
            
            # è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆå¼€åœºç™½ï¼‰
            if self.interview_plan and len(self.interview_plan['questions']) > 0:
                first_question = self.interview_plan['questions'][0]['question']
                self.current_question_index = 1  # ä¸‹æ¬¡ä»ç¬¬äºŒä¸ªé—®é¢˜å¼€å§‹
                
                yield f"data: {json.dumps({'type': 'chunk', 'content': first_question}, ensure_ascii=False)}\n\n"
                yield f"data: {json.dumps({'type': 'DONE', 'content': '[DONE]'}, ensure_ascii=False)}\n\n"
                return

        language, say_hi, self.TTS_model = self.language_dict[self.interview_language]
        system_message = f'''
                        æœ¬æ¬¡é¢è¯•å‚è€ƒèµ„æ–™å¦‚ä¸‹ï¼š
                        #é¢è¯•äº¤æµå…¨ç¨‹ä½¿ç”¨è¯­ç§ï¼š{language}
                        é¢è¯•å¼€å§‹æ—¶é—´ï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}
                        å€™é€‰äººä¿¡æ¯ï¼š{self.user_portrait}
                        å²—ä½ä¿¡æ¯ï¼š{self.jd_title},{self.jd_portrait}
                        å…¬å¸ä¿¡æ¯ï¼š{self.company_portrait}
                        #é¢è¯•äº¤æµå…¨ç¨‹ä½¿ç”¨è¯­ç§ï¼š{language}
                        é¢è¯•æ­£å¼å¼€å§‹ï¼
                                        '''

        self.speech_speed = 1
        
        # åŠ è½½promptæ¨¡æ¿
        celue_template = load_prompt("strategy_white_collar.txt")
        zhixing_template = load_prompt("executor_white_collar.txt")
        lanling_template = load_prompt("executor_blue_collar.txt")
        
        # æ›¿æ¢å˜é‡
        lang = self.language_dict[self.interview_language][0]
        self.celue_sys_prompt = celue_template.format(language=lang)
        self.zhixing_sys_prompt = zhixing_template.format(language=lang)
        self.lanling_sys_prompt = lanling_template.format(language=lang)
        try:
            # å‡†å¤‡é¢è¯•è®¡åˆ’ä¿¡æ¯
            plan_info = ""
            if self.interview_plan:
                remaining_questions = self.interview_plan['questions'][self.current_question_index:]
                plan_info = f"""
                ã€é¢è¯•è®¡åˆ’å‚è€ƒã€‘
                å½“å‰è¿›åº¦ï¼šå·²æé—® {self.current_question_index}/{len(self.interview_plan['questions'])} ä¸ªé—®é¢˜
                å¾…æé—®é¢˜ï¼š{json.dumps(remaining_questions[:3], ensure_ascii=False)}
                ï¼ˆæ³¨æ„ï¼šè¿™åªæ˜¯å‚è€ƒè®¡åˆ’ï¼Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µçµæ´»è°ƒæ•´ï¼‰
                """
            
            #interview_type 1ç™½é¢†ï¼Œ0è“é¢†
            if self.interview_type == 1:
                self.speech_speed = 1.1
                celue_message = [{"role": "system", "content": self.celue_sys_prompt},
                                 {"role": "user",
                                  "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}ï¼Œå½“å‰æ—¶é—´ä¸º{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}{plan_info}"}]
                logging.info(f"-- é¢è¯•ID ï¼š{self.interview_id},å½“å‰ç­–ç•¥promptå¦‚ä¸‹--ï¼š{celue_message}")
                for i in range(2):
                    try:
                        celue = self.celue_client.chat.completions.create(
                            model=self.ai_config['api_model'],
                            temperature=0.4,
                            messages=celue_message,
                            extra_body={"chat_template_kwargs": {"enable_thinking": True}},
                        )
                        celue_text = celue.choices[0].message.content
                        break
                    except Exception as e:
                        logging.error(f"-- é¢è¯•ID : {self.interview_id}---ç­–ç•¥è°ƒç”¨AIå‡ºé”™ï¼Œç¬¬{i + 1}æ¬¡å°è¯•: {e}")
                logging.info(
                    f"-- é¢è¯•ID ï¼š{self.interview_id},æ¨ç†æ—¶é—´ï¼š{time.time() - start_time}s,å½“å‰ç­–ç•¥å¦‚ä¸‹--ï¼š\n{celue_text}")
                # å¯ç”¨æµå¼ä¼ è¾“
                zhixing_message = [{"role": "system", "content": self.zhixing_sys_prompt},
                                   {"role": "user",
                                    "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history},å½“å‰æ—¶é—´ä¸ºï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}ï¼Œå½“å‰å›å¤ç­–ç•¥å¦‚ä¸‹{celue_text}{plan_info}"}]
            else:
                zhixing_message = [{"role": "system", "content": self.lanling_sys_prompt},
                                   {"role": "user",
                                    "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}ï¼Œå½“å‰æ—¶é—´ä¸º{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}"}]
            logging.info(f"-- é¢è¯•ID ï¼š{self.interview_id},å½“å‰æ‰§è¡Œpromptå¦‚ä¸‹--ï¼š{zhixing_message}")
            #é‡è¯•æ¬¡æ•°
            for i in range(2):
                try:
                    response = await self.AI_client.chat.completions.create(
                        stream=True,  # å¯ç”¨æµå¼
                        model=self.ai_config['api_model'],
                        temperature=0.7,
                        seed=1,
                        messages=zhixing_message
                    )
                    break
                except Exception as e:
                    logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµå¼è°ƒç”¨AIå‡ºé”™ï¼Œç¬¬{i + 1}æ¬¡å°è¯•: {e}")
            if not response:
                logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµå¼è°ƒç”¨AIå¤±è´¥")
            ai_response = ""
            async for chunk in response:
                if chunk.choices and chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    ai_response += content

                    yield f"data: {json.dumps({'type': 'chunk', 'content': content}, ensure_ascii=False)}\n\n"

            yield f"data: {json.dumps({'type': 'DONE', 'content': "[DONE]"}, ensure_ascii=False)}\n\n"

            # æ›´æ–°é—®é¢˜ç´¢å¼•
            if self.interview_plan:
                self.current_question_index += 1

            end_time = time.time()
            execution_time = end_time - start_time
            logging.info(f"-- é¢è¯•ID : {self.interview_id}---æ‰§è¡Œæ—¶é—´ä¸º: {execution_time} ç§’,AIæµå¼è¿”å›å†…å®¹: {ai_response}")
            try:
                flash_time = time.time()
                #åˆ¤æ–­é¢è¯•æ˜¯å¦ç»“æŸï¼Œæ¯ä¸ªå¾ªç¯éƒ½æœ‰
                liucheng_prompt = load_prompt("judge_flow.txt")
                liucheng = self.celue_client.chat.completions.create(
                    model="qwen-flash",
                    temperature=0.4,
                    messages=[{"role": "system", "content": liucheng_prompt},
                              {"role": "user", "content": f"é¢è¯•å®˜å›å¤å†…å®¹å¦‚ä¸‹{ai_response}"}],
                )
                liucheng_flag = liucheng.choices[0].message.content
                logging.info(
                    f"-- é¢è¯•ID : {self.interview_id}---æµç¨‹åˆ¤æ–­å†…å®¹:  {liucheng_flag}  ,è€—æ—¶{time.time() - flash_time}ç§’")
                if "ç»“æŸ" in liucheng_flag:
                    yield f"data: {json.dumps({'type': 'over', 'content': "over"}, ensure_ascii=False)}\n\n"
            except Exception as e:
                logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµç¨‹åˆ¤æ–­å‡ºé”™: {e}")
                yield f"data: {json.dumps({'type': 'error', 'content': str(e)}, ensure_ascii=False)}\n\n"

        except Exception as e:
            logging.error(f"-- é¢è¯•ID : {self.interview_id}---AIæµå¼å“åº”è·å–é”™è¯¯: {e}")
            yield f"data: {json.dumps({'type': 'error', 'content': "æ¨¡å‹æ£€æµ‹åˆ°éæ³•è¾“å…¥ï¼Œè¯·é‡æ–°å¼€å§‹æœ¬æ¬¡é¢è¯•"+str(e)}, ensure_ascii=False)}\n\n"






```

ç°åœ¨æ£€æŸ¥ä¸€ä¸‹æ˜¯å¦æœ‰è¯­æ³•é”™è¯¯ï¼š

å®Œç¾ï¼ç°åœ¨è®©æˆ‘è¯»å–ä¿®æ”¹åçš„å®Œæ•´æ–‡ä»¶ï¼Œä¸ºä½ æ¢³ç†ä¿®æ”¹å†…å®¹ï¼š

å®Œç¾ï¼ä¿®æ”¹å®Œæˆã€‚ç°åœ¨è®©æˆ‘ä¸ºä½ æ¢³ç†æ‰€æœ‰çš„ä¿®æ”¹å†…å®¹ï¼š

---

## ğŸ“ ä¿®æ”¹å†…å®¹æ€»ç»“

### âœ… ä¿®æ”¹å®Œæˆçš„æ–‡ä»¶

1. **æ–°å»ºæ–‡ä»¶**ï¼š`project_code/career/prompts/interview/generate_plan_white_collar.txt`
2. **ä¿®æ”¹æ–‡ä»¶**ï¼š`project_code/career/code/service/virtual_interview.py`

---

## ğŸ“‹ è¯¦ç»†ä¿®æ”¹å†…å®¹

### 1ï¸âƒ£ æ–°å¢Promptæ–‡ä»¶

**æ–‡ä»¶è·¯å¾„**ï¼š`project_code/career/prompts/interview/generate_plan_white_collar.txt`

**ä½œç”¨**ï¼šç”¨äºç”Ÿæˆé¢è¯•è®¡åˆ’çš„Promptæ¨¡æ¿

**å…³é”®å†…å®¹**ï¼š
- è¦æ±‚ç”Ÿæˆ10-12ä¸ªé—®é¢˜
- è¦†ç›–5ä¸ªç»´åº¦ï¼ˆå²—ä½ä¸“ä¸šæŠ€èƒ½ã€æ€§æ ¼ç‰¹å¾ã€èŒä¸šè§„åˆ’ã€è½¯ç¡¬æ€§èƒ½åŠ›ã€è–ªé…¬é¢„æœŸï¼‰
- åˆ†ä¸º3ä¸ªé˜¶æ®µï¼ˆç ´å†°ã€æ ¸å¿ƒæ¢ç´¢ã€æ”¶å°¾ï¼‰
- è¾“å‡ºJSONæ ¼å¼çš„é—®é¢˜åˆ—è¡¨

---

### 2ï¸âƒ£ ä¿®æ”¹ `virtual_interview.py`

#### ä¿®æ”¹ç‚¹1ï¼šåœ¨ `__init__` ä¸­æ·»åŠ ä¸¤ä¸ªæ–°å±æ€§

```python
# ç¬¬52-53è¡Œ
self.interview_plan = None           # å­˜å‚¨é¢è¯•è®¡åˆ’
self.current_question_index = 0      # è·Ÿè¸ªå½“å‰é—®é¢˜ç´¢å¼•
```

**ä½œç”¨**ï¼šç”¨äºå­˜å‚¨ç”Ÿæˆçš„é¢è¯•è®¡åˆ’å’Œè·Ÿè¸ªé¢è¯•è¿›åº¦

---

#### ä¿®æ”¹ç‚¹2ï¼šæ–°å¢ `generate_interview_plan` æ–¹æ³•

```python
# ç¬¬120-150è¡Œ
async def generate_interview_plan(self):
    """ç”Ÿæˆé¢è¯•è®¡åˆ’ï¼ˆä»…é¦–æ¬¡è°ƒç”¨ï¼‰"""
```

**ä½œç”¨**ï¼š
- åŠ è½½ `generate_plan_white_collar.txt` prompt
- è°ƒç”¨LLMç”Ÿæˆ10-12ä¸ªé—®é¢˜çš„é¢è¯•è®¡åˆ’
- è§£æJSONå¹¶å­˜å‚¨åˆ° `self.interview_plan`
- å¤±è´¥æ—¶è¿”å›Noneï¼Œèµ°åŸæœ‰é€»è¾‘

**å…³é”®é€»è¾‘**ï¼š
- ä½¿ç”¨ `celue_client`ï¼ˆåŒæ­¥å®¢æˆ·ç«¯ï¼‰è°ƒç”¨
- temperature=0.7ï¼ˆä¿æŒåˆ›æ„æ€§ï¼‰
- æ¸…ç†å¯èƒ½çš„markdownæ ‡è®°ï¼ˆé˜²æ­¢LLMè¾“å‡º ```jsonï¼‰
- è®°å½•æ—¥å¿—

---

#### ä¿®æ”¹ç‚¹3ï¼šåœ¨ `get_ai_response` å¼€å¤´æ·»åŠ é¦–æ¬¡è°ƒç”¨åˆ¤æ–­

```python
# ç¬¬156-168è¡Œ
if not dialog_history or len(dialog_history) == 0:
    # é¦–æ¬¡è°ƒç”¨ï¼šç”Ÿæˆé¢è¯•è®¡åˆ’
    await self.generate_interview_plan()
    
    # è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆå¼€åœºç™½ï¼‰
    if self.interview_plan and len(self.interview_plan['questions']) > 0:
        first_question = self.interview_plan['questions'][0]['question']
        self.current_question_index = 1  # ä¸‹æ¬¡ä»ç¬¬äºŒä¸ªé—®é¢˜å¼€å§‹
        
        yield f"data: {json.dumps({'type': 'chunk', 'content': first_question}, ensure_ascii=False)}\n\n"
        yield f"data: {json.dumps({'type': 'DONE', 'content': '[DONE]'}, ensure_ascii=False)}\n\n"
        return
```

**ä½œç”¨**ï¼š
- åˆ¤æ–­æ˜¯å¦é¦–æ¬¡è°ƒç”¨ï¼ˆ`dialog_history`ä¸ºç©ºï¼‰
- ç”Ÿæˆé¢è¯•è®¡åˆ’
- ç›´æ¥è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜
- è®¾ç½®ç´¢å¼•ä¸º1ï¼ˆä¸‹æ¬¡ä»ç¬¬äºŒä¸ªé—®é¢˜å¼€å§‹ï¼‰

---

#### ä¿®æ”¹ç‚¹4ï¼šå‡†å¤‡é¢è¯•è®¡åˆ’ä¿¡æ¯å¹¶ä¼ é€’ç»™ç­–ç•¥å±‚

```python
# ç¬¬195-204è¡Œ
# å‡†å¤‡é¢è¯•è®¡åˆ’ä¿¡æ¯
plan_info = ""
if self.interview_plan:
    remaining_questions = self.interview_plan['questions'][self.current_question_index:]
    plan_info = f"""
    ã€é¢è¯•è®¡åˆ’å‚è€ƒã€‘
    å½“å‰è¿›åº¦ï¼šå·²æé—® {self.current_question_index}/{len(self.interview_plan['questions'])} ä¸ªé—®é¢˜
    å¾…æé—®é¢˜ï¼š{json.dumps(remaining_questions[:3], ensure_ascii=False)}
    ï¼ˆæ³¨æ„ï¼šè¿™åªæ˜¯å‚è€ƒè®¡åˆ’ï¼Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µçµæ´»è°ƒæ•´ï¼‰
    """
```

**ä½œç”¨**ï¼š
- æå–å‰©ä½™æœªé—®çš„é—®é¢˜
- åªæ˜¾ç¤ºæ¥ä¸‹æ¥3ä¸ªé—®é¢˜ï¼ˆé¿å…ä¿¡æ¯è¿‡è½½ï¼‰
- æç¤ºå¯ä»¥çµæ´»è°ƒæ•´

---

#### ä¿®æ”¹ç‚¹5ï¼šç­–ç•¥å±‚åŠ å…¥é¢è¯•è®¡åˆ’ä¿¡æ¯

```python
# ç¬¬211è¡Œ
content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}ï¼Œå½“å‰æ—¶é—´ä¸º{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}{plan_info}"
```

**ä½œç”¨**ï¼šè®©ç­–ç•¥å±‚åœ¨å†³ç­–æ—¶å‚è€ƒé¢è¯•è®¡åˆ’

---

#### ä¿®æ”¹ç‚¹6ï¼šæ‰§è¡Œå±‚åŠ å…¥é¢è¯•è®¡åˆ’ä¿¡æ¯

```python
# ç¬¬230è¡Œ
content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history},å½“å‰æ—¶é—´ä¸ºï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}ï¼Œå½“å‰å›å¤ç­–ç•¥å¦‚ä¸‹{celue_text}{plan_info}"
```

**ä½œç”¨**ï¼šè®©æ‰§è¡Œå±‚åœ¨ç”Ÿæˆå›å¤æ—¶å‚è€ƒé¢è¯•è®¡åˆ’

---

#### ä¿®æ”¹ç‚¹7ï¼šæ›´æ–°é—®é¢˜ç´¢å¼•

```python
# ç¬¬261-263è¡Œ
# æ›´æ–°é—®é¢˜ç´¢å¼•
if self.interview_plan:
    self.current_question_index += 1
```

**ä½œç”¨**ï¼šæ¯æ¬¡AIå›å¤åï¼Œç´¢å¼•+1ï¼Œè·Ÿè¸ªé¢è¯•è¿›åº¦

---

## ğŸ¯ ä¿®æ”¹åçš„æ‰§è¡Œæµç¨‹

### é¦–æ¬¡è°ƒç”¨ï¼ˆdialogHistoryä¸ºç©ºï¼‰

```
å‰ç«¯è¯·æ±‚ â†’ get_ai_response(dialog_history=[])
  â†“
åˆ¤æ–­é¦–æ¬¡è°ƒç”¨ â†’ è°ƒç”¨ generate_interview_plan()
  â†“
ã€LLMè°ƒç”¨1ã€‘ç”Ÿæˆé¢è¯•è®¡åˆ’ï¼ˆ10-12ä¸ªé—®é¢˜ï¼‰
  â†“
å­˜å‚¨åˆ° self.interview_plan
  â†“
è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆæµå¼è¾“å‡ºï¼‰
  â†“
è®¾ç½® current_question_index = 1
```

### åç»­è°ƒç”¨ï¼ˆç”¨æˆ·å·²å›ç­”ï¼‰

```
å‰ç«¯è¯·æ±‚ â†’ get_ai_response(dialog_history=[...])
  â†“
å‡†å¤‡é¢è¯•è®¡åˆ’ä¿¡æ¯ï¼ˆå½“å‰è¿›åº¦ + å¾…æé—®é¢˜ï¼‰
  â†“
ã€ç­–ç•¥å±‚ã€‘LLMè°ƒç”¨2ï¼šåˆ†æ + å‚è€ƒè®¡åˆ’ â†’ ç­–ç•¥å†³ç­–
  â†“
ã€æ‰§è¡Œå±‚ã€‘LLMè°ƒç”¨3ï¼šåŸºäºç­–ç•¥ + è®¡åˆ’ â†’ ç”Ÿæˆå›å¤ï¼ˆæµå¼ï¼‰
  â†“
æ›´æ–° current_question_index += 1
  â†“
ã€æµç¨‹åˆ¤æ–­ã€‘LLMè°ƒç”¨4ï¼šåˆ¤æ–­æ˜¯å¦ç»“æŸ
```

---

## âœ… æ ¸å¿ƒç‰¹ç‚¹

1. **ç»“æ„åŒ–é¢è¯•**ï¼šé¦–æ¬¡ç”Ÿæˆ10-12ä¸ªé—®é¢˜çš„å®Œæ•´è®¡åˆ’
2. **çµæ´»æ€§ä¿æŒ**ï¼šç­–ç•¥å±‚ä»ç„¶ä¿ç•™ï¼Œå¯åŠ¨æ€è°ƒæ•´
3. **ç»´åº¦è¦†ç›–**ï¼šç¡®ä¿5å¤§ç»´åº¦éƒ½è¢«è¦†ç›–
4. **æ¸è¿›å¼æé—®**ï¼šé€ä¸ªé—®é¢˜æ¨è¿›ï¼Œä¸æ˜¯ä¸€æ¬¡æ€§æŠ›å‡ºæ‰€æœ‰é—®é¢˜
5. **è¿›åº¦è·Ÿè¸ª**ï¼šé€šè¿‡ `current_question_index` è·Ÿè¸ªé¢è¯•è¿›åº¦
6. **å®¹é”™æœºåˆ¶**ï¼šå¦‚æœè®¡åˆ’ç”Ÿæˆå¤±è´¥ï¼Œèµ°åŸæœ‰é€»è¾‘
7. **æœ€å°æ”¹åŠ¨**ï¼šåªåœ¨å¿…è¦çš„åœ°æ–¹æ·»åŠ ä»£ç ï¼Œä¸å½±å“ç°æœ‰é€»è¾‘

---

## ğŸ” éªŒè¯è¦ç‚¹

ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹å‡ ç‚¹æ¥éªŒè¯ä¿®æ”¹æ˜¯å¦æ­£ç¡®ï¼š

1. âœ… æ–°å¢äº†2ä¸ªå®ä¾‹å±æ€§ï¼ˆ`interview_plan`, `current_question_index`ï¼‰
2. âœ… æ–°å¢äº†1ä¸ªæ–¹æ³•ï¼ˆ`generate_interview_plan`ï¼‰
3. âœ… æ–°å¢äº†1ä¸ªpromptæ–‡ä»¶ï¼ˆ`generate_plan_white_collar.txt`ï¼‰
4. âœ… é¦–æ¬¡è°ƒç”¨åˆ¤æ–­é€»è¾‘æ­£ç¡®ï¼ˆ`if not dialog_history or len(dialog_history) == 0:`ï¼‰
5. âœ… ç­–ç•¥å±‚å’Œæ‰§è¡Œå±‚éƒ½æ”¶åˆ°äº† `plan_info`
6. âœ… æ¯æ¬¡å›å¤åç´¢å¼•è‡ªå¢
7. âœ… æ²¡æœ‰è¯­æ³•é”™è¯¯ï¼ˆå·²é€šè¿‡linteræ£€æŸ¥ï¼‰

---

æ‰€æœ‰ä¿®æ”¹éƒ½ä¸¥æ ¼æŒ‰ç…§ä¹‹å‰è®¨è®ºçš„æ–¹æ¡ˆæ‰§è¡Œï¼Œæ²¡æœ‰æ·»åŠ ä»»ä½•é¢å¤–åŠŸèƒ½ã€‚ä¿®æ”¹å®Œæˆï¼ğŸ‰

```python
# ç¬¬52-53è¡Œ
self.interview_plan = None           # å­˜å‚¨é¢è¯•è®¡åˆ’
self.current_question_index = 0      # è·Ÿè¸ªå½“å‰é—®é¢˜ç´¢å¼•
```

```python
# ç¬¬120-150è¡Œ
async def generate_interview_plan(self):
    """ç”Ÿæˆé¢è¯•è®¡åˆ’ï¼ˆä»…é¦–æ¬¡è°ƒç”¨ï¼‰"""
```

```plaintext
# ç¬¬156-168è¡Œ
if not dialog_history or len(dialog_history) == 0:
    # é¦–æ¬¡è°ƒç”¨ï¼šç”Ÿæˆé¢è¯•è®¡åˆ’
    await self.generate_interview_plan()
    
    # è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆå¼€åœºç™½ï¼‰
    if self.interview_plan and len(self.interview_plan['questions']) > 0:
        first_question = self.interview_plan['questions'][0]['question']
        self.current_question_index = 1  # ä¸‹æ¬¡ä»ç¬¬äºŒä¸ªé—®é¢˜å¼€å§‹
        
        yield f"data: {json.dumps({'type': 'chunk', 'content': first_question}, ensure_ascii=False)}\n\n"
        yield f"data: {json.dumps({'type': 'DONE', 'content': '[DONE]'}, ensure_ascii=False)}\n\n"
        return

```

```python
# ç¬¬156-168è¡Œ
if not dialog_history or len(dialog_history) == 0:
    # é¦–æ¬¡è°ƒç”¨ï¼šç”Ÿæˆé¢è¯•è®¡åˆ’
    await self.generate_interview_plan()
    
    # è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆå¼€åœºç™½ï¼‰
    if self.interview_plan and len(self.interview_plan['questions']) > 0:
        first_question = self.interview_plan['questions'][0]['question']
        self.current_question_index = 1  # ä¸‹æ¬¡ä»ç¬¬äºŒä¸ªé—®é¢˜å¼€å§‹
        
        yield f"data: {json.dumps({'type': 'chunk', 'content': first_question}, ensure_ascii=False)}\n\n"
        yield f"data: {json.dumps({'type': 'DONE', 'content': '[DONE]'}, ensure_ascii=False)}\n\n"
        return

```

```python
# ç¬¬195-204è¡Œ
# å‡†å¤‡é¢è¯•è®¡åˆ’ä¿¡æ¯
plan_info = ""
if self.interview_plan:
    remaining_questions = self.interview_plan['questions'][self.current_question_index:]
    plan_info = f"""
    ã€é¢è¯•è®¡åˆ’å‚è€ƒã€‘
    å½“å‰è¿›åº¦ï¼šå·²æé—® {self.current_question_index}/{len(self.interview_plan['questions'])} ä¸ªé—®é¢˜
    å¾…æé—®é¢˜ï¼š{json.dumps(remaining_questions[:3], ensure_ascii=False)}
    ï¼ˆæ³¨æ„ï¼šè¿™åªæ˜¯å‚è€ƒè®¡åˆ’ï¼Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µçµæ´»è°ƒæ•´ï¼‰
    """
```

```python
# ç¬¬211è¡Œ
content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}ï¼Œå½“å‰æ—¶é—´ä¸º{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}{plan_info}"
```

```python
# ç¬¬230è¡Œ
content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history},å½“å‰æ—¶é—´ä¸ºï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}ï¼Œå½“å‰å›å¤ç­–ç•¥å¦‚ä¸‹{celue_text}{plan_info}"
```

```python
# ç¬¬261-263è¡Œ
# æ›´æ–°é—®é¢˜ç´¢å¼•
if self.interview_plan:
    self.current_question_index += 1
```

```plaintext
å‰ç«¯è¯·æ±‚ â†’ get_ai_response(dialog_history=[])
  â†“
åˆ¤æ–­é¦–æ¬¡è°ƒç”¨ â†’ è°ƒç”¨ generate_interview_plan()
  â†“
ã€LLMè°ƒç”¨1ã€‘ç”Ÿæˆé¢è¯•è®¡åˆ’ï¼ˆ10-12ä¸ªé—®é¢˜ï¼‰
  â†“
å­˜å‚¨åˆ° self.interview_plan
  â†“
è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆæµå¼è¾“å‡ºï¼‰
  â†“
è®¾ç½® current_question_index = 1
```

```plaintext
å‰ç«¯è¯·æ±‚ â†’ get_ai_response(dialog_history=[...])
  â†“
å‡†å¤‡é¢è¯•è®¡åˆ’ä¿¡æ¯ï¼ˆå½“å‰è¿›åº¦ + å¾…æé—®é¢˜ï¼‰
  â†“
ã€ç­–ç•¥å±‚ã€‘LLMè°ƒç”¨2ï¼šåˆ†æ + å‚è€ƒè®¡åˆ’ â†’ ç­–ç•¥å†³ç­–
  â†“
ã€æ‰§è¡Œå±‚ã€‘LLMè°ƒç”¨3ï¼šåŸºäºç­–ç•¥ + è®¡åˆ’ â†’ ç”Ÿæˆå›å¤ï¼ˆæµå¼ï¼‰
  â†“
æ›´æ–° current_question_index += 1
  â†“
ã€æµç¨‹åˆ¤æ–­ã€‘LLMè°ƒç”¨4ï¼šåˆ¤æ–­æ˜¯å¦ç»“æŸ
```

---

**User**

ç„¶åæ¥ä¸‹æ¥å‘¢ï¼Œæˆ‘åº”è¯¥æ˜¯å¦‚ä½•å»æµ‹è¯•å‘¢ï¼Ÿä½†æ˜¯æˆ‘è¿™è¾¹ä½¿ç”¨postmanæ¥è¿›è¡Œæµ‹è¯•çš„è¯æ¯æ¬¡åªèƒ½å‘é€ä¸€ä¸ªå‘€ã€‚ä½†å®é™…ä¸Šæˆ‘è¦æ£€æµ‹æ•ˆæœçš„æ˜¯å¤šæ¬¡çš„è¿ç»­è°ƒç”¨è¿™ä¸ªç»“æœï¼Œæ˜¯å§ï¼Ÿ æ‰€ä»¥è¿™ä¸€ç‚¹æˆ‘æœ‰ç‚¹å›°æƒ‘å•Šï¼Œä¸çŸ¥é“è¯¥æ€ä¹ˆæµ‹è¯•å‘€ï¼Ÿæ€ä¹ˆæµ‹è¯•è¿™ä¸ªæç¤ºè¯æ˜¯å¦è¾¾åˆ°é¢„æœŸçš„æ•ˆæœå‘¢ï¼Ÿ èƒ½ä¸èƒ½å¸®æˆ‘æ¢³ç†ä¸€ä¸‹å‘¢ï¼Ÿ

---

**Cursor**

ç†è§£ä½ çš„å›°æƒ‘ï¼ç¡®å®ï¼Œç”¨Postmanæµ‹è¯•å¤šè½®å¯¹è¯æœ‰ç‚¹éº»çƒ¦ã€‚è®©æˆ‘å¸®ä½ æ¢³ç†ä¸€ä¸‹æµ‹è¯•æ–¹æ¡ˆå’Œæ–¹æ³•ã€‚

## ğŸ§ª æµ‹è¯•ç­–ç•¥

### æ–¹æ¡ˆ1ï¼šä½¿ç”¨Postmanè¿›è¡Œå¤šè½®æµ‹è¯•ï¼ˆæ‰‹åŠ¨æ¨¡æ‹Ÿï¼‰

è™½ç„¶æ¯æ¬¡åªèƒ½å‘ä¸€ä¸ªè¯·æ±‚ï¼Œä½†æˆ‘ä»¬å¯ä»¥**æ‰‹åŠ¨æ¨¡æ‹Ÿå®Œæ•´å¯¹è¯æµç¨‹**ï¼š

#### æ­¥éª¤æµç¨‹ï¼š

```
ç¬¬1æ¬¡è¯·æ±‚ â†’ é¦–æ¬¡è°ƒç”¨ï¼Œç”Ÿæˆè®¡åˆ’ + è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜
  â†“ï¼ˆå¤åˆ¶è¿”å›çš„é—®é¢˜ï¼‰
ç¬¬2æ¬¡è¯·æ±‚ â†’ æ„é€ dialogHistoryï¼ŒåŒ…å«AIçš„ç¬¬ä¸€ä¸ªé—®é¢˜ + ç”¨æˆ·å›ç­”
  â†“ï¼ˆç»§ç»­æ·»åŠ åˆ°dialogHistoryï¼‰
ç¬¬3æ¬¡è¯·æ±‚ â†’ ç»§ç»­å¯¹è¯...
  â†“
...æŒç»­åˆ°é¢è¯•ç»“æŸ
```

#### å…·ä½“ç¤ºä¾‹ï¼š

**ç¬¬1æ¬¡è¯·æ±‚ï¼ˆå¼€å§‹é¢è¯•ï¼‰**
```json
POST http://your-api/virturlInterview

{
  "id": "test_interview_001",
  "userPortrait": "æˆ‘å«å¼ ä¸‰ï¼Œ5å¹´Pythonåç«¯å¼€å‘ç»éªŒï¼Œç†Ÿæ‚‰Djangoã€Flaskæ¡†æ¶ï¼Œåšè¿‡ç”µå•†å’Œé‡‘èç³»ç»Ÿ",
  "jdTitle": "é«˜çº§Pythonå·¥ç¨‹å¸ˆ",
  "jdPortrait": "è´Ÿè´£åç«¯APIå¼€å‘ï¼Œè¦æ±‚ç†Ÿæ‚‰å¾®æœåŠ¡æ¶æ„ï¼Œæœ‰é«˜å¹¶å‘ç³»ç»Ÿç»éªŒ",
  "interviewType": 1,
  "interviewLanguage": "zhongwen",
  "dialogHistory": []
}
```

**é¢„æœŸè¿”å›**ï¼š
```
data: {"type": "chunk", "content": "å¼ ä¸‰ä½ å¥½ï¼çœ‹åˆ°ä½ æœ‰5å¹´Pythonå¼€å‘ç»éªŒ..."}
data: {"type": "DONE", "content": "[DONE]"}
```

---

**ç¬¬2æ¬¡è¯·æ±‚ï¼ˆç”¨æˆ·å›ç­”åï¼‰**
```json
POST http://your-api/virturlInterview

{
  "id": "test_interview_001",
  "userPortrait": "æˆ‘å«å¼ ä¸‰ï¼Œ5å¹´Pythonåç«¯å¼€å‘ç»éªŒï¼Œç†Ÿæ‚‰Djangoã€Flaskæ¡†æ¶ï¼Œåšè¿‡ç”µå•†å’Œé‡‘èç³»ç»Ÿ",
  "jdTitle": "é«˜çº§Pythonå·¥ç¨‹å¸ˆ",
  "jdPortrait": "è´Ÿè´£åç«¯APIå¼€å‘ï¼Œè¦æ±‚ç†Ÿæ‚‰å¾®æœåŠ¡æ¶æ„ï¼Œæœ‰é«˜å¹¶å‘ç³»ç»Ÿç»éªŒ",
  "interviewType": 1,
  "interviewLanguage": "zhongwen",
  "dialogHistory": [
    {
      "role": "assistant",
      "content": "å¼ ä¸‰ä½ å¥½ï¼çœ‹åˆ°ä½ æœ‰5å¹´Pythonå¼€å‘ç»éªŒï¼Œèƒ½ç®€å•èŠèŠä½ æ˜¯æ€ä¹ˆå¼€å§‹æ¥è§¦Pythonçš„å—ï¼Ÿå½“åˆä¸ºä»€ä¹ˆé€‰æ‹©åç«¯å¼€å‘è¿™ä¸ªæ–¹å‘ï¼Ÿ"
    },
    {
      "role": "user",
      "content": "æˆ‘å¤§å­¦å­¦çš„æ˜¯è®¡ç®—æœºä¸“ä¸šï¼Œå¤§ä¸‰å¼€å§‹æ¥è§¦Pythonï¼Œè§‰å¾—è¯­æ³•ç®€æ´ï¼Œå¾ˆé€‚åˆåšåç«¯å¼€å‘ã€‚åæ¥å®ä¹ æ—¶åšäº†ä¸€ä¸ªDjangoé¡¹ç›®ï¼Œå°±çˆ±ä¸Šäº†åç«¯å¼€å‘ã€‚"
    }
  ]
}
```

**é¢„æœŸè¿”å›**ï¼š
```
data: {"type": "chunk", "content": "å¾ˆå¥½ï¼ä»å®ä¹ é¡¹ç›®å¼€å§‹ç§¯ç´¯ç»éªŒ..."}
```

---

**ç¬¬3æ¬¡è¯·æ±‚ï¼ˆç»§ç»­å¯¹è¯ï¼‰**
```json
{
  "id": "test_interview_001",
  "userPortrait": "...",
  "jdTitle": "é«˜çº§Pythonå·¥ç¨‹å¸ˆ",
  "jdPortrait": "...",
  "interviewType": 1,
  "interviewLanguage": "zhongwen",
  "dialogHistory": [
    {"role": "assistant", "content": "ç¬¬ä¸€ä¸ªé—®é¢˜..."},
    {"role": "user", "content": "ç¬¬ä¸€ä¸ªå›ç­”..."},
    {"role": "assistant", "content": "ç¬¬äºŒä¸ªé—®é¢˜..."},
    {"role": "user", "content": "ç¬¬äºŒä¸ªå›ç­”..."}
  ]
}
```

---

### æ–¹æ¡ˆ2ï¼šç¼–å†™Pythonæµ‹è¯•è„šæœ¬ï¼ˆæ¨èï¼ï¼‰

åˆ›å»ºä¸€ä¸ªç®€å•çš„Pythonè„šæœ¬æ¥è‡ªåŠ¨åŒ–æµ‹è¯•å¤šè½®å¯¹è¯ï¼š

```python
# test_interview.py
import requests
import json
import time

API_URL = "http://localhost:8000/virturlInterview"  # ä¿®æ”¹ä¸ºä½ çš„APIåœ°å€

def test_interview():
    """æµ‹è¯•å®Œæ•´çš„é¢è¯•æµç¨‹"""
    
    # åˆå§‹è¯·æ±‚æ•°æ®
    request_data = {
        "id": "test_interview_001",
        "userPortrait": "æˆ‘å«å¼ ä¸‰ï¼Œ5å¹´Pythonåç«¯å¼€å‘ç»éªŒï¼Œç†Ÿæ‚‰Djangoã€Flaskæ¡†æ¶ï¼Œåšè¿‡ç”µå•†å’Œé‡‘èç³»ç»Ÿ",
        "jdTitle": "é«˜çº§Pythonå·¥ç¨‹å¸ˆ",
        "jdPortrait": "è´Ÿè´£åç«¯APIå¼€å‘ï¼Œè¦æ±‚ç†Ÿæ‚‰å¾®æœåŠ¡æ¶æ„ï¼Œæœ‰é«˜å¹¶å‘ç³»ç»Ÿç»éªŒ",
        "interviewType": 1,
        "interviewLanguage": "zhongwen",
        "dialogHistory": []
    }
    
    # é¢„è®¾çš„ç”¨æˆ·å›ç­”ï¼ˆæ¨¡æ‹ŸçœŸå®å¯¹è¯ï¼‰
    user_answers = [
        "æˆ‘å¤§å­¦å­¦çš„æ˜¯è®¡ç®—æœºä¸“ä¸šï¼Œå¤§ä¸‰å¼€å§‹æ¥è§¦Pythonï¼Œè§‰å¾—è¯­æ³•ç®€æ´ã€‚åæ¥å®ä¹ æ—¶åšäº†ä¸€ä¸ªDjangoé¡¹ç›®ï¼Œå°±çˆ±ä¸Šäº†åç«¯å¼€å‘ã€‚",
        "æˆ‘æœ€æœ‰æˆå°±æ„Ÿçš„æ˜¯åœ¨é‡‘èå…¬å¸åšçš„æ”¯ä»˜ç³»ç»Ÿé‡æ„é¡¹ç›®ã€‚å½“æ—¶ç³»ç»Ÿå“åº”æ…¢ï¼Œé«˜å³°æœŸç»å¸¸è¶…æ—¶ã€‚æˆ‘è´Ÿè´£ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢å’Œå¼•å…¥Redisç¼“å­˜ï¼Œæœ€ç»ˆå°†å“åº”æ—¶é—´ä»2ç§’é™åˆ°200æ¯«ç§’ä»¥å†…ã€‚",
        "æˆ‘ä¼šå…ˆé€šè¿‡æ—¥å¿—å®šä½é—®é¢˜å‡ºç°çš„ä½ç½®ï¼Œç„¶åæ£€æŸ¥æ”¯ä»˜æ¥å£çš„äº‹åŠ¡å¤„ç†é€»è¾‘ã€‚å¯èƒ½æ˜¯æ”¯ä»˜æˆåŠŸåçš„å›è°ƒå¤„ç†å¤±è´¥ï¼Œæˆ–è€…æ¶ˆæ¯é˜Ÿåˆ—å»¶è¿Ÿå¯¼è‡´è®¢å•çŠ¶æ€æ›´æ–°ä¸åŠæ—¶ã€‚æˆ‘ä¼šå…ˆå›æ»šæ•°æ®ä¿®å¤ç”¨æˆ·è®¢å•ï¼Œå†ä¿®å¤æ ¹æœ¬é—®é¢˜ã€‚",
        "Djangoé€‚åˆå¿«é€Ÿå¼€å‘ï¼ŒORMå¾ˆæ–¹ä¾¿ï¼Œé€‚åˆä¼ ç»ŸWebåº”ç”¨ã€‚Flaskæ›´è½»é‡ï¼Œé€‚åˆå¾®æœåŠ¡å’ŒAPIå¼€å‘ã€‚å¦‚æœé¡¹ç›®éœ€è¦å¿«é€Ÿä¸Šçº¿ä¸”åŠŸèƒ½å®Œå–„ï¼Œæˆ‘ä¼šé€‰Djangoï¼›å¦‚æœæ˜¯å¾®æœåŠ¡æ¶æ„æˆ–å¯¹æ€§èƒ½è¦æ±‚é«˜ï¼Œæˆ‘ä¼šé€‰Flaské…åˆSQLAlchemyã€‚",
        "æˆ‘æ¯å‘¨ä¼šèŠ±æ—¶é—´å­¦ä¹ æ–°æŠ€æœ¯ï¼Œæœ€è¿‘åœ¨å­¦ä¹ Kuberneteså’Œäº‘åŸç”Ÿç›¸å…³çš„æŠ€æœ¯ã€‚å¹³æ—¶ä¼šå…³æ³¨æŠ€æœ¯åšå®¢ã€å‚åŠ æŠ€æœ¯ç¤¾åŒºæ´»åŠ¨ã€‚æˆ‘è§‰å¾—ä¿æŒå­¦ä¹ å¾ˆé‡è¦ï¼Œå°¤å…¶æ˜¯åç«¯æŠ€æœ¯æ›´æ–°å¾ˆå¿«ã€‚",
        "æˆ‘å¸Œæœ›æœªæ¥3å¹´èƒ½æ·±å…¥å¾®æœåŠ¡æ¶æ„å’Œåˆ†å¸ƒå¼ç³»ç»Ÿï¼Œ5å¹´åå¸Œæœ›èƒ½æˆä¸ºæŠ€æœ¯ä¸“å®¶æˆ–æ¶æ„å¸ˆã€‚æˆ‘å¯¹æŠ€æœ¯å¾ˆæœ‰çƒ­æƒ…ï¼Œä¹Ÿæ„¿æ„å¸¦å›¢é˜Ÿåˆ†äº«ç»éªŒã€‚",
        "æˆ‘åœ¨é‡‘èé¡¹ç›®ä¸Šçº¿å‰ä¸€å‘¨å‘ç°äº†ä¸€ä¸ªä¸¥é‡çš„å¹¶å‘bugï¼Œå‹åŠ›å¾ˆå¤§ã€‚ä½†æˆ‘æ²¡æœ‰æ…Œï¼Œç»„ç»‡å›¢é˜Ÿç´§æ€¥æ’æŸ¥ï¼Œè¿ç»­åŠ ç­ä¸‰å¤©å®šä½å¹¶ä¿®å¤äº†é—®é¢˜ã€‚æœ€ç»ˆé¡¹ç›®æŒ‰æ—¶ä¸Šçº¿ï¼Œè¿™æ¬¡ç»å†è®©æˆ‘å­¦ä¼šäº†åœ¨å‹åŠ›ä¸‹ä¿æŒå†·é™å’Œé«˜æ•ˆã€‚",
        "è¿‡å»ä¸€å¹´æˆ‘ä¸»è¦æ”»å…‹äº†åˆ†å¸ƒå¼äº‹åŠ¡ä¸€è‡´æ€§çš„é—®é¢˜ã€‚æˆ‘èŠ±äº†å¾ˆå¤šæ—¶é—´ç ”ç©¶Sagaæ¨¡å¼å’ŒTCCæ¨¡å¼ï¼Œæœ€ååœ¨é¡¹ç›®ä¸­æˆåŠŸå®ç°äº†å¯é çš„åˆ†å¸ƒå¼äº‹åŠ¡å¤„ç†ã€‚è¿™ä¸ªè¿‡ç¨‹è®©æˆ‘å¯¹åˆ†å¸ƒå¼ç³»ç»Ÿæœ‰äº†æ›´æ·±çš„ç†è§£ã€‚",
        "æˆ‘è§‰å¾—è¿™ä¸ªå²—ä½å¾ˆé€‚åˆæˆ‘çš„æŠ€æœ¯æ ˆï¼Œå¾®æœåŠ¡æ¶æ„å’Œé«˜å¹¶å‘æ­£æ˜¯æˆ‘æ“…é•¿çš„é¢†åŸŸã€‚æˆ‘çš„ä¼˜åŠ¿æ˜¯æœ‰å®é™…çš„å¤§å‹é¡¹ç›®ç»éªŒï¼Œèƒ½å¿«é€Ÿå®šä½å’Œè§£å†³é—®é¢˜ã€‚",
        "æˆ‘æœŸæœ›çš„è–ªèµ„åœ¨25-30Kä¹‹é—´ã€‚è¿™æ˜¯åŸºäºæˆ‘5å¹´çš„å·¥ä½œç»éªŒã€æŠ€æœ¯èƒ½åŠ›å’Œå¸‚åœºè¡Œæƒ…æ¥è€ƒè™‘çš„ã€‚æˆ‘ä¹‹å‰çš„è–ªèµ„æ˜¯22Kï¼Œå¸Œæœ›èƒ½æœ‰ä¸€å®šçš„æå‡ã€‚",
        "æˆ‘æƒ³äº†è§£ä¸€ä¸‹å›¢é˜Ÿè§„æ¨¡ã€æŠ€æœ¯æ ˆçš„é€‰å‹ï¼Œä»¥åŠæ—¥å¸¸å¼€å‘æµç¨‹æ˜¯æ€æ ·çš„ï¼Ÿ"
    ]
    
    print("=" * 60)
    print("å¼€å§‹é¢è¯•æµ‹è¯•")
    print("=" * 60)
    
    interview_over = False
    round_count = 0
    
    while not interview_over and round_count < len(user_answers) + 1:
        round_count += 1
        print(f"\n{'='*60}")
        print(f"ç¬¬ {round_count} è½®å¯¹è¯")
        print(f"{'='*60}")
        
        # å‘é€è¯·æ±‚
        print(f"\nğŸ“¤ å‘é€è¯·æ±‚...")
        print(f"dialogHistoryé•¿åº¦: {len(request_data['dialogHistory'])}")
        
        try:
            response = requests.post(
                API_URL,
                json=request_data,
                headers={"Content-Type": "application/json"},
                stream=True,
                timeout=60
            )
            
            if response.status_code != 200:
                print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
                print(response.text)
                break
            
            # è§£ææµå¼å“åº”
            ai_response = ""
            print("\nğŸ¤– AIé¢è¯•å®˜å›å¤:")
            print("-" * 60)
            
            for line in response.iter_lines():
                if line:
                    line_str = line.decode('utf-8')
                    if line_str.startswith('data: '):
                        data_str = line_str[6:]  # å»æ‰ "data: " å‰ç¼€
                        try:
                            data = json.loads(data_str)
                            if data['type'] == 'chunk':
                                content = data['content']
                                print(content, end='', flush=True)
                                ai_response += content
                            elif data['type'] == 'DONE':
                                print("\n" + "-" * 60)
                            elif data['type'] == 'over':
                                print("\n\nâœ… é¢è¯•ç»“æŸ!")
                                interview_over = True
                            elif data['type'] == 'error':
                                print(f"\n\nâŒ é”™è¯¯: {data['content']}")
                                interview_over = True
                        except json.JSONDecodeError:
                            pass
            
            # å¦‚æœé¢è¯•è¿˜æ²¡ç»“æŸï¼Œæ·»åŠ AIå›å¤åˆ°å¯¹è¯å†å²
            if not interview_over and ai_response:
                request_data['dialogHistory'].append({
                    "role": "assistant",
                    "content": ai_response
                })
                
                # å¦‚æœè¿˜æœ‰é¢„è®¾çš„ç”¨æˆ·å›ç­”ï¼Œæ·»åŠ åˆ°å¯¹è¯å†å²
                if round_count - 1 < len(user_answers):
                    user_answer = user_answers[round_count - 1]
                    print(f"\nğŸ‘¤ ç”¨æˆ·å›ç­”:")
                    print("-" * 60)
                    print(user_answer)
                    print("-" * 60)
                    
                    request_data['dialogHistory'].append({
                        "role": "user",
                        "content": user_answer
                    })
                    
                    # ç­‰å¾…ä¸€ä¸‹ï¼Œæ¨¡æ‹ŸçœŸå®å¯¹è¯é—´éš”
                    time.sleep(1)
                else:
                    print("\nâš ï¸ ç”¨æˆ·å›ç­”å·²ç”¨å®Œï¼Œæµ‹è¯•ç»“æŸ")
                    break
        
        except requests.exceptions.RequestException as e:
            print(f"\nâŒ è¯·æ±‚å¼‚å¸¸: {e}")
            break
    
    print("\n" + "=" * 60)
    print("é¢è¯•æµ‹è¯•å®Œæˆ")
    print(f"æ€»è½®æ•°: {round_count}")
    print(f"å¯¹è¯å†å²é•¿åº¦: {len(request_data['dialogHistory'])}")
    print("=" * 60)
    
    # ä¿å­˜å®Œæ•´å¯¹è¯å†å²åˆ°æ–‡ä»¶
    with open('interview_history.json', 'w', encoding='utf-8') as f:
        json.dump(request_data['dialogHistory'], f, ensure_ascii=False, indent=2)
    print("\nğŸ’¾ å®Œæ•´å¯¹è¯å†å²å·²ä¿å­˜åˆ°: interview_history.json")


if __name__ == "__main__":
    test_interview()
```

---

### æ–¹æ¡ˆ3ï¼šåˆ›å»ºç®€å•çš„Webç•Œé¢ï¼ˆæ›´ç›´è§‚ï¼‰

å¦‚æœä½ æƒ³æ›´ç›´è§‚åœ°æµ‹è¯•ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªç®€å•çš„HTMLé¡µé¢ï¼š

```html
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>é¢è¯•æµ‹è¯•ç•Œé¢</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 50px auto;
            padding: 20px;
        }
        .chat-container {
            border: 1px solid #ccc;
            height: 400px;
            overflow-y: auto;
            padding: 15px;
            margin-bottom: 20px;
            background: #f9f9f9;
        }
        .message {
            margin: 10px 0;
            padding: 10px;
            border-radius: 5px;
        }
        .ai-message {
            background: #e3f2fd;
            text-align: left;
        }
        .user-message {
            background: #c8e6c9;
            text-align: right;
        }
        .input-area {
            display: flex;
            gap: 10px;
        }
        input {
            flex: 1;
            padding: 10px;
            font-size: 14px;
        }
        button {
            padding: 10px 20px;
            background: #2196F3;
            color: white;
            border: none;
            cursor: pointer;
            border-radius: 4px;
        }
        button:hover {
            background: #1976D2;
        }
        button:disabled {
            background: #ccc;
            cursor: not-allowed;
        }
        .info {
            color: #666;
            font-size: 12px;
            margin-bottom: 10px;
        }
    </style>
</head>
<body>
    <h1>ğŸ¤ æ¨¡æ‹Ÿé¢è¯•æµ‹è¯•</h1>
    
    <div class="info">
        <strong>é¢è¯•ID:</strong> <span id="interviewId">test_interview_001</span> | 
        <strong>å¯¹è¯è½®æ•°:</strong> <span id="roundCount">0</span>
    </div>
    
    <div class="chat-container" id="chatContainer"></div>
    
    <div class="input-area">
        <input type="text" id="userInput" placeholder="è¾“å…¥ä½ çš„å›ç­”..." />
        <button onclick="sendMessage()" id="sendBtn">å‘é€</button>
        <button onclick="startInterview()" id="startBtn">å¼€å§‹é¢è¯•</button>
    </div>

    <script>
        const API_URL = 'http://localhost:8000/virturlInterview';  // ä¿®æ”¹ä¸ºä½ çš„API
        
        let dialogHistory = [];
        let roundCount = 0;
        let isProcessing = false;
        
        const interviewData = {
            id: "test_interview_001",
            userPortrait: "æˆ‘å«å¼ ä¸‰ï¼Œ5å¹´Pythonåç«¯å¼€å‘ç»éªŒï¼Œç†Ÿæ‚‰Djangoã€Flaskæ¡†æ¶ï¼Œåšè¿‡ç”µå•†å’Œé‡‘èç³»ç»Ÿ",
            jdTitle: "é«˜çº§Pythonå·¥ç¨‹å¸ˆ",
            jdPortrait: "è´Ÿè´£åç«¯APIå¼€å‘ï¼Œè¦æ±‚ç†Ÿæ‚‰å¾®æœåŠ¡æ¶æ„ï¼Œæœ‰é«˜å¹¶å‘ç³»ç»Ÿç»éªŒ",
            interviewType: 1,
            interviewLanguage: "zhongwen"
        };
        
        function addMessage(content, isUser = false) {
            const chatContainer = document.getElementById('chatContainer');
            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${isUser ? 'user-message' : 'ai-message'}`;
            messageDiv.textContent = content;
            chatContainer.appendChild(messageDiv);
            chatContainer.scrollTop = chatContainer.scrollHeight;
        }
        
        async function startInterview() {
            if (isProcessing) return;
            
            dialogHistory = [];
            roundCount = 0;
            document.getElementById('chatContainer').innerHTML = '';
            document.getElementById('roundCount').textContent = '0';
            
            await callAPI();
        }
        
        async function sendMessage() {
            const input = document.getElementById('userInput');
            const userAnswer = input.value.trim();
            
            if (!userAnswer || isProcessing) return;
            
            addMessage(userAnswer, true);
            dialogHistory.push({
                role: "user",
                content: userAnswer
            });
            
            input.value = '';
            await callAPI();
        }
        
        async function callAPI() {
            isProcessing = true;
            document.getElementById('sendBtn').disabled = true;
            document.getElementById('startBtn').disabled = true;
            
            try {
                const response = await fetch(API_URL, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({
                        ...interviewData,
                        dialogHistory: dialogHistory
                    })
                });
                
                const reader = response.body.getReader();
                const decoder = new TextDecoder();
                let aiResponse = '';
                
                while (true) {
                    const {done, value} = await reader.read();
                    if (done) break;
                    
                    const chunk = decoder.decode(value);
                    const lines = chunk.split('\n');
                    
                    for (const line of lines) {
                        if (line.startsWith('data: ')) {
                            const dataStr = line.substring(6);
                            try {
                                const data = JSON.parse(dataStr);
                                if (data.type === 'chunk') {
                                    aiResponse += data.content;
                                } else if (data.type === 'over') {
                                    alert('é¢è¯•ç»“æŸï¼');
                                }
                            } catch (e) {}
                        }
                    }
                }
                
                if (aiResponse) {
                    addMessage(aiResponse, false);
                    dialogHistory.push({
                        role: "assistant",
                        content: aiResponse
                    });
                    roundCount++;
                    document.getElementById('roundCount').textContent = roundCount;
                }
                
            } catch (error) {
                console.error('Error:', error);
                alert('è¯·æ±‚å¤±è´¥: ' + error.message);
            } finally {
                isProcessing = false;
                document.getElementById('sendBtn').disabled = false;
                document.getElementById('startBtn').disabled = false;
            }
        }
        
        document.getElementById('userInput').addEventListener('keypress', (e) => {
            if (e.key === 'Enter') {
                sendMessage();
            }
        });
    </script>
</body>
</html>
```

---

## ğŸ“Š æµ‹è¯•æ£€æŸ¥æ¸…å•

### 1. é¦–æ¬¡è°ƒç”¨æµ‹è¯•
- [ ] `dialogHistory` ä¸ºç©ºæ—¶èƒ½æ­£å¸¸ç”Ÿæˆé¢è¯•è®¡åˆ’
- [ ] è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜
- [ ] æ—¥å¿—ä¸­æ˜¾ç¤º"é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±Xä¸ªé—®é¢˜"

### 2. é¢è¯•è®¡åˆ’è´¨é‡æµ‹è¯•
- [ ] é—®é¢˜æ•°é‡åœ¨10-12ä¸ªä¹‹é—´
- [ ] åŒ…å«ç ´å†°ã€æ ¸å¿ƒæ¢ç´¢ã€æ”¶å°¾ä¸‰ä¸ªé˜¶æ®µ
- [ ] è¦†ç›–5ä¸ªç»´åº¦ï¼ˆä¸“ä¸šæŠ€èƒ½ã€èŒä¸šè§„åˆ’ã€è½¯ç¡¬èƒ½åŠ›ã€è–ªé…¬ã€æ€§æ ¼ï¼‰
- [ ] é—®é¢˜ä¸å€™é€‰äººèƒŒæ™¯ç›¸å…³
- [ ] é—®é¢˜å£è¯­åŒ–ã€è‡ªç„¶

### 3. å¤šè½®å¯¹è¯æµ‹è¯•
- [ ] ç­–ç•¥å±‚èƒ½æ”¶åˆ°é¢è¯•è®¡åˆ’ä¿¡æ¯
- [ ] æ‰§è¡Œå±‚èƒ½å‚è€ƒè®¡åˆ’ç”Ÿæˆå›å¤
- [ ] `current_question_index` æ­£å¸¸é€’å¢
- [ ] AIå›å¤ç¬¦åˆé¢è¯•æµç¨‹

### 4. çµæ´»æ€§æµ‹è¯•
- [ ] AIèƒ½æ ¹æ®ç”¨æˆ·å›ç­”çµæ´»è°ƒæ•´
- [ ] ä¸æ˜¯æœºæ¢°åœ°æŒ‰é¡ºåºæé—®
- [ ] èƒ½é€‚å½“è¿½é—®æˆ–è·³è¿‡

---

## ğŸ¯ æˆ‘çš„æ¨è

**æ¨èä½¿ç”¨æ–¹æ¡ˆ2ï¼ˆPythonæµ‹è¯•è„šæœ¬ï¼‰**ï¼Œå› ä¸ºï¼š

âœ… å¯ä»¥è‡ªåŠ¨åŒ–æµ‹è¯•å®Œæ•´æµç¨‹  
âœ… å¯ä»¥ä¿å­˜å¯¹è¯å†å²æ–¹ä¾¿åˆ†æ  
âœ… å¯ä»¥é‡å¤æµ‹è¯•ï¼Œè°ƒæ•´ç”¨æˆ·å›ç­”  
âœ… å¯ä»¥å¿«é€Ÿè¿­ä»£æµ‹è¯•ä¸åŒåœºæ™¯  

ä½ è§‰å¾—å“ªä¸ªæ–¹æ¡ˆæ›´é€‚åˆä½ ï¼Ÿéœ€è¦æˆ‘å¸®ä½ å®Œå–„æµ‹è¯•è„šæœ¬å—ï¼Ÿ

```plaintext
ç¬¬1æ¬¡è¯·æ±‚ â†’ é¦–æ¬¡è°ƒç”¨ï¼Œç”Ÿæˆè®¡åˆ’ + è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜
  â†“ï¼ˆå¤åˆ¶è¿”å›çš„é—®é¢˜ï¼‰
ç¬¬2æ¬¡è¯·æ±‚ â†’ æ„é€ dialogHistoryï¼ŒåŒ…å«AIçš„ç¬¬ä¸€ä¸ªé—®é¢˜ + ç”¨æˆ·å›ç­”
  â†“ï¼ˆç»§ç»­æ·»åŠ åˆ°dialogHistoryï¼‰
ç¬¬3æ¬¡è¯·æ±‚ â†’ ç»§ç»­å¯¹è¯...
  â†“
...æŒç»­åˆ°é¢è¯•ç»“æŸ
```

```json
POST http://your-api/virturlInterview

{
  "id": "test_interview_001",
  "userPortrait": "æˆ‘å«å¼ ä¸‰ï¼Œ5å¹´Pythonåç«¯å¼€å‘ç»éªŒï¼Œç†Ÿæ‚‰Djangoã€Flaskæ¡†æ¶ï¼Œåšè¿‡ç”µå•†å’Œé‡‘èç³»ç»Ÿ",
  "jdTitle": "é«˜çº§Pythonå·¥ç¨‹å¸ˆ",
  "jdPortrait": "è´Ÿè´£åç«¯APIå¼€å‘ï¼Œè¦æ±‚ç†Ÿæ‚‰å¾®æœåŠ¡æ¶æ„ï¼Œæœ‰é«˜å¹¶å‘ç³»ç»Ÿç»éªŒ",
  "interviewType": 1,
  "interviewLanguage": "zhongwen",
  "dialogHistory": []
}
```

```plaintext
data: {"type": "chunk", "content": "å¼ ä¸‰ä½ å¥½ï¼çœ‹åˆ°ä½ æœ‰5å¹´Pythonå¼€å‘ç»éªŒ..."}
data: {"type": "DONE", "content": "[DONE]"}
```

```json
POST http://your-api/virturlInterview

{
  "id": "test_interview_001",
  "userPortrait": "æˆ‘å«å¼ ä¸‰ï¼Œ5å¹´Pythonåç«¯å¼€å‘ç»éªŒï¼Œç†Ÿæ‚‰Djangoã€Flaskæ¡†æ¶ï¼Œåšè¿‡ç”µå•†å’Œé‡‘èç³»ç»Ÿ",
  "jdTitle": "é«˜çº§Pythonå·¥ç¨‹å¸ˆ",
  "jdPortrait": "è´Ÿè´£åç«¯APIå¼€å‘ï¼Œè¦æ±‚ç†Ÿæ‚‰å¾®æœåŠ¡æ¶æ„ï¼Œæœ‰é«˜å¹¶å‘ç³»ç»Ÿç»éªŒ",
  "interviewType": 1,
  "interviewLanguage": "zhongwen",
  "dialogHistory": [
    {
      "role": "assistant",
      "content": "å¼ ä¸‰ä½ å¥½ï¼çœ‹åˆ°ä½ æœ‰5å¹´Pythonå¼€å‘ç»éªŒï¼Œèƒ½ç®€å•èŠèŠä½ æ˜¯æ€ä¹ˆå¼€å§‹æ¥è§¦Pythonçš„å—ï¼Ÿå½“åˆä¸ºä»€ä¹ˆé€‰æ‹©åç«¯å¼€å‘è¿™ä¸ªæ–¹å‘ï¼Ÿ"
    },
    {
      "role": "user",
      "content": "æˆ‘å¤§å­¦å­¦çš„æ˜¯è®¡ç®—æœºä¸“ä¸šï¼Œå¤§ä¸‰å¼€å§‹æ¥è§¦Pythonï¼Œè§‰å¾—è¯­æ³•ç®€æ´ï¼Œå¾ˆé€‚åˆåšåç«¯å¼€å‘ã€‚åæ¥å®ä¹ æ—¶åšäº†ä¸€ä¸ªDjangoé¡¹ç›®ï¼Œå°±çˆ±ä¸Šäº†åç«¯å¼€å‘ã€‚"
    }
  ]
}
```

```plaintext
data: {"type": "chunk", "content": "å¾ˆå¥½ï¼ä»å®ä¹ é¡¹ç›®å¼€å§‹ç§¯ç´¯ç»éªŒ..."}
```

```json
{
  "id": "test_interview_001",
  "userPortrait": "...",
  "jdTitle": "é«˜çº§Pythonå·¥ç¨‹å¸ˆ",
  "jdPortrait": "...",
  "interviewType": 1,
  "interviewLanguage": "zhongwen",
  "dialogHistory": [
    {"role": "assistant", "content": "ç¬¬ä¸€ä¸ªé—®é¢˜..."},
    {"role": "user", "content": "ç¬¬ä¸€ä¸ªå›ç­”..."},
    {"role": "assistant", "content": "ç¬¬äºŒä¸ªé—®é¢˜..."},
    {"role": "user", "content": "ç¬¬äºŒä¸ªå›ç­”..."}
  ]
}
```

```python
# test_interview.py
import requests
import json
import time

API_URL = "http://localhost:8000/virturlInterview"  # ä¿®æ”¹ä¸ºä½ çš„APIåœ°å€

def test_interview():
    """æµ‹è¯•å®Œæ•´çš„é¢è¯•æµç¨‹"""
    
    # åˆå§‹è¯·æ±‚æ•°æ®
    request_data = {
        "id": "test_interview_001",
        "userPortrait": "æˆ‘å«å¼ ä¸‰ï¼Œ5å¹´Pythonåç«¯å¼€å‘ç»éªŒï¼Œç†Ÿæ‚‰Djangoã€Flaskæ¡†æ¶ï¼Œåšè¿‡ç”µå•†å’Œé‡‘èç³»ç»Ÿ",
        "jdTitle": "é«˜çº§Pythonå·¥ç¨‹å¸ˆ",
        "jdPortrait": "è´Ÿè´£åç«¯APIå¼€å‘ï¼Œè¦æ±‚ç†Ÿæ‚‰å¾®æœåŠ¡æ¶æ„ï¼Œæœ‰é«˜å¹¶å‘ç³»ç»Ÿç»éªŒ",
        "interviewType": 1,
        "interviewLanguage": "zhongwen",
        "dialogHistory": []
    }
    
    # é¢„è®¾çš„ç”¨æˆ·å›ç­”ï¼ˆæ¨¡æ‹ŸçœŸå®å¯¹è¯ï¼‰
    user_answers = [
        "æˆ‘å¤§å­¦å­¦çš„æ˜¯è®¡ç®—æœºä¸“ä¸šï¼Œå¤§ä¸‰å¼€å§‹æ¥è§¦Pythonï¼Œè§‰å¾—è¯­æ³•ç®€æ´ã€‚åæ¥å®ä¹ æ—¶åšäº†ä¸€ä¸ªDjangoé¡¹ç›®ï¼Œå°±çˆ±ä¸Šäº†åç«¯å¼€å‘ã€‚",
        "æˆ‘æœ€æœ‰æˆå°±æ„Ÿçš„æ˜¯åœ¨é‡‘èå…¬å¸åšçš„æ”¯ä»˜ç³»ç»Ÿé‡æ„é¡¹ç›®ã€‚å½“æ—¶ç³»ç»Ÿå“åº”æ…¢ï¼Œé«˜å³°æœŸç»å¸¸è¶…æ—¶ã€‚æˆ‘è´Ÿè´£ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢å’Œå¼•å…¥Redisç¼“å­˜ï¼Œæœ€ç»ˆå°†å“åº”æ—¶é—´ä»2ç§’é™åˆ°200æ¯«ç§’ä»¥å†…ã€‚",
        "æˆ‘ä¼šå…ˆé€šè¿‡æ—¥å¿—å®šä½é—®é¢˜å‡ºç°çš„ä½ç½®ï¼Œç„¶åæ£€æŸ¥æ”¯ä»˜æ¥å£çš„äº‹åŠ¡å¤„ç†é€»è¾‘ã€‚å¯èƒ½æ˜¯æ”¯ä»˜æˆåŠŸåçš„å›è°ƒå¤„ç†å¤±è´¥ï¼Œæˆ–è€…æ¶ˆæ¯é˜Ÿåˆ—å»¶è¿Ÿå¯¼è‡´è®¢å•çŠ¶æ€æ›´æ–°ä¸åŠæ—¶ã€‚æˆ‘ä¼šå…ˆå›æ»šæ•°æ®ä¿®å¤ç”¨æˆ·è®¢å•ï¼Œå†ä¿®å¤æ ¹æœ¬é—®é¢˜ã€‚",
        "Djangoé€‚åˆå¿«é€Ÿå¼€å‘ï¼ŒORMå¾ˆæ–¹ä¾¿ï¼Œé€‚åˆä¼ ç»ŸWebåº”ç”¨ã€‚Flaskæ›´è½»é‡ï¼Œé€‚åˆå¾®æœåŠ¡å’ŒAPIå¼€å‘ã€‚å¦‚æœé¡¹ç›®éœ€è¦å¿«é€Ÿä¸Šçº¿ä¸”åŠŸèƒ½å®Œå–„ï¼Œæˆ‘ä¼šé€‰Djangoï¼›å¦‚æœæ˜¯å¾®æœåŠ¡æ¶æ„æˆ–å¯¹æ€§èƒ½è¦æ±‚é«˜ï¼Œæˆ‘ä¼šé€‰Flaské…åˆSQLAlchemyã€‚",
        "æˆ‘æ¯å‘¨ä¼šèŠ±æ—¶é—´å­¦ä¹ æ–°æŠ€æœ¯ï¼Œæœ€è¿‘åœ¨å­¦ä¹ Kuberneteså’Œäº‘åŸç”Ÿç›¸å…³çš„æŠ€æœ¯ã€‚å¹³æ—¶ä¼šå…³æ³¨æŠ€æœ¯åšå®¢ã€å‚åŠ æŠ€æœ¯ç¤¾åŒºæ´»åŠ¨ã€‚æˆ‘è§‰å¾—ä¿æŒå­¦ä¹ å¾ˆé‡è¦ï¼Œå°¤å…¶æ˜¯åç«¯æŠ€æœ¯æ›´æ–°å¾ˆå¿«ã€‚",
        "æˆ‘å¸Œæœ›æœªæ¥3å¹´èƒ½æ·±å…¥å¾®æœåŠ¡æ¶æ„å’Œåˆ†å¸ƒå¼ç³»ç»Ÿï¼Œ5å¹´åå¸Œæœ›èƒ½æˆä¸ºæŠ€æœ¯ä¸“å®¶æˆ–æ¶æ„å¸ˆã€‚æˆ‘å¯¹æŠ€æœ¯å¾ˆæœ‰çƒ­æƒ…ï¼Œä¹Ÿæ„¿æ„å¸¦å›¢é˜Ÿåˆ†äº«ç»éªŒã€‚",
        "æˆ‘åœ¨é‡‘èé¡¹ç›®ä¸Šçº¿å‰ä¸€å‘¨å‘ç°äº†ä¸€ä¸ªä¸¥é‡çš„å¹¶å‘bugï¼Œå‹åŠ›å¾ˆå¤§ã€‚ä½†æˆ‘æ²¡æœ‰æ…Œï¼Œç»„ç»‡å›¢é˜Ÿç´§æ€¥æ’æŸ¥ï¼Œè¿ç»­åŠ ç­ä¸‰å¤©å®šä½å¹¶ä¿®å¤äº†é—®é¢˜ã€‚æœ€ç»ˆé¡¹ç›®æŒ‰æ—¶ä¸Šçº¿ï¼Œè¿™æ¬¡ç»å†è®©æˆ‘å­¦ä¼šäº†åœ¨å‹åŠ›ä¸‹ä¿æŒå†·é™å’Œé«˜æ•ˆã€‚",
        "è¿‡å»ä¸€å¹´æˆ‘ä¸»è¦æ”»å…‹äº†åˆ†å¸ƒå¼äº‹åŠ¡ä¸€è‡´æ€§çš„é—®é¢˜ã€‚æˆ‘èŠ±äº†å¾ˆå¤šæ—¶é—´ç ”ç©¶Sagaæ¨¡å¼å’ŒTCCæ¨¡å¼ï¼Œæœ€ååœ¨é¡¹ç›®ä¸­æˆåŠŸå®ç°äº†å¯é çš„åˆ†å¸ƒå¼äº‹åŠ¡å¤„ç†ã€‚è¿™ä¸ªè¿‡ç¨‹è®©æˆ‘å¯¹åˆ†å¸ƒå¼ç³»ç»Ÿæœ‰äº†æ›´æ·±çš„ç†è§£ã€‚",
        "æˆ‘è§‰å¾—è¿™ä¸ªå²—ä½å¾ˆé€‚åˆæˆ‘çš„æŠ€æœ¯æ ˆï¼Œå¾®æœåŠ¡æ¶æ„å’Œé«˜å¹¶å‘æ­£æ˜¯æˆ‘æ“…é•¿çš„é¢†åŸŸã€‚æˆ‘çš„ä¼˜åŠ¿æ˜¯æœ‰å®é™…çš„å¤§å‹é¡¹ç›®ç»éªŒï¼Œèƒ½å¿«é€Ÿå®šä½å’Œè§£å†³é—®é¢˜ã€‚",
        "æˆ‘æœŸæœ›çš„è–ªèµ„åœ¨25-30Kä¹‹é—´ã€‚è¿™æ˜¯åŸºäºæˆ‘5å¹´çš„å·¥ä½œç»éªŒã€æŠ€æœ¯èƒ½åŠ›å’Œå¸‚åœºè¡Œæƒ…æ¥è€ƒè™‘çš„ã€‚æˆ‘ä¹‹å‰çš„è–ªèµ„æ˜¯22Kï¼Œå¸Œæœ›èƒ½æœ‰ä¸€å®šçš„æå‡ã€‚",
        "æˆ‘æƒ³äº†è§£ä¸€ä¸‹å›¢é˜Ÿè§„æ¨¡ã€æŠ€æœ¯æ ˆçš„é€‰å‹ï¼Œä»¥åŠæ—¥å¸¸å¼€å‘æµç¨‹æ˜¯æ€æ ·çš„ï¼Ÿ"
    ]
    
    print("=" * 60)
    print("å¼€å§‹é¢è¯•æµ‹è¯•")
    print("=" * 60)
    
    interview_over = False
    round_count = 0
    
    while not interview_over and round_count < len(user_answers) + 1:
        round_count += 1
        print(f"\n{'='*60}")
        print(f"ç¬¬ {round_count} è½®å¯¹è¯")
        print(f"{'='*60}")
        
        # å‘é€è¯·æ±‚
        print(f"\nğŸ“¤ å‘é€è¯·æ±‚...")
        print(f"dialogHistoryé•¿åº¦: {len(request_data['dialogHistory'])}")
        
        try:
            response = requests.post(
                API_URL,
                json=request_data,
                headers={"Content-Type": "application/json"},
                stream=True,
                timeout=60
            )
            
            if response.status_code != 200:
                print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
                print(response.text)
                break
            
            # è§£ææµå¼å“åº”
            ai_response = ""
            print("\nğŸ¤– AIé¢è¯•å®˜å›å¤:")
            print("-" * 60)
            
            for line in response.iter_lines():
                if line:
                    line_str = line.decode('utf-8')
                    if line_str.startswith('data: '):
                        data_str = line_str[6:]  # å»æ‰ "data: " å‰ç¼€
                        try:
                            data = json.loads(data_str)
                            if data['type'] == 'chunk':
                                content = data['content']
                                print(content, end='', flush=True)
                                ai_response += content
                            elif data['type'] == 'DONE':
                                print("\n" + "-" * 60)
                            elif data['type'] == 'over':
                                print("\n\nâœ… é¢è¯•ç»“æŸ!")
                                interview_over = True
                            elif data['type'] == 'error':
                                print(f"\n\nâŒ é”™è¯¯: {data['content']}")
                                interview_over = True
                        except json.JSONDecodeError:
                            pass
            
            # å¦‚æœé¢è¯•è¿˜æ²¡ç»“æŸï¼Œæ·»åŠ AIå›å¤åˆ°å¯¹è¯å†å²
            if not interview_over and ai_response:
                request_data['dialogHistory'].append({
                    "role": "assistant",
                    "content": ai_response
                })
                
                # å¦‚æœè¿˜æœ‰é¢„è®¾çš„ç”¨æˆ·å›ç­”ï¼Œæ·»åŠ åˆ°å¯¹è¯å†å²
                if round_count - 1 < len(user_answers):
                    user_answer = user_answers[round_count - 1]
                    print(f"\nğŸ‘¤ ç”¨æˆ·å›ç­”:")
                    print("-" * 60)
                    print(user_answer)
                    print("-" * 60)
                    
                    request_data['dialogHistory'].append({
                        "role": "user",
                        "content": user_answer
                    })
                    
                    # ç­‰å¾…ä¸€ä¸‹ï¼Œæ¨¡æ‹ŸçœŸå®å¯¹è¯é—´éš”
                    time.sleep(1)
                else:
                    print("\nâš ï¸ ç”¨æˆ·å›ç­”å·²ç”¨å®Œï¼Œæµ‹è¯•ç»“æŸ")
                    break
        
        except requests.exceptions.RequestException as e:
            print(f"\nâŒ è¯·æ±‚å¼‚å¸¸: {e}")
            break
    
    print("\n" + "=" * 60)
    print("é¢è¯•æµ‹è¯•å®Œæˆ")
    print(f"æ€»è½®æ•°: {round_count}")
    print(f"å¯¹è¯å†å²é•¿åº¦: {len(request_data['dialogHistory'])}")
    print("=" * 60)
    
    # ä¿å­˜å®Œæ•´å¯¹è¯å†å²åˆ°æ–‡ä»¶
    with open('interview_history.json', 'w', encoding='utf-8') as f:
        json.dump(request_data['dialogHistory'], f, ensure_ascii=False, indent=2)
    print("\nğŸ’¾ å®Œæ•´å¯¹è¯å†å²å·²ä¿å­˜åˆ°: interview_history.json")


if __name__ == "__main__":
    test_interview()
```

```html
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>é¢è¯•æµ‹è¯•ç•Œé¢</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 50px auto;
            padding: 20px;
        }
        .chat-container {
            border: 1px solid #ccc;
            height: 400px;
            overflow-y: auto;
            padding: 15px;
            margin-bottom: 20px;
            background: #f9f9f9;
        }
        .message {
            margin: 10px 0;
            padding: 10px;
            border-radius: 5px;
        }
        .ai-message {
            background: #e3f2fd;
            text-align: left;
        }
        .user-message {
            background: #c8e6c9;
            text-align: right;
        }
        .input-area {
            display: flex;
            gap: 10px;
        }
        input {
            flex: 1;
            padding: 10px;
            font-size: 14px;
        }
        button {
            padding: 10px 20px;
            background: #2196F3;
            color: white;
            border: none;
            cursor: pointer;
            border-radius: 4px;
        }
        button:hover {
            background: #1976D2;
        }
        button:disabled {
            background: #ccc;
            cursor: not-allowed;
        }
        .info {
            color: #666;
            font-size: 12px;
            margin-bottom: 10px;
        }
    </style>
</head>
<body>
    <h1>ğŸ¤ æ¨¡æ‹Ÿé¢è¯•æµ‹è¯•</h1>
    
    <div class="info">
        <strong>é¢è¯•ID:</strong> <span id="interviewId">test_interview_001</span> | 
        <strong>å¯¹è¯è½®æ•°:</strong> <span id="roundCount">0</span>
    </div>
    
    <div class="chat-container" id="chatContainer"></div>
    
    <div class="input-area">
        <input type="text" id="userInput" placeholder="è¾“å…¥ä½ çš„å›ç­”..." />
        <button onclick="sendMessage()" id="sendBtn">å‘é€</button>
        <button onclick="startInterview()" id="startBtn">å¼€å§‹é¢è¯•</button>
    </div>

    <script>
        const API_URL = 'http://localhost:8000/virturlInterview';  // ä¿®æ”¹ä¸ºä½ çš„API
        
        let dialogHistory = [];
        let roundCount = 0;
        let isProcessing = false;
        
        const interviewData = {
            id: "test_interview_001",
            userPortrait: "æˆ‘å«å¼ ä¸‰ï¼Œ5å¹´Pythonåç«¯å¼€å‘ç»éªŒï¼Œç†Ÿæ‚‰Djangoã€Flaskæ¡†æ¶ï¼Œåšè¿‡ç”µå•†å’Œé‡‘èç³»ç»Ÿ",
            jdTitle: "é«˜çº§Pythonå·¥ç¨‹å¸ˆ",
            jdPortrait: "è´Ÿè´£åç«¯APIå¼€å‘ï¼Œè¦æ±‚ç†Ÿæ‚‰å¾®æœåŠ¡æ¶æ„ï¼Œæœ‰é«˜å¹¶å‘ç³»ç»Ÿç»éªŒ",
            interviewType: 1,
            interviewLanguage: "zhongwen"
        };
        
        function addMessage(content, isUser = false) {
            const chatContainer = document.getElementById('chatContainer');
            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${isUser ? 'user-message' : 'ai-message'}`;
            messageDiv.textContent = content;
            chatContainer.appendChild(messageDiv);
            chatContainer.scrollTop = chatContainer.scrollHeight;
        }
        
        async function startInterview() {
            if (isProcessing) return;
            
            dialogHistory = [];
            roundCount = 0;
            document.getElementById('chatContainer').innerHTML = '';
            document.getElementById('roundCount').textContent = '0';
            
            await callAPI();
        }
        
        async function sendMessage() {
            const input = document.getElementById('userInput');
            const userAnswer = input.value.trim();
            
            if (!userAnswer || isProcessing) return;
            
            addMessage(userAnswer, true);
            dialogHistory.push({
                role: "user",
                content: userAnswer
            });
            
            input.value = '';
            await callAPI();
        }
        
        async function callAPI() {
            isProcessing = true;
            document.getElementById('sendBtn').disabled = true;
            document.getElementById('startBtn').disabled = true;
            
            try {
                const response = await fetch(API_URL, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({
                        ...interviewData,
                        dialogHistory: dialogHistory
                    })
                });
                
                const reader = response.body.getReader();
                const decoder = new TextDecoder();
                let aiResponse = '';
                
                while (true) {
                    const {done, value} = await reader.read();
                    if (done) break;
                    
                    const chunk = decoder.decode(value);
                    const lines = chunk.split('\n');
                    
                    for (const line of lines) {
                        if (line.startsWith('data: ')) {
                            const dataStr = line.substring(6);
                            try {
                                const data = JSON.parse(dataStr);
                                if (data.type === 'chunk') {
                                    aiResponse += data.content;
                                } else if (data.type === 'over') {
                                    alert('é¢è¯•ç»“æŸï¼');
                                }
                            } catch (e) {}
                        }
                    }
                }
                
                if (aiResponse) {
                    addMessage(aiResponse, false);
                    dialogHistory.push({
                        role: "assistant",
                        content: aiResponse
                    });
                    roundCount++;
                    document.getElementById('roundCount').textContent = roundCount;
                }
                
            } catch (error) {
                console.error('Error:', error);
                alert('è¯·æ±‚å¤±è´¥: ' + error.message);
            } finally {
                isProcessing = false;
                document.getElementById('sendBtn').disabled = false;
                document.getElementById('startBtn').disabled = false;
            }
        }
        
        document.getElementById('userInput').addEventListener('keypress', (e) => {
            if (e.key === 'Enter') {
                sendMessage();
            }
        });
    </script>
</body>
</html>
```

---

**User**

@python (258-353) ä½ èƒ½ä¸èƒ½æŠŠçœ‹ä¸€ä¸‹è¿™ä¸ªç»ˆç«¯çš„è¾“å‡ºå‘€ï¼Ÿç„¶ååˆ©ç”¨postmanå‘é€äº†ä¸€ä¸ªè¯·æ±‚ï¼Œç„¶åç»ˆç«¯è¾“å‡ºå†…å®¹å¦‚ä¸‹æ‰€ç¤ºï¼Œ æ ¹æ®è¿™ä¸ªè¾“å‡ºæ¥å¸®æˆ‘åˆ¤æ–­ä¸€ä¸‹æˆ‘ä¹‹å‰è®¾ç½®çš„æç¤ºè¯ï¼Œå°¤å…¶æ˜¯ é¢è¯•é—®é¢˜ç”Ÿæˆçš„æç¤ºè¯æ˜¯å¦ç”Ÿæ•ˆäº†å‘¢ï¼Ÿæˆ‘æ€ä¹ˆçœ‹ä¸å‡ºæ¥å‘€ï¼Ÿèƒ½ä¸èƒ½å¸®æˆ‘åˆ¤æ–­ä¸€ä¸‹å‘€ï¼Ÿå¦‚ä½•åˆ¤æ–­å‘¢ï¼Ÿ 

---

**Cursor**

å¾ˆå¥½çš„é—®é¢˜ï¼è®©æˆ‘ä»”ç»†åˆ†æä¸€ä¸‹è¿™ä¸ªæ—¥å¿—è¾“å‡ºï¼Œå¸®ä½ åˆ¤æ–­é¢è¯•è®¡åˆ’ç”ŸæˆPromptæ˜¯å¦ç”Ÿæ•ˆäº†ã€‚

## ğŸ” æ—¥å¿—åˆ†æ

çœ‹è¿™ä¸ªæ—¥å¿—æˆ‘å‘ç°äº†ä¸€ä¸ª**å…³é”®é—®é¢˜**ï¼š

### âŒ é—®é¢˜ï¼šé¢è¯•è®¡åˆ’ç”ŸæˆPromptæ²¡æœ‰æ‰§è¡Œï¼

ä»æ—¥å¿—æ¥çœ‹ï¼š

```
ç¬¬258-260è¡Œï¼šæ˜¾ç¤ºè°ƒç”¨äº†"ç­–ç•¥å±‚" (strategy_white_collar.txt)
ç¬¬261è¡Œï¼šè°ƒç”¨äº†ç­–ç•¥å±‚çš„LLM
ç¬¬262-266è¡Œï¼šè¾“å‡ºäº†ç­–ç•¥å±‚çš„ç»“æœ
ç¬¬267è¡Œï¼šç›´æ¥è¿›å…¥äº†"æ‰§è¡Œå±‚" (executor_white_collar.txt)
```

**å…³é”®ç‚¹**ï¼šæ—¥å¿—ä¸­**æ²¡æœ‰çœ‹åˆ°**é¢è¯•è®¡åˆ’ç”Ÿæˆç›¸å…³çš„æ—¥å¿—ï¼Œæ¯”å¦‚ï¼š
```
é¢è¯•ID: test_interview_001 - é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±Xä¸ªé—®é¢˜
æˆ–è€…
é¢è¯•ID: test_interview_001 - ç”Ÿæˆé¢è¯•è®¡åˆ’å¤±è´¥
```

---

## ğŸ¤” ä¸ºä»€ä¹ˆæ²¡æœ‰æ‰§è¡Œï¼Ÿ

è®©æˆ‘å›é¡¾ä¸€ä¸‹ä»£ç é€»è¾‘ï¼š

```python
# ç¬¬156-168è¡Œ
if not dialog_history or len(dialog_history) == 0:
    # é¦–æ¬¡è°ƒç”¨ï¼šç”Ÿæˆé¢è¯•è®¡åˆ’
    await self.generate_interview_plan()
    ...
    return  # â† è¿™é‡Œåº”è¯¥è¿”å›ï¼Œä¸ç»§ç»­æ‰§è¡Œåé¢çš„ä»£ç 
```

ä½†ä»æ—¥å¿—æ¥çœ‹ï¼Œ**ä»£ç ç»§ç»­æ‰§è¡Œåˆ°äº†ç­–ç•¥å±‚**ï¼Œè¿™è¯´æ˜ï¼š

### é—®é¢˜åŸå› åˆ†æï¼š

**å¯èƒ½çš„åŸå› 1**ï¼š`dialog_history` ä¸æ˜¯ç©ºçš„
- ä½ å‘é€è¯·æ±‚æ—¶ï¼Œ`dialogHistory` å¯èƒ½ä¸æ˜¯ç©ºæ•°ç»„ï¼Ÿ
- éœ€è¦æ£€æŸ¥ä¸€ä¸‹Postmançš„è¯·æ±‚ä½“

**å¯èƒ½çš„åŸå› 2**ï¼šåˆ¤æ–­æ¡ä»¶æ²¡æœ‰æ­£ç¡®ç”Ÿæ•ˆ
- `if not dialog_history` çš„åˆ¤æ–­å¯èƒ½æœ‰é—®é¢˜

---

## ğŸ” æ€ä¹ˆåˆ¤æ–­é¢è¯•è®¡åˆ’æ˜¯å¦ç”Ÿæ•ˆï¼Ÿ

### æ–¹æ³•1ï¼šæŸ¥çœ‹æ—¥å¿—ä¸­æ˜¯å¦æœ‰ç”Ÿæˆè®¡åˆ’çš„ä¿¡æ¯

åº”è¯¥çœ‹åˆ°ç±»ä¼¼è¿™æ ·çš„æ—¥å¿—ï¼š
```
é¢è¯•ID: test_interview_001 - é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±12ä¸ªé—®é¢˜
```

æˆ–è€…é”™è¯¯æ—¥å¿—ï¼š
```
é¢è¯•ID: test_interview_001 - ç”Ÿæˆé¢è¯•è®¡åˆ’å¤±è´¥: xxxxx
```

**ä½ ç°åœ¨çš„æ—¥å¿—ä¸­æ²¡æœ‰è¿™ä¸ªï¼Œè¯´æ˜ç”Ÿæˆè®¡åˆ’å‡½æ•°æ²¡æœ‰è¢«è°ƒç”¨ã€‚**

---

### æ–¹æ³•2ï¼šæ£€æŸ¥è¿”å›çš„ç¬¬ä¸€ä¸ªé—®é¢˜

å¦‚æœè®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œä½ åº”è¯¥æ”¶åˆ°çš„æ˜¯**æ¥è‡ªè®¡åˆ’çš„ç¬¬ä¸€ä¸ªé—®é¢˜**ï¼Œè€Œä¸æ˜¯ç°åœ¨çš„"ä½ å¥½ï¼Œæˆ‘æ˜¯æœ¬æ¬¡é¢è¯•çš„AIé¢è¯•å®˜..."

ç°åœ¨ä½ æ”¶åˆ°çš„å›å¤æ˜¯æ¥è‡ª**æ‰§è¡Œå±‚**çš„æ ‡å‡†å¼€åœºç™½ï¼Œè¯´æ˜ä»£ç èµ°è¿›äº†æ­£å¸¸æµç¨‹ï¼Œæ²¡æœ‰èµ°é¦–æ¬¡è°ƒç”¨çš„åˆ†æ”¯ã€‚

---

### æ–¹æ³•3ï¼šæ£€æŸ¥Postmançš„è¯·æ±‚ä½“

ä½ ç”¨Postmanå‘é€çš„è¯·æ±‚æ˜¯è¿™æ ·å—ï¼Ÿ

```json
{
  "id": "test_interview_001",
  "userPortrait": "...",
  "jdTitle": "...",
  "jdPortrait": "...",
  "interviewType": 1,
  "interviewLanguage": "zhongwen",
  "dialogHistory": []  â† ä¸€å®šè¦æ˜¯ç©ºæ•°ç»„ï¼
}
```

---

## ğŸ› é—®é¢˜å®šä½æ­¥éª¤

### æ­¥éª¤1ï¼šæ£€æŸ¥Postmanè¯·æ±‚

ä½ ç°åœ¨ç”¨Postmanå‘é€çš„è¯·æ±‚ä½“æ˜¯ä»€ä¹ˆï¼Ÿèƒ½å¦è´´å‡ºæ¥ï¼Ÿ

ç‰¹åˆ«æ³¨æ„ï¼š
- `dialogHistory` æ˜¯å¦çœŸçš„ä¸ºç©ºï¼Ÿ
- æˆ–è€…æ˜¯ `null`ï¼Ÿ
- è¿˜æ˜¯ `undefined`ï¼Ÿ

### æ­¥éª¤2ï¼šæ·»åŠ è°ƒè¯•æ—¥å¿—

å»ºè®®åœ¨ä»£ç ä¸­åŠ å…¥æ›´è¯¦ç»†çš„æ—¥å¿—æ¥å®šä½é—®é¢˜ï¼š

```python
async def get_ai_response(self, dialog_history):
    start_time = time.time()
    
    # æ·»åŠ è°ƒè¯•æ—¥å¿—
    logging.info(f"é¢è¯•ID: {self.interview_id} - dialog_historyç±»å‹: {type(dialog_history)}, é•¿åº¦: {len(dialog_history) if dialog_history else 0}, å†…å®¹: {dialog_history}")
    
    # åˆ¤æ–­æ˜¯å¦é¦–æ¬¡è°ƒç”¨
    if not dialog_history or len(dialog_history) == 0:
        logging.info(f"é¢è¯•ID: {self.interview_id} - æ£€æµ‹åˆ°é¦–æ¬¡è°ƒç”¨ï¼Œå¼€å§‹ç”Ÿæˆé¢è¯•è®¡åˆ’")
        # é¦–æ¬¡è°ƒç”¨ï¼šç”Ÿæˆé¢è¯•è®¡åˆ’
        await self.generate_interview_plan()
        ...
```

### æ­¥éª¤3ï¼šæŸ¥çœ‹æ–°å¢çš„æ—¥å¿—

é‡æ–°ç”¨Postmanå‘é€è¯·æ±‚ï¼ŒæŸ¥çœ‹æ˜¯å¦æœ‰è¿™æ¡æ—¥å¿—ï¼š
```
é¢è¯•ID: test_interview_001 - æ£€æµ‹åˆ°é¦–æ¬¡è°ƒç”¨ï¼Œå¼€å§‹ç”Ÿæˆé¢è¯•è®¡åˆ’
```

å¦‚æœæœ‰ï¼Œè¯´æ˜ä»£ç è¿›å…¥äº†é¦–æ¬¡è°ƒç”¨çš„åˆ†æ”¯ã€‚  
å¦‚æœæ²¡æœ‰ï¼Œè¯´æ˜ `dialog_history` ä¸ä¸ºç©ºã€‚

---

## ğŸ“ å¦‚ä½•æ­£ç¡®éªŒè¯é¢è¯•è®¡åˆ’ç”Ÿæ•ˆï¼Ÿ

### å®Œæ•´çš„éªŒè¯æ­¥éª¤ï¼š

**ç¬¬1æ­¥**ï¼šç¡®ä¿Postmanè¯·æ±‚ä½“æ­£ç¡®
```json
{
  "id": "test_interview_001",
  "userPortrait": "æˆ‘å«å¼ ä¸‰ï¼Œ5å¹´Pythonåç«¯å¼€å‘ç»éªŒï¼Œç†Ÿæ‚‰Djangoã€Flaskæ¡†æ¶ï¼Œåšè¿‡ç”µå•†å’Œé‡‘èç³»ç»Ÿ",
  "jdTitle": "é«˜çº§Pythonå·¥ç¨‹å¸ˆ",
  "jdPortrait": "è´Ÿè´£åç«¯APIå¼€å‘ï¼Œè¦æ±‚ç†Ÿæ‚‰å¾®æœåŠ¡æ¶æ„ï¼Œæœ‰é«˜å¹¶å‘ç³»ç»Ÿç»éªŒ",
  "interviewType": 1,
  "interviewLanguage": "zhongwen",
  "dialogHistory": []
}
```

**ç¬¬2æ­¥**ï¼šæŸ¥çœ‹ç»ˆç«¯æ—¥å¿—ï¼Œåº”è¯¥çœ‹åˆ°ï¼š
```
é¢è¯•ID: test_interview_001 - é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±12ä¸ªé—®é¢˜
```

**ç¬¬3æ­¥**ï¼šæ£€æŸ¥è¿”å›çš„å“åº”

åº”è¯¥æ”¶åˆ°ä¸€ä¸ªé—®é¢˜ï¼Œæ¯”å¦‚ï¼š
```
"ä½ å¥½ï¼Œæˆ‘æ˜¯æœ¬æ¬¡é¢è¯•çš„AIé¢è¯•å®˜......"
```

âŒ **ä¸åº”è¯¥æ˜¯**ï¼šç°åœ¨è¿™ç§å®Œæ•´çš„å¼€åœºç™½ã€‚

**ç¬¬4æ­¥**ï¼šç»§ç»­ç¬¬äºŒè½®å¯¹è¯

åœ¨dialogHistoryä¸­æ·»åŠ ï¼š
1. ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆæ¥è‡ªè®¡åˆ’ï¼‰
2. ç”¨æˆ·å›ç­”

ç„¶åå‘é€ç¬¬äºŒä¸ªè¯·æ±‚ï¼Œåº”è¯¥çœ‹åˆ°ï¼š
```
é¢è¯•ID: test_interview_001 - å½“å‰è¿›åº¦ï¼šå·²æé—® 1/12 ä¸ªé—®é¢˜
```

è¿™ä¸ªlogåœ¨ç­–ç•¥å±‚æˆ–æ‰§è¡Œå±‚çš„promptä¸­ä¼šå‡ºç°ã€‚

---

## ğŸ¯ å¿«é€Ÿè¯Šæ–­ï¼š3ä¸ªå…³é”®æŒ‡æ ‡

| æŒ‡æ ‡ | ç°åœ¨çš„æ—¥å¿— | åº”è¯¥çœ‹åˆ° | ç”Ÿæ•ˆäº†ï¼Ÿ |
|------|----------|--------|--------|
| **æ˜¯å¦ç”Ÿæˆè®¡åˆ’æ—¥å¿—** | âŒ æ²¡çœ‹åˆ° | âœ… "é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸ" | âŒ æ²¡ç”Ÿæ•ˆ |
| **é¦–æ¬¡è¿”å›çš„å†…å®¹** | âœ… æ ‡å‡†å¼€åœºç™½ | âŒ åº”è¯¥æ˜¯è®¡åˆ’ä¸­çš„ç¬¬ä¸€ä¸ªé—®é¢˜ | âŒ æ²¡ç”Ÿæ•ˆ |
| **ç­–ç•¥å±‚æ˜¯å¦æ”¶åˆ°plan_info** | â“ ä¸ç¡®å®š | âœ… åº”è¯¥åœ¨promptä¸­çœ‹åˆ°è®¡åˆ’ä¿¡æ¯ | â“ ä¸ç¡®å®š |

---

## ğŸ’¡ æˆ‘çš„åˆ¤æ–­

**ç›®å‰çš„æƒ…å†µ**ï¼š**é¢è¯•è®¡åˆ’ç”ŸæˆPromptæ²¡æœ‰ç”Ÿæ•ˆ**

**åŸå› **ï¼š`dialog_history` ä¸ä¸ºç©ºï¼Œå¯¼è‡´ä»£ç æ²¡æœ‰è¿›å…¥é¦–æ¬¡è°ƒç”¨çš„åˆ†æ”¯

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. æ£€æŸ¥Postmançš„è¯·æ±‚ä½“ï¼Œç¡®ä¿ `dialogHistory` æ˜¯ `[]`
2. å¦‚æœç¡®å®æ˜¯ç©ºï¼Œé‚£éœ€è¦æ£€æŸ¥è·¯ç”±å±‚æ˜¯å¦æ­£ç¡®ä¼ é€’å‚æ•°
3. æ·»åŠ è°ƒè¯•æ—¥å¿—æ¥å®šä½é—®é¢˜

---

éœ€è¦æˆ‘å¸®ä½ æ£€æŸ¥ä¸€ä¸‹è·¯ç”±å±‚çš„ä»£ç å—ï¼Ÿæˆ–è€…ä½ èƒ½å¦æŠŠPostmançš„è¯·æ±‚ä½“æˆªå›¾ç»™æˆ‘çœ‹ä¸€ä¸‹ï¼ŸğŸ˜Š

```plaintext
ç¬¬258-260è¡Œï¼šæ˜¾ç¤ºè°ƒç”¨äº†"ç­–ç•¥å±‚" (strategy_white_collar.txt)
ç¬¬261è¡Œï¼šè°ƒç”¨äº†ç­–ç•¥å±‚çš„LLM
ç¬¬262-266è¡Œï¼šè¾“å‡ºäº†ç­–ç•¥å±‚çš„ç»“æœ
ç¬¬267è¡Œï¼šç›´æ¥è¿›å…¥äº†"æ‰§è¡Œå±‚" (executor_white_collar.txt)
```

```plaintext
é¢è¯•ID: test_interview_001 - é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±Xä¸ªé—®é¢˜
æˆ–è€…
é¢è¯•ID: test_interview_001 - ç”Ÿæˆé¢è¯•è®¡åˆ’å¤±è´¥
```

```python
# ç¬¬156-168è¡Œ
if not dialog_history or len(dialog_history) == 0:
    # é¦–æ¬¡è°ƒç”¨ï¼šç”Ÿæˆé¢è¯•è®¡åˆ’
    await self.generate_interview_plan()
    ...
    return  # â† è¿™é‡Œåº”è¯¥è¿”å›ï¼Œä¸ç»§ç»­æ‰§è¡Œåé¢çš„ä»£ç 
```

```plaintext
é¢è¯•ID: test_interview_001 - é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±12ä¸ªé—®é¢˜
```

```plaintext
é¢è¯•ID: test_interview_001 - ç”Ÿæˆé¢è¯•è®¡åˆ’å¤±è´¥: xxxxx
```

```json
{
  "id": "test_interview_001",
  "userPortrait": "...",
  "jdTitle": "...",
  "jdPortrait": "...",
  "interviewType": 1,
  "interviewLanguage": "zhongwen",
  "dialogHistory": []  â† ä¸€å®šè¦æ˜¯ç©ºæ•°ç»„ï¼
}
```

```python
async def get_ai_response(self, dialog_history):
    start_time = time.time()
    
    # æ·»åŠ è°ƒè¯•æ—¥å¿—
    logging.info(f"é¢è¯•ID: {self.interview_id} - dialog_historyç±»å‹: {type(dialog_history)}, é•¿åº¦: {len(dialog_history) if dialog_history else 0}, å†…å®¹: {dialog_history}")
    
    # åˆ¤æ–­æ˜¯å¦é¦–æ¬¡è°ƒç”¨
    if not dialog_history or len(dialog_history) == 0:
        logging.info(f"é¢è¯•ID: {self.interview_id} - æ£€æµ‹åˆ°é¦–æ¬¡è°ƒç”¨ï¼Œå¼€å§‹ç”Ÿæˆé¢è¯•è®¡åˆ’")
        # é¦–æ¬¡è°ƒç”¨ï¼šç”Ÿæˆé¢è¯•è®¡åˆ’
        await self.generate_interview_plan()
        ...
```

```plaintext
é¢è¯•ID: test_interview_001 - æ£€æµ‹åˆ°é¦–æ¬¡è°ƒç”¨ï¼Œå¼€å§‹ç”Ÿæˆé¢è¯•è®¡åˆ’
```

```json
{
  "id": "test_interview_001",
  "userPortrait": "æˆ‘å«å¼ ä¸‰ï¼Œ5å¹´Pythonåç«¯å¼€å‘ç»éªŒï¼Œç†Ÿæ‚‰Djangoã€Flaskæ¡†æ¶ï¼Œåšè¿‡ç”µå•†å’Œé‡‘èç³»ç»Ÿ",
  "jdTitle": "é«˜çº§Pythonå·¥ç¨‹å¸ˆ",
  "jdPortrait": "è´Ÿè´£åç«¯APIå¼€å‘ï¼Œè¦æ±‚ç†Ÿæ‚‰å¾®æœåŠ¡æ¶æ„ï¼Œæœ‰é«˜å¹¶å‘ç³»ç»Ÿç»éªŒ",
  "interviewType": 1,
  "interviewLanguage": "zhongwen",
  "dialogHistory": []
}
```

```plaintext
é¢è¯•ID: test_interview_001 - é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±12ä¸ªé—®é¢˜
```

```plaintext
"ä½ å¥½ï¼Œæˆ‘æ˜¯æœ¬æ¬¡é¢è¯•çš„AIé¢è¯•å®˜......"
```

```plaintext
é¢è¯•ID: test_interview_001 - å½“å‰è¿›åº¦ï¼šå·²æé—® 1/12 ä¸ªé—®é¢˜
```

---

**User**



æˆ‘æ˜æ˜æˆ‘æ¯æ¬¡postmanä¼ å…¥çš„è¿™ä¸ªè¯·æ±‚å‚æ•°éƒ½æ˜¯ä¸‹é¢è¿™ä¸ªæ ·å­çš„ã€‚ dialogHistoryæ¯æ¬¡éƒ½æ˜¯ç©ºçš„å‘€æ‰€ä»¥ä¸ºä»€ä¹ˆæ²¡æœ‰è§¦å‘é¢è¯•é—®é¢˜ç”Ÿæˆçš„é€»è¾‘å‘¢ï¼Ÿèƒ½ä¸èƒ½åŠ å…¥é¢å¤–çš„è°ƒè¯•ä¿¡æ¯å‘¢ï¼Ÿå°±æ¯”å¦‚é€šè¿‡æ—¥å¿—çš„æ–¹å¼å¸®æˆ‘è¾“å‡ºå‘¢ï¼Ÿå¸®æˆ‘åˆ¤æ–­ä¸€ä¸‹ã€‚ æ·»åŠ æ›´å¤šæ—¥å¿—è¾“å‡ºçš„ä¿¡æ¯ï¼Ÿ @interview_process_router.py è¿™ä¸ªæ–‡ä»¶çš„è¯ï¼Œå­˜æ”¾çš„æ˜¯å…·ä½“çš„è·¯ç”±ä¿¡æ¯å‘€ã€‚ä½ å¸®æˆ‘çœ‹ä¸€ä¸‹åˆ°åº•å­˜åœ¨å“ªäº›é—®é¢˜å‘¢ï¼Ÿæˆ‘ä»¬å…ˆé€æ­¥æ¢³ç†å¹¶å®šä½é—®é¢˜çš„æ ¹å› ã€‚å½“å‰ä»…åšåˆ†æä¸ç»“è®ºæ²‰æ·€ï¼Œæš‚ä¸äº§å‡ºä»»ä½•ä»£ç ã€‚

```

{



  "id": "test_interview_001",

  "userPortrait": "æˆ‘å«å¼ ä¸‰ï¼Œ5å¹´Pythonåç«¯å¼€å‘ç»éªŒï¼Œç†Ÿæ‚‰Djangoã€Flaskæ¡†æ¶ï¼Œåšè¿‡ç”µå•†å’Œé‡‘èç³»ç»Ÿ",

  "jdTitle": "é«˜çº§Pythonå·¥ç¨‹å¸ˆ",

  "jdPortrait": "è´Ÿè´£åç«¯APIå¼€å‘ï¼Œè¦æ±‚ç†Ÿæ‚‰å¾®æœåŠ¡æ¶æ„ï¼Œæœ‰é«˜å¹¶å‘ç³»ç»Ÿç»éªŒ",

  "interviewType": 1,

  "interviewLanguage": "zhongwen",

  "dialogHistory": []

}

---

**Cursor**

å¥½çš„ï¼è®©æˆ‘ä»”ç»†åˆ†æä¸€ä¸‹é—®é¢˜æ ¹å› ã€‚æˆ‘ä¼šé€æ­¥æ¢³ç†æ•´ä¸ªè¯·æ±‚æµç¨‹ã€‚

å¥½çš„ï¼ç°åœ¨è®©æˆ‘è¯¦ç»†åˆ†ææ•´ä¸ªæµç¨‹ï¼Œæ‰¾å‡ºé—®é¢˜æ ¹å› ã€‚

---

## ğŸ” å®Œæ•´æµç¨‹æ¢³ç†

### 1. è¯·æ±‚è·¯å¾„åˆ†æ

```
Postmanå‘é€è¯·æ±‚
  â†“
interview_process_router.py (ç¬¬29è¡Œ)
  dialog_history = json_data.get("dialogHistory")  
  # ä½ å‘é€çš„æ˜¯ []ï¼Œæ‰€ä»¥è¿™é‡Œ dialog_history = []
  â†“
interview_process_router.py (ç¬¬42è¡Œ)
  interview.get_ai_response(dialog_history)
  # ä¼ å…¥çš„æ˜¯ []
  â†“
virtual_interview.py (ç¬¬153è¡Œ)
  async def get_ai_response(self, dialog_history):
  # dialog_history = []
  â†“
virtual_interview.py (ç¬¬157è¡Œ)
  if not dialog_history or len(dialog_history) == 0:
  # [] æ˜¯ç©ºåˆ—è¡¨ï¼Œnot [] = True
  # len([]) = 0
  # åº”è¯¥è¿›å…¥è¿™ä¸ªåˆ†æ”¯ï¼
```

### 2. é—®é¢˜å®šä½

æ ¹æ®ä½ çš„æ—¥å¿—ï¼Œä»£ç **æ²¡æœ‰è¿›å…¥**ç¬¬157è¡Œçš„ifåˆ†æ”¯ï¼Œè€Œæ˜¯**ç›´æ¥è·³åˆ°äº†ç¬¬170è¡Œ**å¼€å§‹æ‰§è¡ŒåŸæœ‰çš„ç­–ç•¥å±‚é€»è¾‘ã€‚

---

## ğŸ› æ ¹å› åˆ†æ

æˆ‘å‘ç°äº†**å…³é”®é—®é¢˜**ï¼š

### é—®é¢˜1ï¼šåˆ¤æ–­æ¡ä»¶çš„é€»è¾‘

çœ‹ä»£ç ç¬¬157è¡Œï¼š
```python
if not dialog_history or len(dialog_history) == 0:
```

è¿™ä¸ªæ¡ä»¶çœ‹èµ·æ¥æ²¡é—®é¢˜ï¼Œä½†æˆ‘ä»¬éœ€è¦ç¡®è®¤ï¼š
- `dialog_history` çš„ç±»å‹æ˜¯ä»€ä¹ˆï¼Ÿ
- æ˜¯ `list` å—ï¼Ÿ
- è¿˜æ˜¯ `None`ï¼Ÿ
- è¿˜æ˜¯å­—ç¬¦ä¸² `"[]"`ï¼Ÿ

### é—®é¢˜2ï¼šå¼‚å¸¸å¤„ç†çš„éšè—å½±å“

çœ‹ç¬¬120-150è¡Œçš„ `generate_interview_plan()` æ–¹æ³•ï¼š

```python
async def generate_interview_plan(self):
    try:
        # ... ç”Ÿæˆè®¡åˆ’çš„é€»è¾‘ ...
        logging.info(f"é¢è¯•ID: {self.interview_id} - é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸ")
    except Exception as e:
        logging.error(f"é¢è¯•ID: {self.interview_id} - ç”Ÿæˆé¢è¯•è®¡åˆ’å¤±è´¥: {e}")
        return None
```

**å…³é”®ç‚¹**ï¼šå¦‚æœè¿™ä¸ªæ–¹æ³•æŠ›å‡ºå¼‚å¸¸è¢«æ•è·äº†ï¼Œä¼šï¼š
1. æ‰“å°é”™è¯¯æ—¥å¿—ï¼ˆä½†ä½ çš„æ—¥å¿—é‡Œæ²¡æœ‰ï¼‰
2. è¿”å› `None`

ç„¶åçœ‹ç¬¬162è¡Œï¼š
```python
if self.interview_plan and len(self.interview_plan['questions']) > 0:
```

**å¦‚æœ `generate_interview_plan()` å¤±è´¥äº†**ï¼Œ`self.interview_plan` ä¼šæ˜¯ `None`ï¼Œè¿™ä¸ªæ¡ä»¶å°±æ˜¯ `False`ï¼Œä»£ç ä¼š**ç»§ç»­å¾€ä¸‹æ‰§è¡Œ**åˆ°ç¬¬170è¡Œï¼

---

## ğŸ¯ æœ€å¯èƒ½çš„3ä¸ªåŸå› 

### åŸå› 1ï¼šåˆ¤æ–­æ¡ä»¶æ ¹æœ¬æ²¡æœ‰è¿›å…¥ï¼ˆæœ€ä¸å¯èƒ½ï¼‰

å¦‚æœ `dialog_history` ä¸æ˜¯ç©ºåˆ—è¡¨ï¼Œæ¯”å¦‚ï¼š
- æ˜¯ `None`ï¼Ÿä½† `not None` ä¹Ÿä¼šè¿›å…¥if
- æ˜¯å­—ç¬¦ä¸² `"[]"`ï¼Ÿè¿™å°±æœ‰é—®é¢˜äº†

**å¯èƒ½æ€§**ï¼š10%

---

### åŸå› 2ï¼š`generate_interview_plan()` è¢«è°ƒç”¨äº†ï¼Œä½†æ˜¯æŠ›å‡ºå¼‚å¸¸äº†ï¼ˆæœ€å¯èƒ½ï¼ï¼‰

çœ‹ä»£ç ç¬¬123-128è¡Œï¼š
```python
plan_prompt = load_prompt("generate_plan_white_collar.txt")
plan_prompt_filled = plan_prompt.format(
    user_portrait=self.user_portrait,  # â† å¯èƒ½è¿™é‡Œæœ‰é—®é¢˜ï¼
    jd_title=self.jd_title,
    jd_portrait=self.jd_portrait
)
```

**é—®é¢˜ç‚¹**ï¼š
- å¦‚æœ prompt æ–‡ä»¶ä¸­çš„å ä½ç¬¦å’Œè¿™é‡Œçš„å˜é‡åä¸åŒ¹é…
- æˆ–è€… prompt æ–‡ä»¶ä¸­è¿˜æœ‰å…¶ä»– `{xxx}` å ä½ç¬¦æ²¡æœ‰è¢«æ›¿æ¢
- å°±ä¼šæŠ›å‡º `KeyError` å¼‚å¸¸ï¼

æ¯”å¦‚ï¼Œå¦‚æœ prompt ä¸­æœ‰ `{company_portrait}`ï¼Œä½†è¿™é‡Œæ²¡æœ‰ä¼ ï¼Œå°±ä¼šæŠ¥é”™ã€‚

**ç„¶å**ï¼š
1. å¼‚å¸¸è¢«æ•è·
2. æ‰“å°é”™è¯¯æ—¥å¿—ï¼ˆä½ çš„æ—¥å¿—æ–‡ä»¶å¯èƒ½è¢«æˆªæ–­äº†ï¼Œæ²¡çœ‹åˆ°ï¼‰
3. è¿”å› `None`
4. ç¬¬162è¡Œçš„ifæ¡ä»¶æ˜¯ `False`
5. **ç»§ç»­æ‰§è¡Œåˆ°ç¬¬170è¡Œçš„åŸæœ‰é€»è¾‘**

**å¯èƒ½æ€§**ï¼š**80%**ï¼ˆæœ€å¯èƒ½ï¼ï¼‰

---

### åŸå› 3ï¼š`generate_interview_plan()` æˆåŠŸäº†ï¼Œä½†JSONè§£æå¤±è´¥

ç¬¬139-142è¡Œï¼š
```python
plan_json = response.choices[0].message.content
plan_json = plan_json.replace("```json", "").replace("```", "").strip()
self.interview_plan = json.loads(plan_json)
```

å¦‚æœ LLM è¿”å›çš„å†…å®¹ä¸æ˜¯æ ‡å‡†JSONï¼Œ`json.loads()` ä¼šæŠ›å‡º `JSONDecodeError`ï¼Œç„¶åï¼š
1. å¼‚å¸¸è¢«æ•è·
2. æ‰“å°é”™è¯¯æ—¥å¿—
3. è¿”å› `None`
4. ç»§ç»­æ‰§è¡ŒåŸæœ‰é€»è¾‘

**å¯èƒ½æ€§**ï¼š15%

---

## ğŸ” è¯æ®åˆ†æ

ä»ä½ çš„æ—¥å¿—æ¥çœ‹ï¼š

```
2025-11-10 00:21:27,641 - æ”¶åˆ°é¢è¯•çš„ç½‘ç»œè¯·æ±‚
2025-11-10 00:21:27,691 - å½“å‰ç­–ç•¥promptå¦‚ä¸‹
```

**æ—¶é—´é—´éš”åªæœ‰50msï¼**

å¦‚æœçœŸçš„è°ƒç”¨äº† `generate_interview_plan()`ï¼Œé‚£ä¹ˆï¼š
1. éœ€è¦è°ƒç”¨ä¸€æ¬¡LLMï¼ˆè‡³å°‘éœ€è¦1-5ç§’ï¼‰
2. éœ€è¦è§£æJSON
3. éœ€è¦æ‰“å°æ—¥å¿—

**ä¸å¯èƒ½åœ¨50mså†…å®Œæˆï¼**

è¿™è¯´æ˜ï¼š**ä»£ç æ ¹æœ¬æ²¡æœ‰è¿›å…¥ç¬¬157è¡Œçš„ifåˆ†æ”¯ï¼Œæˆ–è€…è¿›å…¥äº†ä½†ç«‹å³è¿”å›äº†ã€‚**

---

## ğŸ¯ æœ€ç»ˆç»“è®º

### ç»“è®º1ï¼šåˆ¤æ–­æ¡ä»¶çš„é—®é¢˜ï¼ˆéœ€è¦éªŒè¯ï¼‰

è™½ç„¶ä½ çš„è¯·æ±‚ä½“æ˜¯ `"dialogHistory": []`ï¼Œä½†æˆ‘ä»¬éœ€è¦ç¡®è®¤ï¼š

**åœ¨è·¯ç”±å±‚ç¬¬29è¡Œ**ï¼š
```python
dialog_history = json_data.get("dialogHistory")
```

è¿™é‡Œå¾—åˆ°çš„ `dialog_history` åˆ°åº•æ˜¯ï¼š
- `[]` ï¼ˆç©ºåˆ—è¡¨ï¼‰âœ… æ­£ç¡®
- `None` ï¼ˆå¦‚æœkeyä¸å­˜åœ¨ï¼‰
- è¿˜æ˜¯å…¶ä»–ç±»å‹ï¼Ÿ

### ç»“è®º2ï¼šç”Ÿæˆè®¡åˆ’å‡½æ•°æœ‰å¼‚å¸¸ï¼ˆæœ€å¯èƒ½ï¼‰

å³ä½¿è¿›å…¥äº†ifåˆ†æ”¯ï¼Œ`generate_interview_plan()` ä¹Ÿå¯èƒ½ï¼š
1. æŠ›å‡ºå¼‚å¸¸ï¼ˆpromptæ ¼å¼åŒ–å¤±è´¥ã€LLMè°ƒç”¨å¤±è´¥ã€JSONè§£æå¤±è´¥ï¼‰
2. å¼‚å¸¸è¢«æ•è·ï¼Œè¿”å› `None`
3. ç¬¬162è¡Œæ¡ä»¶ä¸æ»¡è¶³
4. **æ²¡æœ‰ `return`ï¼Œç»§ç»­æ‰§è¡Œä¸‹é¢çš„ä»£ç **

**è¿™å°±æ˜¯é—®é¢˜æ‰€åœ¨ï¼**

---

## ğŸ’¡ é—®é¢˜çš„æ ¹æœ¬åŸå› 

**çœ‹ç¬¬168è¡Œï¼š**
```python
return
```

è¿™ä¸ª `return` åªåœ¨ `if self.interview_plan and ...` æ¡ä»¶ä¸º `True` æ—¶æ‰æ‰§è¡Œã€‚

**å¦‚æœ `generate_interview_plan()` å¤±è´¥äº†**ï¼š
- `self.interview_plan` æ˜¯ `None`
- ç¬¬162è¡Œçš„ifä¸æ»¡è¶³
- **æ²¡æœ‰ `return`**
- **ä»£ç ç»§ç»­æ‰§è¡Œåˆ°ç¬¬170è¡Œ**ï¼

---

## ğŸ“‹ éœ€è¦æ·»åŠ çš„è°ƒè¯•æ—¥å¿—ï¼ˆå»ºè®®ï¼‰

### åœ¨è·¯ç”±å±‚æ·»åŠ ï¼š

```python
# ç¬¬29è¡Œä¹‹å
dialog_history = json_data.get("dialogHistory")
logging.info(f"[DEBUG] è·¯ç”±å±‚æ”¶åˆ°çš„ dialogHistory: {dialog_history}, ç±»å‹: {type(dialog_history)}, é•¿åº¦: {len(dialog_history) if dialog_history else 'N/A'}")
```

### åœ¨ `get_ai_response` æ–¹æ³•å¼€å¤´æ·»åŠ ï¼š

```python
# ç¬¬154è¡Œä¹‹å
logging.info(f"[DEBUG] get_ai_response æ”¶åˆ°çš„ dialog_history: {dialog_history}, ç±»å‹: {type(dialog_history)}")
logging.info(f"[DEBUG] åˆ¤æ–­æ¡ä»¶: not dialog_history = {not dialog_history}, len = {len(dialog_history) if dialog_history else 'N/A'}")
```

### åœ¨ç¬¬157è¡Œçš„ifå†…éƒ¨æ·»åŠ ï¼š

```python
if not dialog_history or len(dialog_history) == 0:
    logging.info(f"[DEBUG] è¿›å…¥é¦–æ¬¡è°ƒç”¨åˆ†æ”¯")
    # é¦–æ¬¡è°ƒç”¨ï¼šç”Ÿæˆé¢è¯•è®¡åˆ’
    await self.generate_interview_plan()
    logging.info(f"[DEBUG] ç”Ÿæˆè®¡åˆ’å®Œæˆï¼Œinterview_plan æ˜¯å¦ä¸ºç©º: {self.interview_plan is None}")
    
    # è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆå¼€åœºç™½ï¼‰
    if self.interview_plan and len(self.interview_plan['questions']) > 0:
        logging.info(f"[DEBUG] å‡†å¤‡è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜")
        ...
    else:
        logging.warning(f"[DEBUG] é¢è¯•è®¡åˆ’ä¸ºç©ºæˆ–æ²¡æœ‰é—®é¢˜ï¼Œç»§ç»­æ‰§è¡ŒåŸæœ‰æµç¨‹")
```

### åœ¨ `generate_interview_plan` æ–¹æ³•ä¸­æ·»åŠ ï¼š

```python
async def generate_interview_plan(self):
    """ç”Ÿæˆé¢è¯•è®¡åˆ’ï¼ˆä»…é¦–æ¬¡è°ƒç”¨ï¼‰"""
    logging.info(f"[DEBUG] å¼€å§‹ç”Ÿæˆé¢è¯•è®¡åˆ’")
    try:
        logging.info(f"[DEBUG] åŠ è½½promptæ–‡ä»¶")
        plan_prompt = load_prompt("generate_plan_white_collar.txt")
        
        logging.info(f"[DEBUG] å‡†å¤‡æ ¼å¼åŒ–promptï¼Œå‚æ•°: user_portrait={self.user_portrait[:30]}..., jd_title={self.jd_title}")
        plan_prompt_filled = plan_prompt.format(
            user_portrait=self.user_portrait,
            jd_title=self.jd_title,
            jd_portrait=self.jd_portrait
        )
        logging.info(f"[DEBUG] promptæ ¼å¼åŒ–æˆåŠŸ")
        
        logging.info(f"[DEBUG] å¼€å§‹è°ƒç”¨LLMç”Ÿæˆè®¡åˆ’")
        response = self.celue_client.chat.completions.create(...)
        logging.info(f"[DEBUG] LLMè°ƒç”¨æˆåŠŸ")
        
        plan_json = response.choices[0].message.content
        logging.info(f"[DEBUG] LLMè¿”å›å†…å®¹ï¼ˆå‰100å­—ç¬¦ï¼‰: {plan_json[:100]}")
        
        plan_json = plan_json.replace("```json", "").replace("```", "").strip()
        logging.info(f"[DEBUG] å¼€å§‹è§£æJSON")
        self.interview_plan = json.loads(plan_json)
        logging.info(f"[DEBUG] JSONè§£ææˆåŠŸï¼Œé—®é¢˜æ•°é‡: {len(self.interview_plan['questions'])}")
        
        logging.info(f"é¢è¯•ID: {self.interview_id} - é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±{len(self.interview_plan['questions'])}ä¸ªé—®é¢˜")
        return self.interview_plan
        
    except Exception as e:
        logging.error(f"é¢è¯•ID: {self.interview_id} - ç”Ÿæˆé¢è¯•è®¡åˆ’å¤±è´¥: {e}", exc_info=True)  # æ·»åŠ  exc_info=True æ‰“å°å®Œæ•´å †æ ˆ
        return None
```

---

## ğŸ“Š é—®é¢˜æ’æŸ¥ä¼˜å…ˆçº§

| ä¼˜å…ˆçº§ | é—®é¢˜å‡è®¾ | éªŒè¯æ–¹æ³• | å¯èƒ½æ€§ |
|--------|---------|---------|--------|
| **P0** | `generate_interview_plan()` æŠ›å‡ºå¼‚å¸¸ | æŸ¥çœ‹å®Œæ•´æ—¥å¿—ï¼Œæ·»åŠ è°ƒè¯•ä¿¡æ¯ | 80% |
| **P1** | JSONè§£æå¤±è´¥ | æ·»åŠ æ—¥å¿—æ‰“å°LLMè¿”å›å†…å®¹ | 15% |
| **P2** | `dialog_history` ç±»å‹ä¸å¯¹ | æ·»åŠ æ—¥å¿—æ‰“å°ç±»å‹å’Œå€¼ | 5% |

---

## ğŸ¯ ä¸‹ä¸€æ­¥å»ºè®®

1. **æ·»åŠ ä¸Šè¿°æ‰€æœ‰è°ƒè¯•æ—¥å¿—**
2. **é‡æ–°è¿è¡ŒPostmanæµ‹è¯•**
3. **æŸ¥çœ‹å®Œæ•´çš„æ—¥å¿—è¾“å‡º**ï¼ˆåŒ…æ‹¬é”™è¯¯æ—¥å¿—ï¼‰
4. **æ ¹æ®æ—¥å¿—è¾“å‡ºç¡®å®šé—®é¢˜æ ¹å› **

å¦‚æœä½ åŒæ„è¿™ä¸ªåˆ†æï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”Ÿæˆå®Œæ•´çš„å¸¦è°ƒè¯•æ—¥å¿—çš„ä»£ç ã€‚éœ€è¦æˆ‘ç°åœ¨å°±ç”Ÿæˆå—ï¼Ÿ

```plaintext
Postmanå‘é€è¯·æ±‚
  â†“
interview_process_router.py (ç¬¬29è¡Œ)
  dialog_history = json_data.get("dialogHistory")  
  # ä½ å‘é€çš„æ˜¯ []ï¼Œæ‰€ä»¥è¿™é‡Œ dialog_history = []
  â†“
interview_process_router.py (ç¬¬42è¡Œ)
  interview.get_ai_response(dialog_history)
  # ä¼ å…¥çš„æ˜¯ []
  â†“
virtual_interview.py (ç¬¬153è¡Œ)
  async def get_ai_response(self, dialog_history):
  # dialog_history = []
  â†“
virtual_interview.py (ç¬¬157è¡Œ)
  if not dialog_history or len(dialog_history) == 0:
  # [] æ˜¯ç©ºåˆ—è¡¨ï¼Œnot [] = True
  # len([]) = 0
  # åº”è¯¥è¿›å…¥è¿™ä¸ªåˆ†æ”¯ï¼
```

```python
if not dialog_history or len(dialog_history) == 0:
```

```python
async def generate_interview_plan(self):
    try:
        # ... ç”Ÿæˆè®¡åˆ’çš„é€»è¾‘ ...
        logging.info(f"é¢è¯•ID: {self.interview_id} - é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸ")
    except Exception as e:
        logging.error(f"é¢è¯•ID: {self.interview_id} - ç”Ÿæˆé¢è¯•è®¡åˆ’å¤±è´¥: {e}")
        return None
```

```python
if self.interview_plan and len(self.interview_plan['questions']) > 0:
```

```python
plan_prompt = load_prompt("generate_plan_white_collar.txt")
plan_prompt_filled = plan_prompt.format(
    user_portrait=self.user_portrait,  # â† å¯èƒ½è¿™é‡Œæœ‰é—®é¢˜ï¼
    jd_title=self.jd_title,
    jd_portrait=self.jd_portrait
)
```

```python
plan_json = response.choices[0].message.content
plan_json = plan_json.replace("", "").replace("```", "").strip()
self.interview_plan = json.loads(plan_json)å¦‚æœ LLM è¿”å›çš„å†…å®¹ä¸æ˜¯æ ‡å‡†JSONï¼Œ`json.loads()` ä¼šæŠ›å‡º `JSONDecodeError`ï¼Œç„¶åï¼š
1. å¼‚å¸¸è¢«æ•è·
2. æ‰“å°é”™è¯¯æ—¥å¿—
3. è¿”å› `None`
4. ç»§ç»­æ‰§è¡ŒåŸæœ‰é€»è¾‘

**å¯èƒ½æ€§**ï¼š15%

---

## ğŸ” è¯æ®åˆ†æ

ä»ä½ çš„æ—¥å¿—æ¥çœ‹ï¼š

```

```plaintext

**æ—¶é—´é—´éš”åªæœ‰50msï¼**

å¦‚æœçœŸçš„è°ƒç”¨äº† `generate_interview_plan()`ï¼Œé‚£ä¹ˆï¼š
1. éœ€è¦è°ƒç”¨ä¸€æ¬¡LLMï¼ˆè‡³å°‘éœ€è¦1-5ç§’ï¼‰
2. éœ€è¦è§£æJSON
3. éœ€è¦æ‰“å°æ—¥å¿—

**ä¸å¯èƒ½åœ¨50mså†…å®Œæˆï¼**

è¿™è¯´æ˜ï¼š**ä»£ç æ ¹æœ¬æ²¡æœ‰è¿›å…¥ç¬¬157è¡Œçš„ifåˆ†æ”¯ï¼Œæˆ–è€…è¿›å…¥äº†ä½†ç«‹å³è¿”å›äº†ã€‚**

---

## ğŸ¯ æœ€ç»ˆç»“è®º

### ç»“è®º1ï¼šåˆ¤æ–­æ¡ä»¶çš„é—®é¢˜ï¼ˆéœ€è¦éªŒè¯ï¼‰

è™½ç„¶ä½ çš„è¯·æ±‚ä½“æ˜¯ `"dialogHistory": []`ï¼Œä½†æˆ‘ä»¬éœ€è¦ç¡®è®¤ï¼š

**åœ¨è·¯ç”±å±‚ç¬¬29è¡Œ**ï¼š
dialog_history = json_data.get("dialogHistory")è¿™é‡Œå¾—åˆ°çš„ `dialog_history` åˆ°åº•æ˜¯ï¼š
- `[]` ï¼ˆç©ºåˆ—è¡¨ï¼‰âœ… æ­£ç¡®
- `None` ï¼ˆå¦‚æœkeyä¸å­˜åœ¨ï¼‰
- è¿˜æ˜¯å…¶ä»–ç±»å‹ï¼Ÿ

### ç»“è®º2ï¼šç”Ÿæˆè®¡åˆ’å‡½æ•°æœ‰å¼‚å¸¸ï¼ˆæœ€å¯èƒ½ï¼‰

å³ä½¿è¿›å…¥äº†ifåˆ†æ”¯ï¼Œ`generate_interview_plan()` ä¹Ÿå¯èƒ½ï¼š
1. æŠ›å‡ºå¼‚å¸¸ï¼ˆpromptæ ¼å¼åŒ–å¤±è´¥ã€LLMè°ƒç”¨å¤±è´¥ã€JSONè§£æå¤±è´¥ï¼‰
2. å¼‚å¸¸è¢«æ•è·ï¼Œè¿”å› `None`
3. ç¬¬162è¡Œæ¡ä»¶ä¸æ»¡è¶³
4. **æ²¡æœ‰ `return`ï¼Œç»§ç»­æ‰§è¡Œä¸‹é¢çš„ä»£ç **

**è¿™å°±æ˜¯é—®é¢˜æ‰€åœ¨ï¼**

---

## ğŸ’¡ é—®é¢˜çš„æ ¹æœ¬åŸå› 

**çœ‹ç¬¬168è¡Œï¼š**
returnè¿™ä¸ª `return` åªåœ¨ `if self.interview_plan and ...` æ¡ä»¶ä¸º `True` æ—¶æ‰æ‰§è¡Œã€‚

**å¦‚æœ `generate_interview_plan()` å¤±è´¥äº†**ï¼š
- `self.interview_plan` æ˜¯ `None`
- ç¬¬162è¡Œçš„ifä¸æ»¡è¶³
- **æ²¡æœ‰ `return`**
- **ä»£ç ç»§ç»­æ‰§è¡Œåˆ°ç¬¬170è¡Œ**ï¼

---

## ğŸ“‹ éœ€è¦æ·»åŠ çš„è°ƒè¯•æ—¥å¿—ï¼ˆå»ºè®®ï¼‰

### åœ¨è·¯ç”±å±‚æ·»åŠ ï¼š

# ç¬¬29è¡Œä¹‹å
dialog_history = json_data.get("dialogHistory")
logging.info(f"[DEBUG] è·¯ç”±å±‚æ”¶åˆ°çš„ dialogHistory: {dialog_history}, ç±»å‹: {type(dialog_history)}, é•¿åº¦: {len(dialog_history) if dialog_history else 'N/A'}")### åœ¨ `get_ai_response` æ–¹æ³•å¼€å¤´æ·»åŠ ï¼š

# ç¬¬154è¡Œä¹‹å
logging.info(f"[DEBUG] get_ai_response æ”¶åˆ°çš„ dialog_history: {dialog_history}, ç±»å‹: {type(dialog_history)}")
logging.info(f"[DEBUG] åˆ¤æ–­æ¡ä»¶: not dialog_history = {not dialog_history}, len = {len(dialog_history) if dialog_history else 'N/A'}")### åœ¨ç¬¬157è¡Œçš„ifå†…éƒ¨æ·»åŠ ï¼š

if not dialog_history or len(dialog_history) == 0:
    logging.info(f"[DEBUG] è¿›å…¥é¦–æ¬¡è°ƒç”¨åˆ†æ”¯")
    # é¦–æ¬¡è°ƒç”¨ï¼šç”Ÿæˆé¢è¯•è®¡åˆ’
    await self.generate_interview_plan()
    logging.info(f"[DEBUG] ç”Ÿæˆè®¡åˆ’å®Œæˆï¼Œinterview_plan æ˜¯å¦ä¸ºç©º: {self.interview_plan is None}")
    
    # è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆå¼€åœºç™½ï¼‰
    if self.interview_plan and len(self.interview_plan['questions']) > 0:
        logging.info(f"[DEBUG] å‡†å¤‡è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜")
        ...
    else:
        logging.warning(f"[DEBUG] é¢è¯•è®¡åˆ’ä¸ºç©ºæˆ–æ²¡æœ‰é—®é¢˜ï¼Œç»§ç»­æ‰§è¡ŒåŸæœ‰æµç¨‹")### åœ¨ `generate_interview_plan` æ–¹æ³•ä¸­æ·»åŠ ï¼š

async def generate_interview_plan(self):
    """ç”Ÿæˆé¢è¯•è®¡åˆ’ï¼ˆä»…é¦–æ¬¡è°ƒç”¨ï¼‰"""
    logging.info(f"[DEBUG] å¼€å§‹ç”Ÿæˆé¢è¯•è®¡åˆ’")
    try:
        logging.info(f"[DEBUG] åŠ è½½promptæ–‡ä»¶")
        plan_prompt = load_prompt("generate_plan_white_collar.txt")
        
        logging.info(f"[DEBUG] å‡†å¤‡æ ¼å¼åŒ–promptï¼Œå‚æ•°: user_portrait={self.user_portrait[:30]}..., jd_title={self.jd_title}")
        plan_prompt_filled = plan_prompt.format(
            user_portrait=self.user_portrait,
            jd_title=self.jd_title,
            jd_portrait=self.jd_portrait
        )
        logging.info(f"[DEBUG] promptæ ¼å¼åŒ–æˆåŠŸ")
        
        logging.info(f"[DEBUG] å¼€å§‹è°ƒç”¨LLMç”Ÿæˆè®¡åˆ’")
        response = self.celue_client.chat.completions.create(...)
        logging.info(f"[DEBUG] LLMè°ƒç”¨æˆåŠŸ")
        
        plan_json = response.choices[0].message.content
        logging.info(f"[DEBUG] LLMè¿”å›å†…å®¹ï¼ˆå‰100å­—ç¬¦ï¼‰: {plan_json[:100]}")
        
        plan_json = plan_json.replace("", "").replace("```", "").strip()
        logging.info(f"[DEBUG] å¼€å§‹è§£æJSON")
        self.interview_plan = json.loads(plan_json)
        logging.info(f"[DEBUG] JSONè§£ææˆåŠŸï¼Œé—®é¢˜æ•°é‡: {len(self.interview_plan['questions'])}")
        
        logging.info(f"é¢è¯•ID: {self.interview_id} - é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±{len(self.interview_plan['questions'])}ä¸ªé—®é¢˜")
        return self.interview_plan
        
    except Exception as e:
        logging.error(f"é¢è¯•ID: {self.interview_id} - ç”Ÿæˆé¢è¯•è®¡åˆ’å¤±è´¥: {e}", exc_info=True)  # æ·»åŠ  exc_info=True æ‰“å°å®Œæ•´å †æ ˆ
        return None---

## ğŸ“Š é—®é¢˜æ’æŸ¥ä¼˜å…ˆçº§

| ä¼˜å…ˆçº§ | é—®é¢˜å‡è®¾ | éªŒè¯æ–¹æ³• | å¯èƒ½æ€§ |
|--------|---------|---------|--------|
| **P0** | `generate_interview_plan()` æŠ›å‡ºå¼‚å¸¸ | æŸ¥çœ‹å®Œæ•´æ—¥å¿—ï¼Œæ·»åŠ è°ƒè¯•ä¿¡æ¯ | 80% |
| **P1** | JSONè§£æå¤±è´¥ | æ·»åŠ æ—¥å¿—æ‰“å°LLMè¿”å›å†…å®¹ | 15% |
| **P2** | `dialog_history` ç±»å‹ä¸å¯¹ | æ·»åŠ æ—¥å¿—æ‰“å°ç±»å‹å’Œå€¼ | 5% |

---

## ğŸ¯ ä¸‹ä¸€æ­¥å»ºè®®

1. **æ·»åŠ ä¸Šè¿°æ‰€æœ‰è°ƒè¯•æ—¥å¿—**
2. **é‡æ–°è¿è¡ŒPostmanæµ‹è¯•**
3. **æŸ¥çœ‹å®Œæ•´çš„æ—¥å¿—è¾“å‡º**ï¼ˆåŒ…æ‹¬é”™è¯¯æ—¥å¿—ï¼‰
4. **æ ¹æ®æ—¥å¿—è¾“å‡ºç¡®å®šé—®é¢˜æ ¹å› **

å¦‚æœä½ åŒæ„è¿™ä¸ªåˆ†æï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”Ÿæˆå®Œæ•´çš„å¸¦è°ƒè¯•æ—¥å¿—çš„ä»£ç ã€‚éœ€è¦æˆ‘ç°åœ¨å°±ç”Ÿæˆå—ï¼Ÿ
```

```plaintext

**æ—¶é—´é—´éš”åªæœ‰50msï¼**

å¦‚æœçœŸçš„è°ƒç”¨äº† `generate_interview_plan()`ï¼Œé‚£ä¹ˆï¼š
1. éœ€è¦è°ƒç”¨ä¸€æ¬¡LLMï¼ˆè‡³å°‘éœ€è¦1-5ç§’ï¼‰
2. éœ€è¦è§£æJSON
3. éœ€è¦æ‰“å°æ—¥å¿—

**ä¸å¯èƒ½åœ¨50mså†…å®Œæˆï¼**

è¿™è¯´æ˜ï¼š**ä»£ç æ ¹æœ¬æ²¡æœ‰è¿›å…¥ç¬¬157è¡Œçš„ifåˆ†æ”¯ï¼Œæˆ–è€…è¿›å…¥äº†ä½†ç«‹å³è¿”å›äº†ã€‚**

---

## ğŸ¯ æœ€ç»ˆç»“è®º

### ç»“è®º1ï¼šåˆ¤æ–­æ¡ä»¶çš„é—®é¢˜ï¼ˆéœ€è¦éªŒè¯ï¼‰

è™½ç„¶ä½ çš„è¯·æ±‚ä½“æ˜¯ `"dialogHistory": []`ï¼Œä½†æˆ‘ä»¬éœ€è¦ç¡®è®¤ï¼š

**åœ¨è·¯ç”±å±‚ç¬¬29è¡Œ**ï¼š
dialog_history = json_data.get("dialogHistory")è¿™é‡Œå¾—åˆ°çš„ `dialog_history` åˆ°åº•æ˜¯ï¼š
- `[]` ï¼ˆç©ºåˆ—è¡¨ï¼‰âœ… æ­£ç¡®
- `None` ï¼ˆå¦‚æœkeyä¸å­˜åœ¨ï¼‰
- è¿˜æ˜¯å…¶ä»–ç±»å‹ï¼Ÿ

### ç»“è®º2ï¼šç”Ÿæˆè®¡åˆ’å‡½æ•°æœ‰å¼‚å¸¸ï¼ˆæœ€å¯èƒ½ï¼‰

å³ä½¿è¿›å…¥äº†ifåˆ†æ”¯ï¼Œ`generate_interview_plan()` ä¹Ÿå¯èƒ½ï¼š
1. æŠ›å‡ºå¼‚å¸¸ï¼ˆpromptæ ¼å¼åŒ–å¤±è´¥ã€LLMè°ƒç”¨å¤±è´¥ã€JSONè§£æå¤±è´¥ï¼‰
2. å¼‚å¸¸è¢«æ•è·ï¼Œè¿”å› `None`
3. ç¬¬162è¡Œæ¡ä»¶ä¸æ»¡è¶³
4. **æ²¡æœ‰ `return`ï¼Œç»§ç»­æ‰§è¡Œä¸‹é¢çš„ä»£ç **

**è¿™å°±æ˜¯é—®é¢˜æ‰€åœ¨ï¼**

---

## ğŸ’¡ é—®é¢˜çš„æ ¹æœ¬åŸå› 

**çœ‹ç¬¬168è¡Œï¼š**
returnè¿™ä¸ª `return` åªåœ¨ `if self.interview_plan and ...` æ¡ä»¶ä¸º `True` æ—¶æ‰æ‰§è¡Œã€‚

**å¦‚æœ `generate_interview_plan()` å¤±è´¥äº†**ï¼š
- `self.interview_plan` æ˜¯ `None`
- ç¬¬162è¡Œçš„ifä¸æ»¡è¶³
- **æ²¡æœ‰ `return`**
- **ä»£ç ç»§ç»­æ‰§è¡Œåˆ°ç¬¬170è¡Œ**ï¼

---

## ğŸ“‹ éœ€è¦æ·»åŠ çš„è°ƒè¯•æ—¥å¿—ï¼ˆå»ºè®®ï¼‰

### åœ¨è·¯ç”±å±‚æ·»åŠ ï¼š

# ç¬¬29è¡Œä¹‹å
dialog_history = json_data.get("dialogHistory")
logging.info(f"[DEBUG] è·¯ç”±å±‚æ”¶åˆ°çš„ dialogHistory: {dialog_history}, ç±»å‹: {type(dialog_history)}, é•¿åº¦: {len(dialog_history) if dialog_history else 'N/A'}")### åœ¨ `get_ai_response` æ–¹æ³•å¼€å¤´æ·»åŠ ï¼š

# ç¬¬154è¡Œä¹‹å
logging.info(f"[DEBUG] get_ai_response æ”¶åˆ°çš„ dialog_history: {dialog_history}, ç±»å‹: {type(dialog_history)}")
logging.info(f"[DEBUG] åˆ¤æ–­æ¡ä»¶: not dialog_history = {not dialog_history}, len = {len(dialog_history) if dialog_history else 'N/A'}")### åœ¨ç¬¬157è¡Œçš„ifå†…éƒ¨æ·»åŠ ï¼š

if not dialog_history or len(dialog_history) == 0:
    logging.info(f"[DEBUG] è¿›å…¥é¦–æ¬¡è°ƒç”¨åˆ†æ”¯")
    # é¦–æ¬¡è°ƒç”¨ï¼šç”Ÿæˆé¢è¯•è®¡åˆ’
    await self.generate_interview_plan()
    logging.info(f"[DEBUG] ç”Ÿæˆè®¡åˆ’å®Œæˆï¼Œinterview_plan æ˜¯å¦ä¸ºç©º: {self.interview_plan is None}")
    
    # è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆå¼€åœºç™½ï¼‰
    if self.interview_plan and len(self.interview_plan['questions']) > 0:
        logging.info(f"[DEBUG] å‡†å¤‡è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜")
        ...
    else:
        logging.warning(f"[DEBUG] é¢è¯•è®¡åˆ’ä¸ºç©ºæˆ–æ²¡æœ‰é—®é¢˜ï¼Œç»§ç»­æ‰§è¡ŒåŸæœ‰æµç¨‹")### åœ¨ `generate_interview_plan` æ–¹æ³•ä¸­æ·»åŠ ï¼š

async def generate_interview_plan(self):
    """ç”Ÿæˆé¢è¯•è®¡åˆ’ï¼ˆä»…é¦–æ¬¡è°ƒç”¨ï¼‰"""
    logging.info(f"[DEBUG] å¼€å§‹ç”Ÿæˆé¢è¯•è®¡åˆ’")
    try:
        logging.info(f"[DEBUG] åŠ è½½promptæ–‡ä»¶")
        plan_prompt = load_prompt("generate_plan_white_collar.txt")
        
        logging.info(f"[DEBUG] å‡†å¤‡æ ¼å¼åŒ–promptï¼Œå‚æ•°: user_portrait={self.user_portrait[:30]}..., jd_title={self.jd_title}")
        plan_prompt_filled = plan_prompt.format(
            user_portrait=self.user_portrait,
            jd_title=self.jd_title,
            jd_portrait=self.jd_portrait
        )
        logging.info(f"[DEBUG] promptæ ¼å¼åŒ–æˆåŠŸ")
        
        logging.info(f"[DEBUG] å¼€å§‹è°ƒç”¨LLMç”Ÿæˆè®¡åˆ’")
        response = self.celue_client.chat.completions.create(...)
        logging.info(f"[DEBUG] LLMè°ƒç”¨æˆåŠŸ")
        
        plan_json = response.choices[0].message.content
        logging.info(f"[DEBUG] LLMè¿”å›å†…å®¹ï¼ˆå‰100å­—ç¬¦ï¼‰: {plan_json[:100]}")
        
        plan_json = plan_json.replace("", "").replace("```", "").strip()
        logging.info(f"[DEBUG] å¼€å§‹è§£æJSON")
        self.interview_plan = json.loads(plan_json)
        logging.info(f"[DEBUG] JSONè§£ææˆåŠŸï¼Œé—®é¢˜æ•°é‡: {len(self.interview_plan['questions'])}")
        
        logging.info(f"é¢è¯•ID: {self.interview_id} - é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±{len(self.interview_plan['questions'])}ä¸ªé—®é¢˜")
        return self.interview_plan
        
    except Exception as e:
        logging.error(f"é¢è¯•ID: {self.interview_id} - ç”Ÿæˆé¢è¯•è®¡åˆ’å¤±è´¥: {e}", exc_info=True)  # æ·»åŠ  exc_info=True æ‰“å°å®Œæ•´å †æ ˆ
        return None---

## ğŸ“Š é—®é¢˜æ’æŸ¥ä¼˜å…ˆçº§

| ä¼˜å…ˆçº§ | é—®é¢˜å‡è®¾ | éªŒè¯æ–¹æ³• | å¯èƒ½æ€§ |
|--------|---------|---------|--------|
| **P0** | `generate_interview_plan()` æŠ›å‡ºå¼‚å¸¸ | æŸ¥çœ‹å®Œæ•´æ—¥å¿—ï¼Œæ·»åŠ è°ƒè¯•ä¿¡æ¯ | 80% |
| **P1** | JSONè§£æå¤±è´¥ | æ·»åŠ æ—¥å¿—æ‰“å°LLMè¿”å›å†…å®¹ | 15% |
| **P2** | `dialog_history` ç±»å‹ä¸å¯¹ | æ·»åŠ æ—¥å¿—æ‰“å°ç±»å‹å’Œå€¼ | 5% |

---

## ğŸ¯ ä¸‹ä¸€æ­¥å»ºè®®

1. **æ·»åŠ ä¸Šè¿°æ‰€æœ‰è°ƒè¯•æ—¥å¿—**
2. **é‡æ–°è¿è¡ŒPostmanæµ‹è¯•**
3. **æŸ¥çœ‹å®Œæ•´çš„æ—¥å¿—è¾“å‡º**ï¼ˆåŒ…æ‹¬é”™è¯¯æ—¥å¿—ï¼‰
4. **æ ¹æ®æ—¥å¿—è¾“å‡ºç¡®å®šé—®é¢˜æ ¹å› **

å¦‚æœä½ åŒæ„è¿™ä¸ªåˆ†æï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”Ÿæˆå®Œæ•´çš„å¸¦è°ƒè¯•æ—¥å¿—çš„ä»£ç ã€‚éœ€è¦æˆ‘ç°åœ¨å°±ç”Ÿæˆå—ï¼Ÿ
```

```python
dialog_history = json_data.get("dialogHistory")
```

```python
return
```

```python
# ç¬¬29è¡Œä¹‹å
dialog_history = json_data.get("dialogHistory")
logging.info(f"[DEBUG] è·¯ç”±å±‚æ”¶åˆ°çš„ dialogHistory: {dialog_history}, ç±»å‹: {type(dialog_history)}, é•¿åº¦: {len(dialog_history) if dialog_history else 'N/A'}")
```

```python
# ç¬¬154è¡Œä¹‹å
logging.info(f"[DEBUG] get_ai_response æ”¶åˆ°çš„ dialog_history: {dialog_history}, ç±»å‹: {type(dialog_history)}")
logging.info(f"[DEBUG] åˆ¤æ–­æ¡ä»¶: not dialog_history = {not dialog_history}, len = {len(dialog_history) if dialog_history else 'N/A'}")
```

```python
if not dialog_history or len(dialog_history) == 0:
    logging.info(f"[DEBUG] è¿›å…¥é¦–æ¬¡è°ƒç”¨åˆ†æ”¯")
    # é¦–æ¬¡è°ƒç”¨ï¼šç”Ÿæˆé¢è¯•è®¡åˆ’
    await self.generate_interview_plan()
    logging.info(f"[DEBUG] ç”Ÿæˆè®¡åˆ’å®Œæˆï¼Œinterview_plan æ˜¯å¦ä¸ºç©º: {self.interview_plan is None}")
    
    # è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆå¼€åœºç™½ï¼‰
    if self.interview_plan and len(self.interview_plan['questions']) > 0:
        logging.info(f"[DEBUG] å‡†å¤‡è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜")
        ...
    else:
        logging.warning(f"[DEBUG] é¢è¯•è®¡åˆ’ä¸ºç©ºæˆ–æ²¡æœ‰é—®é¢˜ï¼Œç»§ç»­æ‰§è¡ŒåŸæœ‰æµç¨‹")
```

```python
async def generate_interview_plan(self):
    """ç”Ÿæˆé¢è¯•è®¡åˆ’ï¼ˆä»…é¦–æ¬¡è°ƒç”¨ï¼‰"""
    logging.info(f"[DEBUG] å¼€å§‹ç”Ÿæˆé¢è¯•è®¡åˆ’")
    try:
        logging.info(f"[DEBUG] åŠ è½½promptæ–‡ä»¶")
        plan_prompt = load_prompt("generate_plan_white_collar.txt")
        
        logging.info(f"[DEBUG] å‡†å¤‡æ ¼å¼åŒ–promptï¼Œå‚æ•°: user_portrait={self.user_portrait[:30]}..., jd_title={self.jd_title}")
        plan_prompt_filled = plan_prompt.format(
            user_portrait=self.user_portrait,
            jd_title=self.jd_title,
            jd_portrait=self.jd_portrait
        )
        logging.info(f"[DEBUG] promptæ ¼å¼åŒ–æˆåŠŸ")
        
        logging.info(f"[DEBUG] å¼€å§‹è°ƒç”¨LLMç”Ÿæˆè®¡åˆ’")
        response = self.celue_client.chat.completions.create(...)
        logging.info(f"[DEBUG] LLMè°ƒç”¨æˆåŠŸ")
        
        plan_json = response.choices[0].message.content
        logging.info(f"[DEBUG] LLMè¿”å›å†…å®¹ï¼ˆå‰100å­—ç¬¦ï¼‰: {plan_json[:100]}")
        
        plan_json = plan_json.replace("", "").replace("```", "").strip()
        logging.info(f"[DEBUG] å¼€å§‹è§£æJSON")
        self.interview_plan = json.loads(plan_json)
        logging.info(f"[DEBUG] JSONè§£ææˆåŠŸï¼Œé—®é¢˜æ•°é‡: {len(self.interview_plan['questions'])}")
        
        logging.info(f"é¢è¯•ID: {self.interview_id} - é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±{len(self.interview_plan['questions'])}ä¸ªé—®é¢˜")
        return self.interview_plan
        
    except Exception as e:
        logging.error(f"é¢è¯•ID: {self.interview_id} - ç”Ÿæˆé¢è¯•è®¡åˆ’å¤±è´¥: {e}", exc_info=True)  # æ·»åŠ  exc_info=True æ‰“å°å®Œæ•´å †æ ˆ
        return None
```

---

**User**

å¯ä»¥å•Šã€‚ç›®å‰çš„è¯åªèƒ½é€šè¿‡å¢åŠ æ—¥å¿—è¾“å‡ºæ¥è¿›è¡Œåˆ¤æ–­å§ã€‚ç„¶åä½ èƒ½ä¸èƒ½å¸®æˆ‘åœ¨å„ä¸ªä½ç½®ä¸ŠåŠ ä¸Šç›¸åº”çš„æ—¥å¿—è¾“å‡ºå‘¢ï¼Ÿç„¶åå†æ¬¡è¿è¡Œä¸€ä¸‹ç„¶åä¹‹åå†æŠŠæ—¥å¿—çš„è¾“å‡ºå‘ç»™ä½ ä½ å†å¸®æˆ‘æ¢³ç†ä¸€ä¸‹å¯ä»¥å—ï¼Ÿ

---

**Cursor**

å¥½çš„ï¼æˆ‘ç°åœ¨å¸®ä½ åœ¨å…³é”®ä½ç½®æ·»åŠ è°ƒè¯•æ—¥å¿—ã€‚æˆ‘ä¼šåœ¨è·¯ç”±å±‚å’ŒæœåŠ¡å±‚éƒ½åŠ ä¸Šè¯¦ç»†çš„æ—¥å¿—è¾“å‡ºã€‚

```python
import logging
from datetime import datetime
from fastapi import Request, APIRouter
from starlette.responses import StreamingResponse
from service.virtual_interview import CallInterview

router = APIRouter(prefix='')


#é¢è¯•
@router.post('/virturlInterview')
async def call_interview(request: Request):
    """
    åˆå§‹åŒ–é¢è¯•
    :param request:
    :param background_task:
    :return:
    """
    logging.info(f"æ—¶é—´ï¼š[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}]--æ”¶åˆ°é¢è¯•çš„ç½‘ç»œè¯·æ±‚ ")

    json_data = await request.json()
    user_portrait = json_data.get("userPortrait")
    jd_title = json_data.get("jdTitle")
    jd_portrait = json_data.get("jdPortrait")
    # interview_type 1ç™½é¢†ï¼Œ0è“é¢†
    interview_type = json_data.get("interviewType")
    #zhongwen
    interview_language = json_data.get("interviewLanguage")
    dialog_history=json_data.get("dialogHistory")
    interview_id=json_data.get("id")
    
    # è°ƒè¯•æ—¥å¿—
    logging.info(f"[DEBUG-è·¯ç”±å±‚] interview_id: {interview_id}")
    logging.info(f"[DEBUG-è·¯ç”±å±‚] dialogHistory ç±»å‹: {type(dialog_history)}, å€¼: {dialog_history}")
    logging.info(f"[DEBUG-è·¯ç”±å±‚] dialogHistory é•¿åº¦: {len(dialog_history) if dialog_history is not None else 'None'}")
    logging.info(f"[DEBUG-è·¯ç”±å±‚] dialogHistory æ˜¯å¦ä¸ºç©ºåˆ—è¡¨: {dialog_history == []}")
    interview=CallInterview(interview_id)
    interview.user_portrait=user_portrait
    interview.jd_title=jd_title
    interview.jd_portrait=jd_portrait
    interview.interview_type=interview_type
    interview.interview_language=interview_language
    headers = {
        "Content-Type": "text/event-stream; charset=utf-8",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
    }
    return StreamingResponse(interview.get_ai_response(dialog_history),media_type='text/event-stream', headers=headers)






```

```python
import asyncio
import json
import logging
import time
import re
import regex
from pathlib import Path
from openai import OpenAI, AsyncOpenAI
import dashscope
import dashscope.audio.tts_v2
from app.config import ai_config
from sdk.tts.punctuation_based_tts import PunctuationBasedTTS


def load_prompt(filename: str) -> str:
    """
    åŠ è½½promptæ–‡ä»¶
    
    Args:
        filename: æ–‡ä»¶åï¼Œå¦‚ "strategy_white_collar.txt"
    
    Returns:
        promptå†…å®¹
    """
    current_dir = Path(__file__).parent
    prompt_path = current_dir.parent.parent / "prompts" / "interview" / filename
    with open(prompt_path, 'r', encoding='utf-8') as f:
        return f.read()



dashscope.api_key = ai_config['api_key']
class CallInterview:
    def __init__(self,  interview_id: str):

        self.interview_id = interview_id
        self.ai_config = ai_config
        self.request_id = None
        self.current_question = None
        self.timeout_task = None
        self.voice_active = False
        self.jd_title = None
        self.jd_portrait = None
        self.user_portrait = None
        self.company_portrait = None
        self.company_name = None
        self.interview_type = 1
        self.speech_speed = 1.2
        self.interview_language = "zhongwen"
        self.interview_style = None
        self.TTS_model = 'longanxuan'
        self.interview_plan = None
        self.current_question_index = 0
        self.language_dict = {
            'zhongwen': ('ä¸­æ–‡', 'ä½ å¥½ï¼Œé¢è¯•å®˜ï¼è®©æˆ‘ä»¬ç”¨ä¸­æ–‡äº¤æµï¼', 'longanxuan'),
            'yingwen': ('è‹±æ–‡', "Hello, interviewer!Let's communicate in English!", 'longxiaochun_v2'),
            'riyu': ('æ—¥è¯­', 'ã“ã‚“ã«ã¡ã¯ã€é¢æ¥å®˜ï¼æ—¥æœ¬èªã§è©±ã—åˆã„ã¾ã—ã‚‡ã†ï¼', 'loongtomoka_v2'),
            'hanyu': ('éŸ©è¯­', 'ì•ˆë…•í•˜ì„¸ìš”, ë©´ì ‘ê´€ë‹˜!í•œêµ­ì–´ë¡œ ëŒ€í™”í•´ìš”!', 'loongkyong_v2'),
            'xibanyayu': ('è¥¿ç­ç‰™è¯­', 'Â¡Hola, entrevistador!Â¡Hablemos en espaÃ±ol!', 'sambert-camila-v1'),
            'yinniyu': ('å°å°¼è¯­', 'Halo, pewawancara!Ayo berkomunikasi dalam bahasa Indonesia!', 'sambert-indah-v1'),
            'fayu': ('æ³•è¯­', 'Bonjour, intervieweur !Parlons en franÃ§ais !', 'sambert-clara-v1'),
            'deyu': ('å¾·è¯­', 'Hallo, Interviewer!Lass uns auf Deutsch sprechen!', 'sambert-hanna-v1'),
            'taiyu': ('æ³°è¯­', 'à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¹ˆà¸°/à¸„à¸£à¸±à¸š, à¸œà¸¹à¹‰à¸ªà¸±à¸¡à¸ à¸²à¸©à¸“à¹Œ!à¸¡à¸²à¸à¸¹à¸”à¸„à¸¸à¸¢à¸à¸±à¸™à¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸à¸±à¸™à¹€à¸–à¸­à¸°!', 'sambert-waan-v1')
        }




        # åˆ›å»º OpenAI å®¢æˆ·ç«¯å¯¹è±¡
        self.celue_client = OpenAI(
            api_key=self.ai_config['api_key'],
            base_url=self.ai_config['api_uri'],
            timeout=60.0,
        )
        self.AI_client = AsyncOpenAI(
            api_key=self.ai_config['api_key'],
            base_url=self.ai_config['api_uri'],
            timeout=30.0,
        )
        # 1. è·å–å½“å‰æ­£åœ¨è¿è¡Œçš„äº‹ä»¶å¾ªç¯
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        class FutureCallback(dashscope.audio.tts_v2.ResultCallback):
            def __init__(self, future: asyncio.Future):
                self._future = future
                self._audio_buffer = bytearray()  # ç”¨äºæ‹¼æ¥éŸ³é¢‘ç‰‡æ®µ

            def on_open(self):
                pass  # è¿™é‡Œå¯ä»¥åŠ æ—¥å¿—

            def on_complete(self):
                # è¯­éŸ³åˆæˆå®Œæˆï¼Œæ‰€æœ‰ç‰‡æ®µéƒ½å·²æ”¶åˆ°
                # å°†æ‹¼æ¥å¥½çš„å®Œæ•´éŸ³é¢‘æ•°æ®ä½œä¸ºFutureçš„ç»“æœ
                if not self._future.done():
                    self._future.set_result(bytes(self._audio_buffer))

            def on_error(self, message: str):
                # å‘ç”Ÿé”™è¯¯ï¼Œå°†å¼‚å¸¸ä½œä¸ºFutureçš„ç»“æœ
                if not self._future.done():
                    self._future.set_exception(Exception(f"TTS Error: {message}"))

            def on_close(self):
                # è¿æ¥å…³é—­ï¼Œå¦‚æœFutureè¿˜æœªå®Œæˆï¼ˆå¯èƒ½åœ¨on_completeå‰å…³é—­ï¼‰ï¼Œä¹Ÿè®¾ç½®ç»“æœ
                if not self._future.done():
                    self._future.set_result(bytes(self._audio_buffer))

            def on_data(self, data: bytes):
                # æ”¶åˆ°éŸ³é¢‘ç‰‡æ®µï¼Œå°†å…¶æ‹¼æ¥åˆ°ç¼“å†²åŒº
                if data:
                    self._audio_buffer.extend(data)

        # 3. å°†æ–°çš„Callbackç±»ç»‘å®šåˆ°selfï¼Œä»¥ä¾¿synthesis_ttsæ–¹æ³•ä½¿ç”¨
        self.FutureCallback = FutureCallback


    async def generate_interview_plan(self):
        """ç”Ÿæˆé¢è¯•è®¡åˆ’ï¼ˆä»…é¦–æ¬¡è°ƒç”¨ï¼‰"""
        try:
            plan_prompt = load_prompt("generate_plan_white_collar.txt")
            plan_prompt_filled = plan_prompt.format(
                user_portrait=self.user_portrait,
                jd_title=self.jd_title,
                jd_portrait=self.jd_portrait
            )
            
            response = self.celue_client.chat.completions.create(
                model=self.ai_config['api_model'],
                temperature=0.7,
                messages=[
                    {"role": "system", "content": plan_prompt_filled},
                    {"role": "user", "content": "è¯·ç”Ÿæˆé¢è¯•è®¡åˆ’"}
                ]
            )
            
            plan_json = response.choices[0].message.content
            # æ¸…ç†å¯èƒ½çš„markdownæ ‡è®°
            plan_json = plan_json.replace("```json", "").replace("```", "").strip()
            self.interview_plan = json.loads(plan_json)
            
            logging.info(f"é¢è¯•ID: {self.interview_id} - é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±{len(self.interview_plan['questions'])}ä¸ªé—®é¢˜")
            return self.interview_plan
            
        except Exception as e:
            logging.error(f"é¢è¯•ID: {self.interview_id} - ç”Ÿæˆé¢è¯•è®¡åˆ’å¤±è´¥: {e}")
            # å¤±è´¥æ—¶è¿”å›ç©ºè®¡åˆ’ï¼Œèµ°åŸæœ‰é€»è¾‘
            return None


    async def get_ai_response(self, dialog_history):
        start_time = time.time()

        # è°ƒè¯•æ—¥å¿—
        logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - get_ai_response è¢«è°ƒç”¨")
        logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - dialog_history ç±»å‹: {type(dialog_history)}")
        logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - dialog_history å€¼: {dialog_history}")
        logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - not dialog_history = {not dialog_history}")
        logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - len(dialog_history) = {len(dialog_history) if dialog_history is not None else 'None'}")

        # åˆ¤æ–­æ˜¯å¦é¦–æ¬¡è°ƒç”¨
        if not dialog_history or len(dialog_history) == 0:
            logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - âœ… è¿›å…¥é¦–æ¬¡è°ƒç”¨åˆ†æ”¯")
            # é¦–æ¬¡è°ƒç”¨ï¼šç”Ÿæˆé¢è¯•è®¡åˆ’
            await self.generate_interview_plan()
            
            # è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆå¼€åœºç™½ï¼‰
            if self.interview_plan and len(self.interview_plan['questions']) > 0:
                first_question = self.interview_plan['questions'][0]['question']
                self.current_question_index = 1  # ä¸‹æ¬¡ä»ç¬¬äºŒä¸ªé—®é¢˜å¼€å§‹
                
                yield f"data: {json.dumps({'type': 'chunk', 'content': first_question}, ensure_ascii=False)}\n\n"
                yield f"data: {json.dumps({'type': 'DONE', 'content': '[DONE]'}, ensure_ascii=False)}\n\n"
                return

        language, say_hi, self.TTS_model = self.language_dict[self.interview_language]
        system_message = f'''
                        æœ¬æ¬¡é¢è¯•å‚è€ƒèµ„æ–™å¦‚ä¸‹ï¼š
                        #é¢è¯•äº¤æµå…¨ç¨‹ä½¿ç”¨è¯­ç§ï¼š{language}
                        é¢è¯•å¼€å§‹æ—¶é—´ï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}
                        å€™é€‰äººä¿¡æ¯ï¼š{self.user_portrait}
                        å²—ä½ä¿¡æ¯ï¼š{self.jd_title},{self.jd_portrait}
                        å…¬å¸ä¿¡æ¯ï¼š{self.company_portrait}
                        #é¢è¯•äº¤æµå…¨ç¨‹ä½¿ç”¨è¯­ç§ï¼š{language}
                        é¢è¯•æ­£å¼å¼€å§‹ï¼
                                        '''

        self.speech_speed = 1
        
        # åŠ è½½promptæ¨¡æ¿
        celue_template = load_prompt("strategy_white_collar.txt")
        zhixing_template = load_prompt("executor_white_collar.txt")
        lanling_template = load_prompt("executor_blue_collar.txt")
        
        # æ›¿æ¢å˜é‡
        lang = self.language_dict[self.interview_language][0]
        self.celue_sys_prompt = celue_template.format(language=lang)
        self.zhixing_sys_prompt = zhixing_template.format(language=lang)
        self.lanling_sys_prompt = lanling_template.format(language=lang)
        try:
            # å‡†å¤‡é¢è¯•è®¡åˆ’ä¿¡æ¯
            plan_info = ""
            if self.interview_plan:
                remaining_questions = self.interview_plan['questions'][self.current_question_index:]
                plan_info = f"""
                ã€é¢è¯•è®¡åˆ’å‚è€ƒã€‘
                å½“å‰è¿›åº¦ï¼šå·²æé—® {self.current_question_index}/{len(self.interview_plan['questions'])} ä¸ªé—®é¢˜
                å¾…æé—®é¢˜ï¼š{json.dumps(remaining_questions[:3], ensure_ascii=False)}
                ï¼ˆæ³¨æ„ï¼šè¿™åªæ˜¯å‚è€ƒè®¡åˆ’ï¼Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µçµæ´»è°ƒæ•´ï¼‰
                """
            
            #interview_type 1ç™½é¢†ï¼Œ0è“é¢†
            if self.interview_type == 1:
                self.speech_speed = 1.1
                celue_message = [{"role": "system", "content": self.celue_sys_prompt},
                                 {"role": "user",
                                  "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}ï¼Œå½“å‰æ—¶é—´ä¸º{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}{plan_info}"}]
                logging.info(f"-- é¢è¯•ID ï¼š{self.interview_id},å½“å‰ç­–ç•¥promptå¦‚ä¸‹--ï¼š{celue_message}")
                for i in range(2):
                    try:
                        celue = self.celue_client.chat.completions.create(
                            model=self.ai_config['api_model'],
                            temperature=0.4,
                            messages=celue_message,
                            extra_body={"chat_template_kwargs": {"enable_thinking": True}},
                        )
                        celue_text = celue.choices[0].message.content
                        break
                    except Exception as e:
                        logging.error(f"-- é¢è¯•ID : {self.interview_id}---ç­–ç•¥è°ƒç”¨AIå‡ºé”™ï¼Œç¬¬{i + 1}æ¬¡å°è¯•: {e}")
                logging.info(
                    f"-- é¢è¯•ID ï¼š{self.interview_id},æ¨ç†æ—¶é—´ï¼š{time.time() - start_time}s,å½“å‰ç­–ç•¥å¦‚ä¸‹--ï¼š\n{celue_text}")
                # å¯ç”¨æµå¼ä¼ è¾“
                zhixing_message = [{"role": "system", "content": self.zhixing_sys_prompt},
                                   {"role": "user",
                                    "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history},å½“å‰æ—¶é—´ä¸ºï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}ï¼Œå½“å‰å›å¤ç­–ç•¥å¦‚ä¸‹{celue_text}{plan_info}"}]
            else:
                zhixing_message = [{"role": "system", "content": self.lanling_sys_prompt},
                                   {"role": "user",
                                    "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}ï¼Œå½“å‰æ—¶é—´ä¸º{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}"}]
            logging.info(f"-- é¢è¯•ID ï¼š{self.interview_id},å½“å‰æ‰§è¡Œpromptå¦‚ä¸‹--ï¼š{zhixing_message}")
            #é‡è¯•æ¬¡æ•°
            for i in range(2):
                try:
                    response = await self.AI_client.chat.completions.create(
                        stream=True,  # å¯ç”¨æµå¼
                        model=self.ai_config['api_model'],
                        temperature=0.7,
                        seed=1,
                        messages=zhixing_message
                    )
                    break
                except Exception as e:
                    logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµå¼è°ƒç”¨AIå‡ºé”™ï¼Œç¬¬{i + 1}æ¬¡å°è¯•: {e}")
            if not response:
                logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµå¼è°ƒç”¨AIå¤±è´¥")
            ai_response = ""
            async for chunk in response:
                if chunk.choices and chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    ai_response += content

                    yield f"data: {json.dumps({'type': 'chunk', 'content': content}, ensure_ascii=False)}\n\n"

            yield f"data: {json.dumps({'type': 'DONE', 'content': "[DONE]"}, ensure_ascii=False)}\n\n"

            # æ›´æ–°é—®é¢˜ç´¢å¼•
            if self.interview_plan:
                self.current_question_index += 1

            end_time = time.time()
            execution_time = end_time - start_time
            logging.info(f"-- é¢è¯•ID : {self.interview_id}---æ‰§è¡Œæ—¶é—´ä¸º: {execution_time} ç§’,AIæµå¼è¿”å›å†…å®¹: {ai_response}")
            try:
                flash_time = time.time()
                #åˆ¤æ–­é¢è¯•æ˜¯å¦ç»“æŸï¼Œæ¯ä¸ªå¾ªç¯éƒ½æœ‰
                liucheng_prompt = load_prompt("judge_flow.txt")
                liucheng = self.celue_client.chat.completions.create(
                    model="qwen-flash",
                    temperature=0.4,
                    messages=[{"role": "system", "content": liucheng_prompt},
                              {"role": "user", "content": f"é¢è¯•å®˜å›å¤å†…å®¹å¦‚ä¸‹{ai_response}"}],
                )
                liucheng_flag = liucheng.choices[0].message.content
                logging.info(
                    f"-- é¢è¯•ID : {self.interview_id}---æµç¨‹åˆ¤æ–­å†…å®¹:  {liucheng_flag}  ,è€—æ—¶{time.time() - flash_time}ç§’")
                if "ç»“æŸ" in liucheng_flag:
                    yield f"data: {json.dumps({'type': 'over', 'content': "over"}, ensure_ascii=False)}\n\n"
            except Exception as e:
                logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµç¨‹åˆ¤æ–­å‡ºé”™: {e}")
                yield f"data: {json.dumps({'type': 'error', 'content': str(e)}, ensure_ascii=False)}\n\n"

        except Exception as e:
            logging.error(f"-- é¢è¯•ID : {self.interview_id}---AIæµå¼å“åº”è·å–é”™è¯¯: {e}")
            yield f"data: {json.dumps({'type': 'error', 'content': "æ¨¡å‹æ£€æµ‹åˆ°éæ³•è¾“å…¥ï¼Œè¯·é‡æ–°å¼€å§‹æœ¬æ¬¡é¢è¯•"+str(e)}, ensure_ascii=False)}\n\n"






```

```python
import asyncio
import json
import logging
import time
import re
import regex
from pathlib import Path
from openai import OpenAI, AsyncOpenAI
import dashscope
import dashscope.audio.tts_v2
from app.config import ai_config
from sdk.tts.punctuation_based_tts import PunctuationBasedTTS


def load_prompt(filename: str) -> str:
    """
    åŠ è½½promptæ–‡ä»¶
    
    Args:
        filename: æ–‡ä»¶åï¼Œå¦‚ "strategy_white_collar.txt"
    
    Returns:
        promptå†…å®¹
    """
    current_dir = Path(__file__).parent
    prompt_path = current_dir.parent.parent / "prompts" / "interview" / filename
    with open(prompt_path, 'r', encoding='utf-8') as f:
        return f.read()



dashscope.api_key = ai_config['api_key']
class CallInterview:
    def __init__(self,  interview_id: str):

        self.interview_id = interview_id
        self.ai_config = ai_config
        self.request_id = None
        self.current_question = None
        self.timeout_task = None
        self.voice_active = False
        self.jd_title = None
        self.jd_portrait = None
        self.user_portrait = None
        self.company_portrait = None
        self.company_name = None
        self.interview_type = 1
        self.speech_speed = 1.2
        self.interview_language = "zhongwen"
        self.interview_style = None
        self.TTS_model = 'longanxuan'
        self.interview_plan = None
        self.current_question_index = 0
        self.language_dict = {
            'zhongwen': ('ä¸­æ–‡', 'ä½ å¥½ï¼Œé¢è¯•å®˜ï¼è®©æˆ‘ä»¬ç”¨ä¸­æ–‡äº¤æµï¼', 'longanxuan'),
            'yingwen': ('è‹±æ–‡', "Hello, interviewer!Let's communicate in English!", 'longxiaochun_v2'),
            'riyu': ('æ—¥è¯­', 'ã“ã‚“ã«ã¡ã¯ã€é¢æ¥å®˜ï¼æ—¥æœ¬èªã§è©±ã—åˆã„ã¾ã—ã‚‡ã†ï¼', 'loongtomoka_v2'),
            'hanyu': ('éŸ©è¯­', 'ì•ˆë…•í•˜ì„¸ìš”, ë©´ì ‘ê´€ë‹˜!í•œêµ­ì–´ë¡œ ëŒ€í™”í•´ìš”!', 'loongkyong_v2'),
            'xibanyayu': ('è¥¿ç­ç‰™è¯­', 'Â¡Hola, entrevistador!Â¡Hablemos en espaÃ±ol!', 'sambert-camila-v1'),
            'yinniyu': ('å°å°¼è¯­', 'Halo, pewawancara!Ayo berkomunikasi dalam bahasa Indonesia!', 'sambert-indah-v1'),
            'fayu': ('æ³•è¯­', 'Bonjour, intervieweur !Parlons en franÃ§ais !', 'sambert-clara-v1'),
            'deyu': ('å¾·è¯­', 'Hallo, Interviewer!Lass uns auf Deutsch sprechen!', 'sambert-hanna-v1'),
            'taiyu': ('æ³°è¯­', 'à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¹ˆà¸°/à¸„à¸£à¸±à¸š, à¸œà¸¹à¹‰à¸ªà¸±à¸¡à¸ à¸²à¸©à¸“à¹Œ!à¸¡à¸²à¸à¸¹à¸”à¸„à¸¸à¸¢à¸à¸±à¸™à¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸à¸±à¸™à¹€à¸–à¸­à¸°!', 'sambert-waan-v1')
        }




        # åˆ›å»º OpenAI å®¢æˆ·ç«¯å¯¹è±¡
        self.celue_client = OpenAI(
            api_key=self.ai_config['api_key'],
            base_url=self.ai_config['api_uri'],
            timeout=60.0,
        )
        self.AI_client = AsyncOpenAI(
            api_key=self.ai_config['api_key'],
            base_url=self.ai_config['api_uri'],
            timeout=30.0,
        )
        # 1. è·å–å½“å‰æ­£åœ¨è¿è¡Œçš„äº‹ä»¶å¾ªç¯
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        class FutureCallback(dashscope.audio.tts_v2.ResultCallback):
            def __init__(self, future: asyncio.Future):
                self._future = future
                self._audio_buffer = bytearray()  # ç”¨äºæ‹¼æ¥éŸ³é¢‘ç‰‡æ®µ

            def on_open(self):
                pass  # è¿™é‡Œå¯ä»¥åŠ æ—¥å¿—

            def on_complete(self):
                # è¯­éŸ³åˆæˆå®Œæˆï¼Œæ‰€æœ‰ç‰‡æ®µéƒ½å·²æ”¶åˆ°
                # å°†æ‹¼æ¥å¥½çš„å®Œæ•´éŸ³é¢‘æ•°æ®ä½œä¸ºFutureçš„ç»“æœ
                if not self._future.done():
                    self._future.set_result(bytes(self._audio_buffer))

            def on_error(self, message: str):
                # å‘ç”Ÿé”™è¯¯ï¼Œå°†å¼‚å¸¸ä½œä¸ºFutureçš„ç»“æœ
                if not self._future.done():
                    self._future.set_exception(Exception(f"TTS Error: {message}"))

            def on_close(self):
                # è¿æ¥å…³é—­ï¼Œå¦‚æœFutureè¿˜æœªå®Œæˆï¼ˆå¯èƒ½åœ¨on_completeå‰å…³é—­ï¼‰ï¼Œä¹Ÿè®¾ç½®ç»“æœ
                if not self._future.done():
                    self._future.set_result(bytes(self._audio_buffer))

            def on_data(self, data: bytes):
                # æ”¶åˆ°éŸ³é¢‘ç‰‡æ®µï¼Œå°†å…¶æ‹¼æ¥åˆ°ç¼“å†²åŒº
                if data:
                    self._audio_buffer.extend(data)

        # 3. å°†æ–°çš„Callbackç±»ç»‘å®šåˆ°selfï¼Œä»¥ä¾¿synthesis_ttsæ–¹æ³•ä½¿ç”¨
        self.FutureCallback = FutureCallback


    async def generate_interview_plan(self):
        """ç”Ÿæˆé¢è¯•è®¡åˆ’ï¼ˆä»…é¦–æ¬¡è°ƒç”¨ï¼‰"""
        try:
            plan_prompt = load_prompt("generate_plan_white_collar.txt")
            plan_prompt_filled = plan_prompt.format(
                user_portrait=self.user_portrait,
                jd_title=self.jd_title,
                jd_portrait=self.jd_portrait
            )
            
            response = self.celue_client.chat.completions.create(
                model=self.ai_config['api_model'],
                temperature=0.7,
                messages=[
                    {"role": "system", "content": plan_prompt_filled},
                    {"role": "user", "content": "è¯·ç”Ÿæˆé¢è¯•è®¡åˆ’"}
                ]
            )
            
            plan_json = response.choices[0].message.content
            # æ¸…ç†å¯èƒ½çš„markdownæ ‡è®°
            plan_json = plan_json.replace("```json", "").replace("```", "").strip()
            self.interview_plan = json.loads(plan_json)
            
            logging.info(f"é¢è¯•ID: {self.interview_id} - é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±{len(self.interview_plan['questions'])}ä¸ªé—®é¢˜")
            return self.interview_plan
            
        except Exception as e:
            logging.error(f"é¢è¯•ID: {self.interview_id} - ç”Ÿæˆé¢è¯•è®¡åˆ’å¤±è´¥: {e}")
            # å¤±è´¥æ—¶è¿”å›ç©ºè®¡åˆ’ï¼Œèµ°åŸæœ‰é€»è¾‘
            return None


    async def get_ai_response(self, dialog_history):
        start_time = time.time()

        # è°ƒè¯•æ—¥å¿—
        logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - get_ai_response è¢«è°ƒç”¨")
        logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - dialog_history ç±»å‹: {type(dialog_history)}")
        logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - dialog_history å€¼: {dialog_history}")
        logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - not dialog_history = {not dialog_history}")
        logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - len(dialog_history) = {len(dialog_history) if dialog_history is not None else 'None'}")

        # åˆ¤æ–­æ˜¯å¦é¦–æ¬¡è°ƒç”¨
        if not dialog_history or len(dialog_history) == 0:
            logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - âœ… è¿›å…¥é¦–æ¬¡è°ƒç”¨åˆ†æ”¯")
            # é¦–æ¬¡è°ƒç”¨ï¼šç”Ÿæˆé¢è¯•è®¡åˆ’
            logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - å‡†å¤‡è°ƒç”¨ generate_interview_plan()")
            await self.generate_interview_plan()
            logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - generate_interview_plan() è°ƒç”¨å®Œæˆ")
            logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - interview_plan æ˜¯å¦ä¸º None: {self.interview_plan is None}")
            
            # è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆå¼€åœºç™½ï¼‰
            if self.interview_plan and len(self.interview_plan['questions']) > 0:
                logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - âœ… é¢è¯•è®¡åˆ’æœ‰æ•ˆï¼Œå‡†å¤‡è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜")
                first_question = self.interview_plan['questions'][0]['question']
                self.current_question_index = 1  # ä¸‹æ¬¡ä»ç¬¬äºŒä¸ªé—®é¢˜å¼€å§‹
                
                logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - ç¬¬ä¸€ä¸ªé—®é¢˜: {first_question[:50]}...")
                yield f"data: {json.dumps({'type': 'chunk', 'content': first_question}, ensure_ascii=False)}\n\n"
                yield f"data: {json.dumps({'type': 'DONE', 'content': '[DONE]'}, ensure_ascii=False)}\n\n"
                logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - âœ… å·²è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œå‡½æ•°å³å°†return")
                return
            else:
                logging.warning(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - âš ï¸ é¢è¯•è®¡åˆ’ä¸ºç©ºæˆ–æ²¡æœ‰é—®é¢˜ï¼Œç»§ç»­æ‰§è¡ŒåŸæœ‰æµç¨‹")

        language, say_hi, self.TTS_model = self.language_dict[self.interview_language]
        system_message = f'''
                        æœ¬æ¬¡é¢è¯•å‚è€ƒèµ„æ–™å¦‚ä¸‹ï¼š
                        #é¢è¯•äº¤æµå…¨ç¨‹ä½¿ç”¨è¯­ç§ï¼š{language}
                        é¢è¯•å¼€å§‹æ—¶é—´ï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}
                        å€™é€‰äººä¿¡æ¯ï¼š{self.user_portrait}
                        å²—ä½ä¿¡æ¯ï¼š{self.jd_title},{self.jd_portrait}
                        å…¬å¸ä¿¡æ¯ï¼š{self.company_portrait}
                        #é¢è¯•äº¤æµå…¨ç¨‹ä½¿ç”¨è¯­ç§ï¼š{language}
                        é¢è¯•æ­£å¼å¼€å§‹ï¼
                                        '''

        self.speech_speed = 1
        
        # åŠ è½½promptæ¨¡æ¿
        celue_template = load_prompt("strategy_white_collar.txt")
        zhixing_template = load_prompt("executor_white_collar.txt")
        lanling_template = load_prompt("executor_blue_collar.txt")
        
        # æ›¿æ¢å˜é‡
        lang = self.language_dict[self.interview_language][0]
        self.celue_sys_prompt = celue_template.format(language=lang)
        self.zhixing_sys_prompt = zhixing_template.format(language=lang)
        self.lanling_sys_prompt = lanling_template.format(language=lang)
        try:
            # å‡†å¤‡é¢è¯•è®¡åˆ’ä¿¡æ¯
            plan_info = ""
            if self.interview_plan:
                remaining_questions = self.interview_plan['questions'][self.current_question_index:]
                plan_info = f"""
                ã€é¢è¯•è®¡åˆ’å‚è€ƒã€‘
                å½“å‰è¿›åº¦ï¼šå·²æé—® {self.current_question_index}/{len(self.interview_plan['questions'])} ä¸ªé—®é¢˜
                å¾…æé—®é¢˜ï¼š{json.dumps(remaining_questions[:3], ensure_ascii=False)}
                ï¼ˆæ³¨æ„ï¼šè¿™åªæ˜¯å‚è€ƒè®¡åˆ’ï¼Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µçµæ´»è°ƒæ•´ï¼‰
                """
            
            #interview_type 1ç™½é¢†ï¼Œ0è“é¢†
            if self.interview_type == 1:
                self.speech_speed = 1.1
                celue_message = [{"role": "system", "content": self.celue_sys_prompt},
                                 {"role": "user",
                                  "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}ï¼Œå½“å‰æ—¶é—´ä¸º{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}{plan_info}"}]
                logging.info(f"-- é¢è¯•ID ï¼š{self.interview_id},å½“å‰ç­–ç•¥promptå¦‚ä¸‹--ï¼š{celue_message}")
                for i in range(2):
                    try:
                        celue = self.celue_client.chat.completions.create(
                            model=self.ai_config['api_model'],
                            temperature=0.4,
                            messages=celue_message,
                            extra_body={"chat_template_kwargs": {"enable_thinking": True}},
                        )
                        celue_text = celue.choices[0].message.content
                        break
                    except Exception as e:
                        logging.error(f"-- é¢è¯•ID : {self.interview_id}---ç­–ç•¥è°ƒç”¨AIå‡ºé”™ï¼Œç¬¬{i + 1}æ¬¡å°è¯•: {e}")
                logging.info(
                    f"-- é¢è¯•ID ï¼š{self.interview_id},æ¨ç†æ—¶é—´ï¼š{time.time() - start_time}s,å½“å‰ç­–ç•¥å¦‚ä¸‹--ï¼š\n{celue_text}")
                # å¯ç”¨æµå¼ä¼ è¾“
                zhixing_message = [{"role": "system", "content": self.zhixing_sys_prompt},
                                   {"role": "user",
                                    "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history},å½“å‰æ—¶é—´ä¸ºï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}ï¼Œå½“å‰å›å¤ç­–ç•¥å¦‚ä¸‹{celue_text}{plan_info}"}]
            else:
                zhixing_message = [{"role": "system", "content": self.lanling_sys_prompt},
                                   {"role": "user",
                                    "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}ï¼Œå½“å‰æ—¶é—´ä¸º{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}"}]
            logging.info(f"-- é¢è¯•ID ï¼š{self.interview_id},å½“å‰æ‰§è¡Œpromptå¦‚ä¸‹--ï¼š{zhixing_message}")
            #é‡è¯•æ¬¡æ•°
            for i in range(2):
                try:
                    response = await self.AI_client.chat.completions.create(
                        stream=True,  # å¯ç”¨æµå¼
                        model=self.ai_config['api_model'],
                        temperature=0.7,
                        seed=1,
                        messages=zhixing_message
                    )
                    break
                except Exception as e:
                    logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµå¼è°ƒç”¨AIå‡ºé”™ï¼Œç¬¬{i + 1}æ¬¡å°è¯•: {e}")
            if not response:
                logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµå¼è°ƒç”¨AIå¤±è´¥")
            ai_response = ""
            async for chunk in response:
                if chunk.choices and chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    ai_response += content

                    yield f"data: {json.dumps({'type': 'chunk', 'content': content}, ensure_ascii=False)}\n\n"

            yield f"data: {json.dumps({'type': 'DONE', 'content': "[DONE]"}, ensure_ascii=False)}\n\n"

            # æ›´æ–°é—®é¢˜ç´¢å¼•
            if self.interview_plan:
                self.current_question_index += 1

            end_time = time.time()
            execution_time = end_time - start_time
            logging.info(f"-- é¢è¯•ID : {self.interview_id}---æ‰§è¡Œæ—¶é—´ä¸º: {execution_time} ç§’,AIæµå¼è¿”å›å†…å®¹: {ai_response}")
            try:
                flash_time = time.time()
                #åˆ¤æ–­é¢è¯•æ˜¯å¦ç»“æŸï¼Œæ¯ä¸ªå¾ªç¯éƒ½æœ‰
                liucheng_prompt = load_prompt("judge_flow.txt")
                liucheng = self.celue_client.chat.completions.create(
                    model="qwen-flash",
                    temperature=0.4,
                    messages=[{"role": "system", "content": liucheng_prompt},
                              {"role": "user", "content": f"é¢è¯•å®˜å›å¤å†…å®¹å¦‚ä¸‹{ai_response}"}],
                )
                liucheng_flag = liucheng.choices[0].message.content
                logging.info(
                    f"-- é¢è¯•ID : {self.interview_id}---æµç¨‹åˆ¤æ–­å†…å®¹:  {liucheng_flag}  ,è€—æ—¶{time.time() - flash_time}ç§’")
                if "ç»“æŸ" in liucheng_flag:
                    yield f"data: {json.dumps({'type': 'over', 'content': "over"}, ensure_ascii=False)}\n\n"
            except Exception as e:
                logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµç¨‹åˆ¤æ–­å‡ºé”™: {e}")
                yield f"data: {json.dumps({'type': 'error', 'content': str(e)}, ensure_ascii=False)}\n\n"

        except Exception as e:
            logging.error(f"-- é¢è¯•ID : {self.interview_id}---AIæµå¼å“åº”è·å–é”™è¯¯: {e}")
            yield f"data: {json.dumps({'type': 'error', 'content': "æ¨¡å‹æ£€æµ‹åˆ°éæ³•è¾“å…¥ï¼Œè¯·é‡æ–°å¼€å§‹æœ¬æ¬¡é¢è¯•"+str(e)}, ensure_ascii=False)}\n\n"






```

```python
import asyncio
import json
import logging
import time
import re
import regex
from pathlib import Path
from openai import OpenAI, AsyncOpenAI
import dashscope
import dashscope.audio.tts_v2
from app.config import ai_config
from sdk.tts.punctuation_based_tts import PunctuationBasedTTS


def load_prompt(filename: str) -> str:
    """
    åŠ è½½promptæ–‡ä»¶
    
    Args:
        filename: æ–‡ä»¶åï¼Œå¦‚ "strategy_white_collar.txt"
    
    Returns:
        promptå†…å®¹
    """
    current_dir = Path(__file__).parent
    prompt_path = current_dir.parent.parent / "prompts" / "interview" / filename
    with open(prompt_path, 'r', encoding='utf-8') as f:
        return f.read()



dashscope.api_key = ai_config['api_key']
class CallInterview:
    def __init__(self,  interview_id: str):

        self.interview_id = interview_id
        self.ai_config = ai_config
        self.request_id = None
        self.current_question = None
        self.timeout_task = None
        self.voice_active = False
        self.jd_title = None
        self.jd_portrait = None
        self.user_portrait = None
        self.company_portrait = None
        self.company_name = None
        self.interview_type = 1
        self.speech_speed = 1.2
        self.interview_language = "zhongwen"
        self.interview_style = None
        self.TTS_model = 'longanxuan'
        self.interview_plan = None
        self.current_question_index = 0
        self.language_dict = {
            'zhongwen': ('ä¸­æ–‡', 'ä½ å¥½ï¼Œé¢è¯•å®˜ï¼è®©æˆ‘ä»¬ç”¨ä¸­æ–‡äº¤æµï¼', 'longanxuan'),
            'yingwen': ('è‹±æ–‡', "Hello, interviewer!Let's communicate in English!", 'longxiaochun_v2'),
            'riyu': ('æ—¥è¯­', 'ã“ã‚“ã«ã¡ã¯ã€é¢æ¥å®˜ï¼æ—¥æœ¬èªã§è©±ã—åˆã„ã¾ã—ã‚‡ã†ï¼', 'loongtomoka_v2'),
            'hanyu': ('éŸ©è¯­', 'ì•ˆë…•í•˜ì„¸ìš”, ë©´ì ‘ê´€ë‹˜!í•œêµ­ì–´ë¡œ ëŒ€í™”í•´ìš”!', 'loongkyong_v2'),
            'xibanyayu': ('è¥¿ç­ç‰™è¯­', 'Â¡Hola, entrevistador!Â¡Hablemos en espaÃ±ol!', 'sambert-camila-v1'),
            'yinniyu': ('å°å°¼è¯­', 'Halo, pewawancara!Ayo berkomunikasi dalam bahasa Indonesia!', 'sambert-indah-v1'),
            'fayu': ('æ³•è¯­', 'Bonjour, intervieweur !Parlons en franÃ§ais !', 'sambert-clara-v1'),
            'deyu': ('å¾·è¯­', 'Hallo, Interviewer!Lass uns auf Deutsch sprechen!', 'sambert-hanna-v1'),
            'taiyu': ('æ³°è¯­', 'à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¹ˆà¸°/à¸„à¸£à¸±à¸š, à¸œà¸¹à¹‰à¸ªà¸±à¸¡à¸ à¸²à¸©à¸“à¹Œ!à¸¡à¸²à¸à¸¹à¸”à¸„à¸¸à¸¢à¸à¸±à¸™à¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸à¸±à¸™à¹€à¸–à¸­à¸°!', 'sambert-waan-v1')
        }




        # åˆ›å»º OpenAI å®¢æˆ·ç«¯å¯¹è±¡
        self.celue_client = OpenAI(
            api_key=self.ai_config['api_key'],
            base_url=self.ai_config['api_uri'],
            timeout=60.0,
        )
        self.AI_client = AsyncOpenAI(
            api_key=self.ai_config['api_key'],
            base_url=self.ai_config['api_uri'],
            timeout=30.0,
        )
        # 1. è·å–å½“å‰æ­£åœ¨è¿è¡Œçš„äº‹ä»¶å¾ªç¯
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        class FutureCallback(dashscope.audio.tts_v2.ResultCallback):
            def __init__(self, future: asyncio.Future):
                self._future = future
                self._audio_buffer = bytearray()  # ç”¨äºæ‹¼æ¥éŸ³é¢‘ç‰‡æ®µ

            def on_open(self):
                pass  # è¿™é‡Œå¯ä»¥åŠ æ—¥å¿—

            def on_complete(self):
                # è¯­éŸ³åˆæˆå®Œæˆï¼Œæ‰€æœ‰ç‰‡æ®µéƒ½å·²æ”¶åˆ°
                # å°†æ‹¼æ¥å¥½çš„å®Œæ•´éŸ³é¢‘æ•°æ®ä½œä¸ºFutureçš„ç»“æœ
                if not self._future.done():
                    self._future.set_result(bytes(self._audio_buffer))

            def on_error(self, message: str):
                # å‘ç”Ÿé”™è¯¯ï¼Œå°†å¼‚å¸¸ä½œä¸ºFutureçš„ç»“æœ
                if not self._future.done():
                    self._future.set_exception(Exception(f"TTS Error: {message}"))

            def on_close(self):
                # è¿æ¥å…³é—­ï¼Œå¦‚æœFutureè¿˜æœªå®Œæˆï¼ˆå¯èƒ½åœ¨on_completeå‰å…³é—­ï¼‰ï¼Œä¹Ÿè®¾ç½®ç»“æœ
                if not self._future.done():
                    self._future.set_result(bytes(self._audio_buffer))

            def on_data(self, data: bytes):
                # æ”¶åˆ°éŸ³é¢‘ç‰‡æ®µï¼Œå°†å…¶æ‹¼æ¥åˆ°ç¼“å†²åŒº
                if data:
                    self._audio_buffer.extend(data)

        # 3. å°†æ–°çš„Callbackç±»ç»‘å®šåˆ°selfï¼Œä»¥ä¾¿synthesis_ttsæ–¹æ³•ä½¿ç”¨
        self.FutureCallback = FutureCallback


    async def generate_interview_plan(self):
        """ç”Ÿæˆé¢è¯•è®¡åˆ’ï¼ˆä»…é¦–æ¬¡è°ƒç”¨ï¼‰"""
        logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - ğŸ¯ è¿›å…¥ generate_interview_plan å‡½æ•°")
        try:
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - æ­¥éª¤1: åŠ è½½ prompt æ–‡ä»¶")
            plan_prompt = load_prompt("generate_plan_white_collar.txt")
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - prompt æ–‡ä»¶åŠ è½½æˆåŠŸï¼Œé•¿åº¦: {len(plan_prompt)}")
            
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - æ­¥éª¤2: æ ¼å¼åŒ– prompt")
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - user_portrait: {self.user_portrait[:50] if self.user_portrait else 'None'}...")
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - jd_title: {self.jd_title}")
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - jd_portrait: {self.jd_portrait[:50] if self.jd_portrait else 'None'}...")
            
            plan_prompt_filled = plan_prompt.format(
                user_portrait=self.user_portrait,
                jd_title=self.jd_title,
                jd_portrait=self.jd_portrait
            )
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - prompt æ ¼å¼åŒ–æˆåŠŸ")
            
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - æ­¥éª¤3: è°ƒç”¨ LLM ç”Ÿæˆè®¡åˆ’")
            response = self.celue_client.chat.completions.create(
                model=self.ai_config['api_model'],
                temperature=0.7,
                messages=[
                    {"role": "system", "content": plan_prompt_filled},
                    {"role": "user", "content": "è¯·ç”Ÿæˆé¢è¯•è®¡åˆ’"}
                ]
            )
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - LLM è°ƒç”¨æˆåŠŸ")
            
            plan_json = response.choices[0].message.content
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - LLM è¿”å›å†…å®¹ï¼ˆå‰200å­—ç¬¦ï¼‰: {plan_json[:200]}")
            
            # æ¸…ç†å¯èƒ½çš„markdownæ ‡è®°
            plan_json = plan_json.replace("```json", "").replace("```", "").strip()
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - æ­¥éª¤4: è§£æ JSON")
            self.interview_plan = json.loads(plan_json)
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - JSON è§£ææˆåŠŸ")
            
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - âœ… é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±{len(self.interview_plan['questions'])}ä¸ªé—®é¢˜")
            return self.interview_plan
            
        except Exception as e:
            logging.error(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - âŒ ç”Ÿæˆé¢è¯•è®¡åˆ’å¤±è´¥: {e}", exc_info=True)
            # å¤±è´¥æ—¶è¿”å›ç©ºè®¡åˆ’ï¼Œèµ°åŸæœ‰é€»è¾‘
            return None


    async def get_ai_response(self, dialog_history):
        start_time = time.time()

        # è°ƒè¯•æ—¥å¿—
        logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - get_ai_response è¢«è°ƒç”¨")
        logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - dialog_history ç±»å‹: {type(dialog_history)}")
        logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - dialog_history å€¼: {dialog_history}")
        logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - not dialog_history = {not dialog_history}")
        logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - len(dialog_history) = {len(dialog_history) if dialog_history is not None else 'None'}")

        # åˆ¤æ–­æ˜¯å¦é¦–æ¬¡è°ƒç”¨
        if not dialog_history or len(dialog_history) == 0:
            logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - âœ… è¿›å…¥é¦–æ¬¡è°ƒç”¨åˆ†æ”¯")
            # é¦–æ¬¡è°ƒç”¨ï¼šç”Ÿæˆé¢è¯•è®¡åˆ’
            logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - å‡†å¤‡è°ƒç”¨ generate_interview_plan()")
            await self.generate_interview_plan()
            logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - generate_interview_plan() è°ƒç”¨å®Œæˆ")
            logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - interview_plan æ˜¯å¦ä¸º None: {self.interview_plan is None}")
            
            # è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆå¼€åœºç™½ï¼‰
            if self.interview_plan and len(self.interview_plan['questions']) > 0:
                logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - âœ… é¢è¯•è®¡åˆ’æœ‰æ•ˆï¼Œå‡†å¤‡è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜")
                first_question = self.interview_plan['questions'][0]['question']
                self.current_question_index = 1  # ä¸‹æ¬¡ä»ç¬¬äºŒä¸ªé—®é¢˜å¼€å§‹
                
                logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - ç¬¬ä¸€ä¸ªé—®é¢˜: {first_question[:50]}...")
                yield f"data: {json.dumps({'type': 'chunk', 'content': first_question}, ensure_ascii=False)}\n\n"
                yield f"data: {json.dumps({'type': 'DONE', 'content': '[DONE]'}, ensure_ascii=False)}\n\n"
                logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - âœ… å·²è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œå‡½æ•°å³å°†return")
                return
            else:
                logging.warning(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - âš ï¸ é¢è¯•è®¡åˆ’ä¸ºç©ºæˆ–æ²¡æœ‰é—®é¢˜ï¼Œç»§ç»­æ‰§è¡ŒåŸæœ‰æµç¨‹")

        language, say_hi, self.TTS_model = self.language_dict[self.interview_language]
        system_message = f'''
                        æœ¬æ¬¡é¢è¯•å‚è€ƒèµ„æ–™å¦‚ä¸‹ï¼š
                        #é¢è¯•äº¤æµå…¨ç¨‹ä½¿ç”¨è¯­ç§ï¼š{language}
                        é¢è¯•å¼€å§‹æ—¶é—´ï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}
                        å€™é€‰äººä¿¡æ¯ï¼š{self.user_portrait}
                        å²—ä½ä¿¡æ¯ï¼š{self.jd_title},{self.jd_portrait}
                        å…¬å¸ä¿¡æ¯ï¼š{self.company_portrait}
                        #é¢è¯•äº¤æµå…¨ç¨‹ä½¿ç”¨è¯­ç§ï¼š{language}
                        é¢è¯•æ­£å¼å¼€å§‹ï¼
                                        '''

        self.speech_speed = 1
        
        # åŠ è½½promptæ¨¡æ¿
        celue_template = load_prompt("strategy_white_collar.txt")
        zhixing_template = load_prompt("executor_white_collar.txt")
        lanling_template = load_prompt("executor_blue_collar.txt")
        
        # æ›¿æ¢å˜é‡
        lang = self.language_dict[self.interview_language][0]
        self.celue_sys_prompt = celue_template.format(language=lang)
        self.zhixing_sys_prompt = zhixing_template.format(language=lang)
        self.lanling_sys_prompt = lanling_template.format(language=lang)
        try:
            # å‡†å¤‡é¢è¯•è®¡åˆ’ä¿¡æ¯
            plan_info = ""
            if self.interview_plan:
                remaining_questions = self.interview_plan['questions'][self.current_question_index:]
                plan_info = f"""
                ã€é¢è¯•è®¡åˆ’å‚è€ƒã€‘
                å½“å‰è¿›åº¦ï¼šå·²æé—® {self.current_question_index}/{len(self.interview_plan['questions'])} ä¸ªé—®é¢˜
                å¾…æé—®é¢˜ï¼š{json.dumps(remaining_questions[:3], ensure_ascii=False)}
                ï¼ˆæ³¨æ„ï¼šè¿™åªæ˜¯å‚è€ƒè®¡åˆ’ï¼Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µçµæ´»è°ƒæ•´ï¼‰
                """
            
            #interview_type 1ç™½é¢†ï¼Œ0è“é¢†
            if self.interview_type == 1:
                self.speech_speed = 1.1
                celue_message = [{"role": "system", "content": self.celue_sys_prompt},
                                 {"role": "user",
                                  "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}ï¼Œå½“å‰æ—¶é—´ä¸º{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}{plan_info}"}]
                logging.info(f"-- é¢è¯•ID ï¼š{self.interview_id},å½“å‰ç­–ç•¥promptå¦‚ä¸‹--ï¼š{celue_message}")
                for i in range(2):
                    try:
                        celue = self.celue_client.chat.completions.create(
                            model=self.ai_config['api_model'],
                            temperature=0.4,
                            messages=celue_message,
                            extra_body={"chat_template_kwargs": {"enable_thinking": True}},
                        )
                        celue_text = celue.choices[0].message.content
                        break
                    except Exception as e:
                        logging.error(f"-- é¢è¯•ID : {self.interview_id}---ç­–ç•¥è°ƒç”¨AIå‡ºé”™ï¼Œç¬¬{i + 1}æ¬¡å°è¯•: {e}")
                logging.info(
                    f"-- é¢è¯•ID ï¼š{self.interview_id},æ¨ç†æ—¶é—´ï¼š{time.time() - start_time}s,å½“å‰ç­–ç•¥å¦‚ä¸‹--ï¼š\n{celue_text}")
                # å¯ç”¨æµå¼ä¼ è¾“
                zhixing_message = [{"role": "system", "content": self.zhixing_sys_prompt},
                                   {"role": "user",
                                    "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history},å½“å‰æ—¶é—´ä¸ºï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}ï¼Œå½“å‰å›å¤ç­–ç•¥å¦‚ä¸‹{celue_text}{plan_info}"}]
            else:
                zhixing_message = [{"role": "system", "content": self.lanling_sys_prompt},
                                   {"role": "user",
                                    "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}ï¼Œå½“å‰æ—¶é—´ä¸º{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}"}]
            logging.info(f"-- é¢è¯•ID ï¼š{self.interview_id},å½“å‰æ‰§è¡Œpromptå¦‚ä¸‹--ï¼š{zhixing_message}")
            #é‡è¯•æ¬¡æ•°
            for i in range(2):
                try:
                    response = await self.AI_client.chat.completions.create(
                        stream=True,  # å¯ç”¨æµå¼
                        model=self.ai_config['api_model'],
                        temperature=0.7,
                        seed=1,
                        messages=zhixing_message
                    )
                    break
                except Exception as e:
                    logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµå¼è°ƒç”¨AIå‡ºé”™ï¼Œç¬¬{i + 1}æ¬¡å°è¯•: {e}")
            if not response:
                logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµå¼è°ƒç”¨AIå¤±è´¥")
            ai_response = ""
            async for chunk in response:
                if chunk.choices and chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    ai_response += content

                    yield f"data: {json.dumps({'type': 'chunk', 'content': content}, ensure_ascii=False)}\n\n"

            yield f"data: {json.dumps({'type': 'DONE', 'content': "[DONE]"}, ensure_ascii=False)}\n\n"

            # æ›´æ–°é—®é¢˜ç´¢å¼•
            if self.interview_plan:
                self.current_question_index += 1

            end_time = time.time()
            execution_time = end_time - start_time
            logging.info(f"-- é¢è¯•ID : {self.interview_id}---æ‰§è¡Œæ—¶é—´ä¸º: {execution_time} ç§’,AIæµå¼è¿”å›å†…å®¹: {ai_response}")
            try:
                flash_time = time.time()
                #åˆ¤æ–­é¢è¯•æ˜¯å¦ç»“æŸï¼Œæ¯ä¸ªå¾ªç¯éƒ½æœ‰
                liucheng_prompt = load_prompt("judge_flow.txt")
                liucheng = self.celue_client.chat.completions.create(
                    model="qwen-flash",
                    temperature=0.4,
                    messages=[{"role": "system", "content": liucheng_prompt},
                              {"role": "user", "content": f"é¢è¯•å®˜å›å¤å†…å®¹å¦‚ä¸‹{ai_response}"}],
                )
                liucheng_flag = liucheng.choices[0].message.content
                logging.info(
                    f"-- é¢è¯•ID : {self.interview_id}---æµç¨‹åˆ¤æ–­å†…å®¹:  {liucheng_flag}  ,è€—æ—¶{time.time() - flash_time}ç§’")
                if "ç»“æŸ" in liucheng_flag:
                    yield f"data: {json.dumps({'type': 'over', 'content': "over"}, ensure_ascii=False)}\n\n"
            except Exception as e:
                logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµç¨‹åˆ¤æ–­å‡ºé”™: {e}")
                yield f"data: {json.dumps({'type': 'error', 'content': str(e)}, ensure_ascii=False)}\n\n"

        except Exception as e:
            logging.error(f"-- é¢è¯•ID : {self.interview_id}---AIæµå¼å“åº”è·å–é”™è¯¯: {e}")
            yield f"data: {json.dumps({'type': 'error', 'content': "æ¨¡å‹æ£€æµ‹åˆ°éæ³•è¾“å…¥ï¼Œè¯·é‡æ–°å¼€å§‹æœ¬æ¬¡é¢è¯•"+str(e)}, ensure_ascii=False)}\n\n"






```

ç°åœ¨æ£€æŸ¥ä¸€ä¸‹è¯­æ³•é”™è¯¯ï¼š

å®Œç¾ï¼ç°åœ¨è®©æˆ‘ä¸ºä½ æ€»ç»“ä¸€ä¸‹æ·»åŠ çš„è°ƒè¯•æ—¥å¿—ï¼š

---

## âœ… å·²æ·»åŠ çš„è°ƒè¯•æ—¥å¿—

### 1ï¸âƒ£ è·¯ç”±å±‚ï¼ˆinterview_process_router.pyï¼‰

**ä½ç½®**ï¼šç¬¬32-36è¡Œ

æ·»åŠ äº†4æ¡æ—¥å¿—ï¼š
```python
logging.info(f"[DEBUG-è·¯ç”±å±‚] interview_id: {interview_id}")
logging.info(f"[DEBUG-è·¯ç”±å±‚] dialogHistory ç±»å‹: {type(dialog_history)}, å€¼: {dialog_history}")
logging.info(f"[DEBUG-è·¯ç”±å±‚] dialogHistory é•¿åº¦: {len(dialog_history) if dialog_history is not None else 'None'}")
logging.info(f"[DEBUG-è·¯ç”±å±‚] dialogHistory æ˜¯å¦ä¸ºç©ºåˆ—è¡¨: {dialog_history == []}")
```

**ä½œç”¨**ï¼šéªŒè¯è·¯ç”±å±‚æ¥æ”¶åˆ°çš„ `dialogHistory` çš„ç±»å‹å’Œå€¼

---

### 2ï¸âƒ£ æœåŠ¡å±‚å…¥å£ï¼ˆvirtual_interview.py - get_ai_responseï¼‰

**ä½ç½®**ï¼šç¬¬157-165è¡Œ

æ·»åŠ äº†6æ¡æ—¥å¿—ï¼š
```python
logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - get_ai_response è¢«è°ƒç”¨")
logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - dialog_history ç±»å‹: {type(dialog_history)}")
logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - dialog_history å€¼: {dialog_history}")
logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - not dialog_history = {not dialog_history}")
logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - len(dialog_history) = {len(dialog_history)}")
logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - âœ… è¿›å…¥é¦–æ¬¡è°ƒç”¨åˆ†æ”¯")  # å¦‚æœè¿›å…¥ifåˆ†æ”¯
```

**ä½œç”¨**ï¼šéªŒè¯åˆ¤æ–­æ¡ä»¶æ˜¯å¦æ­£ç¡®

---

### 3ï¸âƒ£ é¦–æ¬¡è°ƒç”¨åˆ†æ”¯ï¼ˆvirtual_interview.pyï¼‰

**ä½ç½®**ï¼šç¬¬167-184è¡Œ

æ·»åŠ äº†7æ¡æ—¥å¿—ï¼š
```python
logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - å‡†å¤‡è°ƒç”¨ generate_interview_plan()")
logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - generate_interview_plan() è°ƒç”¨å®Œæˆ")
logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - interview_plan æ˜¯å¦ä¸º None: {self.interview_plan is None}")
logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - âœ… é¢è¯•è®¡åˆ’æœ‰æ•ˆï¼Œå‡†å¤‡è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜")
logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - ç¬¬ä¸€ä¸ªé—®é¢˜: {first_question[:50]}...")
logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - âœ… å·²è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œå‡½æ•°å³å°†return")
logging.warning(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - âš ï¸ é¢è¯•è®¡åˆ’ä¸ºç©ºæˆ–æ²¡æœ‰é—®é¢˜ï¼Œç»§ç»­æ‰§è¡ŒåŸæœ‰æµç¨‹")
```

**ä½œç”¨**ï¼šè¿½è¸ªé¦–æ¬¡è°ƒç”¨çš„æ‰§è¡Œè·¯å¾„

---

### 4ï¸âƒ£ ç”Ÿæˆè®¡åˆ’å‡½æ•°ï¼ˆvirtual_interview.py - generate_interview_planï¼‰

**ä½ç½®**ï¼šç¬¬122-164è¡Œ

æ·»åŠ äº†13æ¡æ—¥å¿—ï¼š
```python
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - ğŸ¯ è¿›å…¥ generate_interview_plan å‡½æ•°")
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - æ­¥éª¤1: åŠ è½½ prompt æ–‡ä»¶")
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - prompt æ–‡ä»¶åŠ è½½æˆåŠŸï¼Œé•¿åº¦: {len(plan_prompt)}")
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - æ­¥éª¤2: æ ¼å¼åŒ– prompt")
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - user_portrait: ...")
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - jd_title: ...")
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - jd_portrait: ...")
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - prompt æ ¼å¼åŒ–æˆåŠŸ")
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - æ­¥éª¤3: è°ƒç”¨ LLM ç”Ÿæˆè®¡åˆ’")
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - LLM è°ƒç”¨æˆåŠŸ")
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - LLM è¿”å›å†…å®¹ï¼ˆå‰200å­—ç¬¦ï¼‰: ...")
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - æ­¥éª¤4: è§£æ JSON")
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - JSON è§£ææˆåŠŸ")
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - âœ… é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±Xä¸ªé—®é¢˜")
logging.error(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - âŒ ç”Ÿæˆé¢è¯•è®¡åˆ’å¤±è´¥: {e}", exc_info=True)
```

**ä½œç”¨**ï¼šè¯¦ç»†è¿½è¸ªç”Ÿæˆè®¡åˆ’çš„æ¯ä¸ªæ­¥éª¤ï¼Œå¦‚æœå‡ºé”™ä¼šæ‰“å°å®Œæ•´çš„å †æ ˆä¿¡æ¯

---

## ğŸ¯ ä¸‹ä¸€æ­¥æ“ä½œ

ç°åœ¨ä½ å¯ä»¥ï¼š

1. **é‡å¯ä½ çš„æœåŠ¡**ï¼ˆç¡®ä¿æ–°ä»£ç ç”Ÿæ•ˆï¼‰
2. **ç”¨Postmanå‘é€ç›¸åŒçš„è¯·æ±‚**
3. **æŸ¥çœ‹å®Œæ•´çš„æ—¥å¿—è¾“å‡º**
4. **æŠŠæ—¥å¿—å†…å®¹å‘ç»™æˆ‘**

---

## ğŸ“‹ æ—¥å¿—æŸ¥çœ‹å»ºè®®

ä½ çš„æ—¥å¿—åº”è¯¥ä¼šåœ¨ä»¥ä¸‹ä½ç½®ï¼š
- `project_code/career/code/logs/app_info.log` æˆ–
- æ§åˆ¶å°è¾“å‡º

å…³é”®çš„æ—¥å¿—æ ‡ç­¾ï¼š
- `[DEBUG-è·¯ç”±å±‚]` - è·¯ç”±å±‚çš„è°ƒè¯•ä¿¡æ¯
- `[DEBUG-æœåŠ¡å±‚]` - æœåŠ¡å±‚çš„è°ƒè¯•ä¿¡æ¯
- `[DEBUG-ç”Ÿæˆè®¡åˆ’]` - ç”Ÿæˆè®¡åˆ’å‡½æ•°çš„è°ƒè¯•ä¿¡æ¯

---

## ğŸ” é¢„æœŸçœ‹åˆ°çš„æ—¥å¿—æµç¨‹

**å¦‚æœä¸€åˆ‡æ­£å¸¸ï¼Œåº”è¯¥çœ‹åˆ°è¿™æ ·çš„æ—¥å¿—æµç¨‹**ï¼š

```
[DEBUG-è·¯ç”±å±‚] interview_id: test_interview_001
[DEBUG-è·¯ç”±å±‚] dialogHistory ç±»å‹: <class 'list'>, å€¼: []
[DEBUG-è·¯ç”±å±‚] dialogHistory é•¿åº¦: 0
[DEBUG-è·¯ç”±å±‚] dialogHistory æ˜¯å¦ä¸ºç©ºåˆ—è¡¨: True
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - get_ai_response è¢«è°ƒç”¨
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - dialog_history ç±»å‹: <class 'list'>
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - dialog_history å€¼: []
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - not dialog_history = True
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - len(dialog_history) = 0
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - âœ… è¿›å…¥é¦–æ¬¡è°ƒç”¨åˆ†æ”¯
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - å‡†å¤‡è°ƒç”¨ generate_interview_plan()
[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: test_interview_001 - ğŸ¯ è¿›å…¥ generate_interview_plan å‡½æ•°
[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: test_interview_001 - æ­¥éª¤1: åŠ è½½ prompt æ–‡ä»¶
[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: test_interview_001 - prompt æ–‡ä»¶åŠ è½½æˆåŠŸ
... (æ›´å¤šç”Ÿæˆè®¡åˆ’çš„æ—¥å¿—)
[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: test_interview_001 - âœ… é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±12ä¸ªé—®é¢˜
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - generate_interview_plan() è°ƒç”¨å®Œæˆ
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - interview_plan æ˜¯å¦ä¸º None: False
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - âœ… é¢è¯•è®¡åˆ’æœ‰æ•ˆï¼Œå‡†å¤‡è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - ç¬¬ä¸€ä¸ªé—®é¢˜: ...
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - âœ… å·²è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œå‡½æ•°å³å°†return
```

å‡†å¤‡å¥½äº†å—ï¼Ÿé‡å¯æœåŠ¡ï¼Œå‘é€è¯·æ±‚ï¼Œç„¶åæŠŠæ—¥å¿—å‘ç»™æˆ‘ï¼ğŸ˜Š

```python
logging.info(f"[DEBUG-è·¯ç”±å±‚] interview_id: {interview_id}")
logging.info(f"[DEBUG-è·¯ç”±å±‚] dialogHistory ç±»å‹: {type(dialog_history)}, å€¼: {dialog_history}")
logging.info(f"[DEBUG-è·¯ç”±å±‚] dialogHistory é•¿åº¦: {len(dialog_history) if dialog_history is not None else 'None'}")
logging.info(f"[DEBUG-è·¯ç”±å±‚] dialogHistory æ˜¯å¦ä¸ºç©ºåˆ—è¡¨: {dialog_history == []}")
```

```python
logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - get_ai_response è¢«è°ƒç”¨")
logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - dialog_history ç±»å‹: {type(dialog_history)}")
logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - dialog_history å€¼: {dialog_history}")
logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - not dialog_history = {not dialog_history}")
logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - len(dialog_history) = {len(dialog_history)}")
logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - âœ… è¿›å…¥é¦–æ¬¡è°ƒç”¨åˆ†æ”¯")  # å¦‚æœè¿›å…¥ifåˆ†æ”¯
```

```python
logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - å‡†å¤‡è°ƒç”¨ generate_interview_plan()")
logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - generate_interview_plan() è°ƒç”¨å®Œæˆ")
logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - interview_plan æ˜¯å¦ä¸º None: {self.interview_plan is None}")
logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - âœ… é¢è¯•è®¡åˆ’æœ‰æ•ˆï¼Œå‡†å¤‡è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜")
logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - ç¬¬ä¸€ä¸ªé—®é¢˜: {first_question[:50]}...")
logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - âœ… å·²è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œå‡½æ•°å³å°†return")
logging.warning(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - âš ï¸ é¢è¯•è®¡åˆ’ä¸ºç©ºæˆ–æ²¡æœ‰é—®é¢˜ï¼Œç»§ç»­æ‰§è¡ŒåŸæœ‰æµç¨‹")
```

```python
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - ğŸ¯ è¿›å…¥ generate_interview_plan å‡½æ•°")
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - æ­¥éª¤1: åŠ è½½ prompt æ–‡ä»¶")
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - prompt æ–‡ä»¶åŠ è½½æˆåŠŸï¼Œé•¿åº¦: {len(plan_prompt)}")
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - æ­¥éª¤2: æ ¼å¼åŒ– prompt")
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - user_portrait: ...")
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - jd_title: ...")
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - jd_portrait: ...")
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - prompt æ ¼å¼åŒ–æˆåŠŸ")
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - æ­¥éª¤3: è°ƒç”¨ LLM ç”Ÿæˆè®¡åˆ’")
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - LLM è°ƒç”¨æˆåŠŸ")
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - LLM è¿”å›å†…å®¹ï¼ˆå‰200å­—ç¬¦ï¼‰: ...")
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - æ­¥éª¤4: è§£æ JSON")
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - JSON è§£ææˆåŠŸ")
logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - âœ… é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±Xä¸ªé—®é¢˜")
logging.error(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - âŒ ç”Ÿæˆé¢è¯•è®¡åˆ’å¤±è´¥: {e}", exc_info=True)
```

```plaintext
[DEBUG-è·¯ç”±å±‚] interview_id: test_interview_001
[DEBUG-è·¯ç”±å±‚] dialogHistory ç±»å‹: <class 'list'>, å€¼: []
[DEBUG-è·¯ç”±å±‚] dialogHistory é•¿åº¦: 0
[DEBUG-è·¯ç”±å±‚] dialogHistory æ˜¯å¦ä¸ºç©ºåˆ—è¡¨: True
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - get_ai_response è¢«è°ƒç”¨
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - dialog_history ç±»å‹: <class 'list'>
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - dialog_history å€¼: []
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - not dialog_history = True
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - len(dialog_history) = 0
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - âœ… è¿›å…¥é¦–æ¬¡è°ƒç”¨åˆ†æ”¯
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - å‡†å¤‡è°ƒç”¨ generate_interview_plan()
[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: test_interview_001 - ğŸ¯ è¿›å…¥ generate_interview_plan å‡½æ•°
[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: test_interview_001 - æ­¥éª¤1: åŠ è½½ prompt æ–‡ä»¶
[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: test_interview_001 - prompt æ–‡ä»¶åŠ è½½æˆåŠŸ
... (æ›´å¤šç”Ÿæˆè®¡åˆ’çš„æ—¥å¿—)
[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: test_interview_001 - âœ… é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±12ä¸ªé—®é¢˜
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - generate_interview_plan() è°ƒç”¨å®Œæˆ
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - interview_plan æ˜¯å¦ä¸º None: False
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - âœ… é¢è¯•è®¡åˆ’æœ‰æ•ˆï¼Œå‡†å¤‡è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - ç¬¬ä¸€ä¸ªé—®é¢˜: ...
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - âœ… å·²è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œå‡½æ•°å³å°†return
```

---

**User**

é‚£ä¹ˆç°åœ¨æˆ‘çš„é—®é¢˜æ˜¯çœ‹è¿™äº›æ—¥å¿—çš„è¯åªèƒ½åœ¨terminalé‡Œé¢çœ‹å—ï¼Ÿè¿˜æ˜¯è¯´å¯ä»¥åœ¨ç‰¹å®šçš„æ–‡ä»¶é‡Œé¢å¯ä»¥çœ‹åˆ°å‘€ï¼Ÿ

---

**Cursor**

æ£€æŸ¥æ—¥å¿—é…ç½®ï¼Œç¡®è®¤æ—¥å¿—è¾“å‡ºä½ç½®ã€‚


æŸ¥çœ‹æ—¥å¿—é…ç½®è¯¦æƒ…ï¼š


æ ¹æ®æ—¥å¿—é…ç½®ï¼Œæ—¥å¿—ä¼šåŒæ—¶è¾“å‡ºåˆ°ä¸¤ä¸ªä½ç½®ï¼š

## æ—¥å¿—è¾“å‡ºä½ç½®

### 1. æ§åˆ¶å°ï¼ˆTerminalï¼‰
- å®æ—¶è¾“å‡º
- å¯åŠ¨æœåŠ¡æ—¶åœ¨ç»ˆç«¯å¯è§

### 2. æ—¥å¿—æ–‡ä»¶ï¼ˆæ¨èï¼‰
æ—¥å¿—æ–‡ä»¶ä½ç½®ï¼š
```
project_code/career/code/logs/app_info.log
```

è¿™ä¸ªæ–‡ä»¶ä¼šä¿å­˜æ‰€æœ‰INFOçº§åˆ«çš„æ—¥å¿—ï¼ˆåŒ…æ‹¬æˆ‘ä»¬æ·»åŠ çš„DEBUGæ—¥å¿—ï¼‰ã€‚

---

## å¦‚ä½•æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶

### æ–¹æ³•1ï¼šç›´æ¥åœ¨IDEä¸­æ‰“å¼€
åœ¨VS Codeæˆ–ä½ çš„IDEä¸­ç›´æ¥æ‰“å¼€ï¼š
```
project_code/career/code/logs/app_info.log
```

### æ–¹æ³•2ï¼šä½¿ç”¨å‘½ä»¤è¡ŒæŸ¥çœ‹ï¼ˆWindowsï¼‰
```bash
# æŸ¥çœ‹å®Œæ•´æ—¥å¿—æ–‡ä»¶
type project_code\career\code\logs\app_info.log

# æˆ–è€…ä½¿ç”¨PowerShell
Get-Content project_code\career\code\logs\app_info.log

# å®æ—¶æŸ¥çœ‹æœ€æ–°æ—¥å¿—ï¼ˆç±»ä¼¼tail -fï¼‰
Get-Content project_code\career\code\logs\app_info.log -Wait -Tail 50
```

### æ–¹æ³•3ï¼šä½¿ç”¨æ–‡æœ¬ç¼–è¾‘å™¨
ç”¨è®°äº‹æœ¬ã€Notepad++ç­‰æ‰“å¼€ `app_info.log` æ–‡ä»¶

---

## æ—¥å¿—æ–‡ä»¶è¯´æ˜

æ ¹æ®é…ç½®ï¼Œæ—¥å¿—æ–‡ä»¶ä¼šï¼š
- æŒ‰å¤©è‡ªåŠ¨åˆ†å‰²ï¼šæ¯å¤©åˆå¤œåˆ›å»ºæ–°æ–‡ä»¶
- ä¿ç•™å†å²ï¼šINFOæ—¥å¿—ä¿ç•™7å¤©ï¼ŒERRORæ—¥å¿—ä¿ç•™30å¤©
- æ–‡ä»¶å‘½åæ ¼å¼ï¼š
  - ä»Šå¤©çš„æ—¥å¿—ï¼š`app_info.log`
  - å†å²æ—¥å¿—ï¼š`app_info.log.2025-11-09`ï¼ˆæ—¥æœŸæ ¼å¼ï¼‰

---

## æŸ¥çœ‹æœ€æ–°æ—¥å¿—çš„å»ºè®®

### å®æ—¶æŸ¥çœ‹ï¼ˆæ¨èï¼‰
åœ¨ç»ˆç«¯è¿è¡ŒæœåŠ¡æ—¶ï¼Œæ—¥å¿—ä¼šå®æ—¶æ˜¾ç¤ºï¼Œæ–¹ä¾¿è°ƒè¯•ã€‚

### æŸ¥çœ‹æ–‡ä»¶ï¼ˆæ–¹ä¾¿æœç´¢å’Œä¿å­˜ï¼‰
1. æ‰“å¼€ `project_code/career/code/logs/app_info.log`
2. æœç´¢ `[DEBUG-` å¿«é€Ÿå®šä½è°ƒè¯•æ—¥å¿—
3. å¤åˆ¶éœ€è¦çš„æ—¥å¿—å†…å®¹

---

## å¿«é€Ÿå®šä½è°ƒè¯•æ—¥å¿—

åœ¨æ—¥å¿—æ–‡ä»¶ä¸­æœç´¢ä»¥ä¸‹å…³é”®è¯ï¼š
- `[DEBUG-è·¯ç”±å±‚]` - è·¯ç”±å±‚çš„è°ƒè¯•ä¿¡æ¯
- `[DEBUG-æœåŠ¡å±‚]` - æœåŠ¡å±‚çš„è°ƒè¯•ä¿¡æ¯
- `[DEBUG-ç”Ÿæˆè®¡åˆ’]` - ç”Ÿæˆè®¡åˆ’å‡½æ•°çš„è°ƒè¯•ä¿¡æ¯

---

## æ€»ç»“

ä½ å¯ä»¥ï¼š
1. åœ¨Terminalå®æ—¶æŸ¥çœ‹ï¼ˆè¿è¡ŒæœåŠ¡æ—¶ï¼‰
2. åœ¨æ—¥å¿—æ–‡ä»¶ä¸­æŸ¥çœ‹ï¼ˆ`project_code/career/code/logs/app_info.log`ï¼‰

å»ºè®®ï¼šå…ˆç”¨Terminalå®æ—¶æŸ¥çœ‹ï¼Œéœ€è¦ä¿å­˜æˆ–æœç´¢æ—¶å†æ‰“å¼€æ—¥å¿—æ–‡ä»¶ã€‚

ç°åœ¨å¯ä»¥ï¼š
1. é‡å¯æœåŠ¡
2. ç”¨Postmanå‘é€è¯·æ±‚
3. æŸ¥çœ‹Terminalæˆ–æ‰“å¼€ `app_info.log` æ–‡ä»¶
4. æŠŠæ—¥å¿—å†…å®¹å‘ç»™æˆ‘åˆ†æ

éœ€è¦æˆ‘å¸®ä½ æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶å—ï¼Ÿ

```plaintext
project_code/career/code/logs/app_info.log
```

```plaintext
project_code/career/code/logs/app_info.log
```

```shellscript
# æŸ¥çœ‹å®Œæ•´æ—¥å¿—æ–‡ä»¶
type project_code\career\code\logs\app_info.log

# æˆ–è€…ä½¿ç”¨PowerShell
Get-Content project_code\career\code\logs\app_info.log

# å®æ—¶æŸ¥çœ‹æœ€æ–°æ—¥å¿—ï¼ˆç±»ä¼¼tail -fï¼‰
Get-Content project_code\career\code\logs\app_info.log -Wait -Tail 50
```

---

**User**

@logs è¿™ä¸ªæ–‡ä»¶å¤¹é‡Œé¢è¿™ä¹ˆå¤šæ—¥å¿—æ–‡ä»¶ï¼Œæˆ‘åˆ°åº•åº”è¯¥çœ‹å“ªä¸ªå‘¢ï¼Ÿä»¥åŠä¸ºä»€ä¹ˆå‘€ï¼Œè€Œä¸”è¿˜æœ‰ä¸€äº›æ˜¯æ—¥æœŸæ ‡æ³¨çš„ï¼Œæˆ‘æ˜¯ä¸æ˜¯åº”è¯¥åªçœ‹å…³äºå½“å¤©æ—¥æœŸçš„æ—¥å¿—å°±è¡Œäº†å‘€ï¼Ÿä»¥åŠä¸ºä»€ä¹ˆå‘€ï¼Ÿ

---

**Cursor**

æŸ¥çœ‹æœ€æ–°çš„æ—¥å¿—æ–‡ä»¶ï¼Œè§£é‡Šæ—¥å¿—æ–‡ä»¶ç»“æ„ï¼š


è§£é‡Šæ—¥å¿—æ–‡ä»¶ç»“æ„ï¼Œå¹¶åˆ†æä¸ºä»€ä¹ˆæ²¡æœ‰çœ‹åˆ°æˆ‘ä»¬æ·»åŠ çš„DEBUGæ—¥å¿—ã€‚

## æ—¥å¿—æ–‡ä»¶ç»“æ„è¯´æ˜

### æ—¥å¿—æ–‡ä»¶å‘½åè§„åˆ™

æ ¹æ®æ—¥å¿—é…ç½®ï¼ˆ`app/loger.py`ï¼‰ï¼Œæ—¥å¿—æ–‡ä»¶ä¼šæŒ‰å¤©è‡ªåŠ¨åˆ†å‰²ï¼š

| æ–‡ä»¶å | è¯´æ˜ | ä½•æ—¶ä½¿ç”¨ |
|--------|------|---------|
| **`app_info.log`** | ä»Šå¤©çš„æ—¥å¿—æ–‡ä»¶ | å½“å‰æ­£åœ¨ä½¿ç”¨çš„æ–‡ä»¶ï¼ŒåŒ…å«ä»Šå¤©çš„æ‰€æœ‰INFOæ—¥å¿— |
| **`app_info.log.2025-11-09`** | å†å²æ—¥å¿—æ–‡ä»¶ | 2025å¹´11æœˆ9æ—¥çš„æ—¥å¿—ï¼ˆå·²å½’æ¡£ï¼‰ |
| **`app_info.log.2025-11-06`** | å†å²æ—¥å¿—æ–‡ä»¶ | 2025å¹´11æœˆ6æ—¥çš„æ—¥å¿—ï¼ˆå·²å½’æ¡£ï¼‰ |
| **`app_error.log`** | é”™è¯¯æ—¥å¿—æ–‡ä»¶ | æ‰€æœ‰ERRORçº§åˆ«çš„æ—¥å¿—ï¼ˆä¸åˆ†æ—¥æœŸï¼‰ |

### åº”è¯¥çœ‹å“ªä¸ªæ–‡ä»¶ï¼Ÿ

ä½ åº”è¯¥çœ‹ï¼š`app_info.log`ï¼ˆä»Šå¤©çš„æ—¥å¿—æ–‡ä»¶ï¼‰

åŸå› ï¼š
1. è¿™æ˜¯å½“å‰æ­£åœ¨å†™å…¥çš„æ–‡ä»¶
2. åŒ…å«ä»Šå¤©çš„æ‰€æœ‰æ—¥å¿—
3. æœ€æ–°çš„è¯·æ±‚æ—¥å¿—éƒ½åœ¨è¿™é‡Œ

å†å²æ–‡ä»¶ï¼ˆå¸¦æ—¥æœŸçš„ï¼‰ï¼š
- ç”¨äºæŸ¥çœ‹è¿‡å»çš„æ—¥å¿—
- ç³»ç»Ÿä¼šè‡ªåŠ¨ä¿ç•™7å¤©
- è¶…è¿‡7å¤©ä¼šè‡ªåŠ¨åˆ é™¤

---

## é‡è¦å‘ç°ï¼šæ²¡æœ‰çœ‹åˆ°DEBUGæ—¥å¿—

æˆ‘æŸ¥çœ‹äº† `app_info.log`ï¼Œå‘ç°ä¸€ä¸ªé—®é¢˜ï¼š

### æ—¥å¿—ä¸­æ²¡æœ‰æˆ‘ä»¬æ·»åŠ çš„DEBUGæ—¥å¿—

æ—¥å¿—ä¸­åº”è¯¥çœ‹åˆ°ï¼š
- `[DEBUG-è·¯ç”±å±‚]`
- `[DEBUG-æœåŠ¡å±‚]`
- `[DEBUG-ç”Ÿæˆè®¡åˆ’]`

ä½†å®é™…ä¸Šæ—¥å¿—ä¸­åªæœ‰ï¼š
- `æ—¶é—´ï¼š[2025-11-10 00:21:27]--æ”¶åˆ°é¢è¯•çš„ç½‘ç»œè¯·æ±‚`
- `-- é¢è¯•ID ï¼štest_interview_001,å½“å‰ç­–ç•¥promptå¦‚ä¸‹--`

è¿™è¯´æ˜ï¼š
1. ä»£ç å¯èƒ½æ²¡æœ‰æ‰§è¡Œåˆ°æˆ‘ä»¬æ·»åŠ æ—¥å¿—çš„ä½ç½®
2. æˆ–è€…æœåŠ¡æ²¡æœ‰é‡å¯ï¼Œè¿˜åœ¨ä½¿ç”¨æ—§ä»£ç 

---

## æ—¥å¿—æ—¶é—´åˆ†æ

ä»æ—¥å¿—æ—¶é—´æˆ³çœ‹ï¼š
- æœ€æ–°æ—¥å¿—ï¼š`2025-11-10 00:21:27`
- è¿™æ˜¯ä½ ä¹‹å‰æµ‹è¯•çš„æ—¶é—´

å¦‚æœä¹‹åé‡å¯äº†æœåŠ¡å¹¶æ·»åŠ äº†DEBUGæ—¥å¿—ï¼Œåº”è¯¥èƒ½çœ‹åˆ°æ–°çš„æ—¥å¿—ã€‚

---

## ä¸‹ä¸€æ­¥æ“ä½œ

### 1. ç¡®è®¤æœåŠ¡å·²é‡å¯
ç¡®ä¿æ·»åŠ DEBUGæ—¥å¿—åï¼ŒæœåŠ¡å·²ç»é‡å¯ã€‚

### 2. é‡æ–°å‘é€è¯·æ±‚
ç”¨Postmané‡æ–°å‘é€ä¸€æ¬¡è¯·æ±‚ã€‚

### 3. æŸ¥çœ‹æœ€æ–°æ—¥å¿—
æ‰“å¼€ `app_info.log`ï¼Œåº”è¯¥èƒ½çœ‹åˆ°ç±»ä¼¼è¿™æ ·çš„æ—¥å¿—ï¼š

```
[DEBUG-è·¯ç”±å±‚] interview_id: test_interview_001
[DEBUG-è·¯ç”±å±‚] dialogHistory ç±»å‹: <class 'list'>, å€¼: []
[DEBUG-è·¯ç”±å±‚] dialogHistory é•¿åº¦: 0
[DEBUG-è·¯ç”±å±‚] dialogHistory æ˜¯å¦ä¸ºç©ºåˆ—è¡¨: True
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - get_ai_response è¢«è°ƒç”¨
...
```

---

## å¿«é€ŸæŸ¥çœ‹æœ€æ–°æ—¥å¿—çš„æ–¹æ³•

### æ–¹æ³•1ï¼šåœ¨IDEä¸­æ‰“å¼€
ç›´æ¥æ‰“å¼€ `project_code/career/code/logs/app_info.log`ï¼Œæ»šåŠ¨åˆ°æ–‡ä»¶åº•éƒ¨æŸ¥çœ‹æœ€æ–°æ—¥å¿—ã€‚

### æ–¹æ³•2ï¼šæœç´¢å…³é”®è¯
åœ¨æ—¥å¿—æ–‡ä»¶ä¸­æœç´¢ï¼š
- `[DEBUG-` - æ‰¾åˆ°æ‰€æœ‰DEBUGæ—¥å¿—
- `test_interview_001` - æ‰¾åˆ°ä½ çš„æµ‹è¯•è¯·æ±‚

### æ–¹æ³•3ï¼šæŸ¥çœ‹æœ€åå‡ è¡Œ
æ—¥å¿—æ–‡ä»¶æœ€åå‡ è¡Œå°±æ˜¯æœ€æ–°çš„æ—¥å¿—ã€‚

---

## æ€»ç»“

1. çœ‹å“ªä¸ªæ–‡ä»¶ï¼š`app_info.log`ï¼ˆä»Šå¤©çš„æ—¥å¿—ï¼‰
2. ä¸ºä»€ä¹ˆï¼šè¿™æ˜¯å½“å‰æ­£åœ¨å†™å…¥çš„æ–‡ä»¶ï¼ŒåŒ…å«æœ€æ–°çš„æ—¥å¿—
3. å¸¦æ—¥æœŸçš„æ–‡ä»¶ï¼šå†å²å½’æ¡£ï¼Œç”¨äºæŸ¥çœ‹è¿‡å»çš„æ—¥å¿—

ç°åœ¨è¯·ï¼š
1. ç¡®è®¤æœåŠ¡å·²é‡å¯ï¼ˆä½¿ç”¨æ–°ä»£ç ï¼‰
2. ç”¨Postmané‡æ–°å‘é€ä¸€æ¬¡è¯·æ±‚
3. æ‰“å¼€ `app_info.log`ï¼ŒæŸ¥çœ‹æ–‡ä»¶æœ«å°¾çš„æœ€æ–°æ—¥å¿—
4. æŠŠæœ€æ–°çš„æ—¥å¿—å†…å®¹å‘ç»™æˆ‘ï¼Œç‰¹åˆ«æ˜¯åŒ…å« `[DEBUG-` çš„éƒ¨åˆ†

å¦‚æœè¿˜æ˜¯æ²¡æœ‰çœ‹åˆ°DEBUGæ—¥å¿—ï¼Œå¯èƒ½æ˜¯æœåŠ¡æ²¡æœ‰é‡å¯ï¼Œæˆ–è€…ä»£ç æ²¡æœ‰ç”Ÿæ•ˆã€‚æˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥æ’æŸ¥ã€‚

```plaintext
[DEBUG-è·¯ç”±å±‚] interview_id: test_interview_001
[DEBUG-è·¯ç”±å±‚] dialogHistory ç±»å‹: <class 'list'>, å€¼: []
[DEBUG-è·¯ç”±å±‚] dialogHistory é•¿åº¦: 0
[DEBUG-è·¯ç”±å±‚] dialogHistory æ˜¯å¦ä¸ºç©ºåˆ—è¡¨: True
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - get_ai_response è¢«è°ƒç”¨
...
```

---

**User**

å“¦ï¼Œæˆ‘å¥½åƒæ˜ç™½äº†æ‰€ä»¥æ¯æ¬¡ä¿®æ”¹å®Œä»£ç ä»¥åè¦é‡å¯ä¸€ä¸‹æœåŠ¡æ˜¯å§ï¼Ÿå› ä¸ºæˆ‘å¹¶ä¸æ˜¯çƒ­é‡è½½å‘€ï¼Œæ˜¯è¿™æ ·å—ï¼Ÿ

---

**Cursor**

æ£€æŸ¥å¯åŠ¨é…ç½®ï¼Œç¡®è®¤æ˜¯å¦å¯ç”¨äº†çƒ­é‡è½½ï¼š


æ˜¯çš„ï¼Œä½ çš„ç†è§£æ˜¯å¯¹çš„ã€‚

## å½“å‰æƒ…å†µ

çœ‹ä½ çš„ `app.py` ç¬¬75-76è¡Œï¼š

```python
uvicorn.run(app, host='0.0.0.0', port=config['port'], workers=1)
# uvicorn.run(app, host='0.0.0.0', port=config['port'], workers=1, reload=True)
```

å½“å‰ä½¿ç”¨çš„æ˜¯ç¬¬75è¡Œï¼ˆæ²¡æœ‰ `reload=True`ï¼‰ï¼Œæ‰€ä»¥ï¼š
- ä¿®æ”¹ä»£ç åéœ€è¦æ‰‹åŠ¨é‡å¯æœåŠ¡
- ä»£ç å˜æ›´ä¸ä¼šè‡ªåŠ¨ç”Ÿæ•ˆ

---

## çƒ­é‡è½½ vs æ‰‹åŠ¨é‡å¯

### å½“å‰æ¨¡å¼ï¼ˆæ‰‹åŠ¨é‡å¯ï¼‰
```python
uvicorn.run(app, host='0.0.0.0', port=config['port'], workers=1)
```
- ä¿®æ”¹ä»£ç åéœ€è¦æ‰‹åŠ¨é‡å¯
- é€‚åˆç”Ÿäº§ç¯å¢ƒï¼ˆæ›´ç¨³å®šï¼‰

### çƒ­é‡è½½æ¨¡å¼ï¼ˆè‡ªåŠ¨é‡å¯ï¼‰
```python
uvicorn.run(app, host='0.0.0.0', port=config['port'], workers=1, reload=True)
```
- ä¿®æ”¹ä»£ç åè‡ªåŠ¨æ£€æµ‹å¹¶é‡å¯
- é€‚åˆå¼€å‘ç¯å¢ƒï¼ˆæ–¹ä¾¿è°ƒè¯•ï¼‰

---

## å¦‚ä½•å¯ç”¨çƒ­é‡è½½ï¼ˆå¯é€‰ï¼‰

å¦‚æœä½ æƒ³å¯ç”¨çƒ­é‡è½½ï¼Œå¯ä»¥ä¿®æ”¹ `app.py`ï¼š

```python
# æŠŠè¿™è¡Œæ³¨é‡Šæ‰
# uvicorn.run(app, host='0.0.0.0', port=config['port'], workers=1)

# å¯ç”¨è¿™è¡Œï¼ˆå–æ¶ˆæ³¨é‡Šï¼‰
uvicorn.run(app, host='0.0.0.0', port=config['port'], workers=1, reload=True)
```

å¯ç”¨åï¼š
- ä¿®æ”¹ä»£ç ä¿å­˜åï¼ŒæœåŠ¡ä¼šè‡ªåŠ¨é‡å¯
- ç»ˆç«¯ä¼šæ˜¾ç¤º `Reloading...` å’Œ `Application startup complete`
- ä¸éœ€è¦æ‰‹åŠ¨é‡å¯

æ³¨æ„ï¼š
- çƒ­é‡è½½åªé€‚åˆå¼€å‘ç¯å¢ƒ
- ç”Ÿäº§ç¯å¢ƒå»ºè®®å…³é—­ï¼ˆä½¿ç”¨ `workers=1` ä¸”ä¸å¯ç”¨ `reload`ï¼‰

---

## ç°åœ¨ä½ éœ€è¦åšçš„

ç”±äºå½“å‰æ²¡æœ‰å¯ç”¨çƒ­é‡è½½ï¼Œä½ éœ€è¦ï¼š

1. åœæ­¢å½“å‰è¿è¡Œçš„æœåŠ¡ï¼ˆCtrl+Cï¼‰
2. é‡æ–°å¯åŠ¨æœåŠ¡
3. ç”¨Postmanå‘é€è¯·æ±‚
4. æŸ¥çœ‹ `app_info.log` æ–‡ä»¶ï¼Œåº”è¯¥èƒ½çœ‹åˆ° `[DEBUG-` å¼€å¤´çš„æ—¥å¿—

---

## æ€»ç»“

| é—®é¢˜ | ç­”æ¡ˆ |
|------|------|
| ä¿®æ”¹ä»£ç åéœ€è¦é‡å¯å—ï¼Ÿ | æ˜¯çš„ï¼Œå½“å‰é…ç½®éœ€è¦æ‰‹åŠ¨é‡å¯ |
| æ˜¯å¦æ”¯æŒçƒ­é‡è½½ï¼Ÿ | ä»£ç ä¸­æœ‰ï¼Œä½†è¢«æ³¨é‡Šæ‰äº† |
| å¦‚ä½•å¯ç”¨çƒ­é‡è½½ï¼Ÿ | å–æ¶ˆæ³¨é‡Šç¬¬76è¡Œï¼Œæ³¨é‡Šç¬¬75è¡Œ |

ç°åœ¨è¯·ï¼š
1. é‡å¯ä½ çš„æœåŠ¡
2. ç”¨Postmanå‘é€è¯·æ±‚
3. æŸ¥çœ‹ `app_info.log` çš„æœ€æ–°æ—¥å¿—
4. æŠŠåŒ…å« `[DEBUG-` çš„æ—¥å¿—å‘ç»™æˆ‘

é‡å¯ååº”è¯¥èƒ½çœ‹åˆ°æˆ‘ä»¬æ·»åŠ çš„DEBUGæ—¥å¿—äº†ã€‚

```python
uvicorn.run(app, host='0.0.0.0', port=config['port'], workers=1)
# uvicorn.run(app, host='0.0.0.0', port=config['port'], workers=1, reload=True)
```

```python
uvicorn.run(app, host='0.0.0.0', port=config['port'], workers=1)
```

```python
uvicorn.run(app, host='0.0.0.0', port=config['port'], workers=1, reload=True)
```

```python
# æŠŠè¿™è¡Œæ³¨é‡Šæ‰
# uvicorn.run(app, host='0.0.0.0', port=config['port'], workers=1)

# å¯ç”¨è¿™è¡Œï¼ˆå–æ¶ˆæ³¨é‡Šï¼‰
uvicorn.run(app, host='0.0.0.0', port=config['port'], workers=1, reload=True)
```

---

**User**

@app_info.log ä½ å¸®æˆ‘çœ‹ä¸€ä¸‹å¯ä»¥å—ï¼Ÿè¿™ä¸ªåº”è¯¥æ˜¯æœ€æ–°çš„æ—¥å¿—ã€‚æœåŠ¡é‡å¯ä»¥åé‡æ–°å‘äº†è¯·æ±‚è¿‡æ¥ç„¶åæ‰€æœ‰çš„æ—¥å¿—ä¿¡æ¯ä½ å¸®æˆ‘çœ‹ä¸€ä¸‹æ€ä¹ˆæ ·ï¼Ÿ 

---

**Cursor**

åˆ†ææœ€æ–°æ—¥å¿—ï¼Œå®šä½é—®é¢˜ï¼š


å·²å®šä½é—®é¢˜ã€‚æ£€æŸ¥ prompt æ–‡ä»¶å†…å®¹ï¼š


## é—®é¢˜åˆ†æ

### æ—¥å¿—åˆ†æç»“æœ

ä»æ—¥å¿—çœ‹ï¼Œä»£ç æ‰§è¡Œæµç¨‹æ­£å¸¸ï¼š

1. è·¯ç”±å±‚ï¼ˆç¬¬34-37è¡Œï¼‰ï¼š`dialogHistory` æ­£ç¡®ä¼ é€’
2. æœåŠ¡å±‚ï¼ˆç¬¬38-43è¡Œï¼‰ï¼šæ­£ç¡®è¿›å…¥é¦–æ¬¡è°ƒç”¨åˆ†æ”¯
3. ç”Ÿæˆè®¡åˆ’å‡½æ•°ï¼ˆç¬¬44-51è¡Œï¼‰ï¼šprompt æ–‡ä»¶åŠ è½½æˆåŠŸ

### å…³é”®é—®é¢˜ï¼ˆç¬¬52-53è¡Œï¼‰

```
[DEBUG-æœåŠ¡å±‚] interview_plan æ˜¯å¦ä¸º None: True
```

`generate_interview_plan()` è¿”å›äº† `None`ï¼Œè¯´æ˜ç”Ÿæˆè®¡åˆ’å¤±è´¥äº†ã€‚

### é”™è¯¯åŸå› 

æŸ¥çœ‹ `app_error.log`ï¼Œå‘ç°é”™è¯¯ï¼š

```
KeyError: '\n  "questions"'
```

åŸå› ï¼šåœ¨ `generate_plan_white_collar.txt` çš„ JSON ç¤ºä¾‹ä¸­ï¼Œ`{` å’Œ `}` è¢« Python çš„ `.format()` å½“ä½œå ä½ç¬¦å¤„ç†ï¼Œå¯¼è‡´æŠ¥é”™ã€‚

---

## è§£å†³æ–¹æ¡ˆ

éœ€è¦æŠŠ prompt æ–‡ä»¶ä¸­çš„ JSON ç¤ºä¾‹é‡Œçš„ `{` å’Œ `}` è½¬ä¹‰ä¸º `{{` å’Œ `}}`ã€‚

ä¿®æ”¹ `generate_plan_white_collar.txt`ï¼š

```txt
ã€è¾“å‡ºæ ¼å¼ã€‘
ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•markdownæ ‡è®°ï¼š
{{
  "questions": [
    {{
      "id": 1,
      "stage": "ç ´å†°é˜¶æ®µ",
      "question": "é—®é¢˜å†…å®¹",
      "dimension": "ä¸»è¦è€ƒå¯Ÿç»´åº¦",
      "difficulty": "ç®€å•/ä¸­ç­‰/å›°éš¾"
    }}
  ]
}}
```

æŠŠæ‰€æœ‰ JSON ç¤ºä¾‹ä¸­çš„å•ä¸ª `{` æ”¹ä¸º `{{`ï¼Œ`}` æ”¹ä¸º `}}`ã€‚

---

## ä¿®æ”¹å»ºè®®

éœ€è¦æˆ‘å¸®ä½ ä¿®æ”¹è¿™ä¸ªæ–‡ä»¶å—ï¼Ÿä¿®æ”¹åï¼š
1. é‡å¯æœåŠ¡
2. é‡æ–°å‘é€è¯·æ±‚
3. åº”è¯¥å°±èƒ½çœ‹åˆ°é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸäº†

éœ€è¦æˆ‘ç°åœ¨å°±ä¿®æ”¹å—ï¼Ÿ

```plaintext
[DEBUG-æœåŠ¡å±‚] interview_plan æ˜¯å¦ä¸º None: True
```

```plaintext
KeyError: '\n  "questions"'
```

```plaintext
ã€è¾“å‡ºæ ¼å¼ã€‘
ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•markdownæ ‡è®°ï¼š
{{
  "questions": [
    {{
      "id": 1,
      "stage": "ç ´å†°é˜¶æ®µ",
      "question": "é—®é¢˜å†…å®¹",
      "dimension": "ä¸»è¦è€ƒå¯Ÿç»´åº¦",
      "difficulty": "ç®€å•/ä¸­ç­‰/å›°éš¾"
    }}
  ]
}}
```

---

**User**

è¿˜æ˜¯ä¸è¡Œå•Šï¼Œæˆ‘åˆšåˆšé‡å¯äº†ä¸€ä¸‹æœåŠ¡ï¼Œå‘ç° æ—¥å¿—è¾“å‡ºè¿˜æ˜¯è¿™æ ·å­çš„,èƒ½ä¸èƒ½å¸®æˆ‘æ¢³ç†ä¸€ä¸‹åˆ°åº•æ˜¯ä»€ä¹ˆé—®é¢˜å•Šï¼Ÿæˆ‘ä»¬å…ˆé€æ­¥æ¢³ç†å¹¶å®šä½é—®é¢˜çš„æ ¹å› ã€‚å½“å‰ä»…åšåˆ†æä¸ç»“è®ºæ²‰æ·€ï¼Œæš‚ä¸äº§å‡ºä»»ä½•ä»£ç ã€‚

```

[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - interview_plan æ˜¯å¦ä¸º None: True

---

**Cursor**

æŸ¥çœ‹æœ€æ–°æ—¥å¿—å’Œé”™è¯¯æ—¥å¿—ï¼Œå®šä½é—®é¢˜ï¼š


æ ¹æ®æ—¥å¿—å’Œé”™è¯¯ä¿¡æ¯ï¼Œé—®é¢˜å·²å®šä½ã€‚åˆ†æå¦‚ä¸‹ï¼š

## é—®é¢˜åˆ†æ

### æ—¥å¿—æµç¨‹è¿½è¸ª

ä»æœ€æ–°æ—¥å¿—ï¼ˆç¬¬70-90è¡Œï¼‰å¯ä»¥çœ‹åˆ°ï¼š

1. è·¯ç”±å±‚ï¼ˆ71-74è¡Œï¼‰ï¼šæ­£å¸¸ï¼Œ`dialogHistory` æ­£ç¡®ä¼ é€’
2. æœåŠ¡å±‚ï¼ˆ75-80è¡Œï¼‰ï¼šæ­£å¸¸ï¼Œè¿›å…¥é¦–æ¬¡è°ƒç”¨åˆ†æ”¯
3. ç”Ÿæˆè®¡åˆ’å‡½æ•°ï¼ˆ82-88è¡Œï¼‰ï¼š
   - æ­¥éª¤1ï¼šprompt æ–‡ä»¶åŠ è½½æˆåŠŸï¼ˆé•¿åº¦ï¼š571ï¼‰
   - æ­¥éª¤2ï¼šå¼€å§‹æ ¼å¼åŒ– prompt
   - å‚æ•°éƒ½æœ‰å€¼ï¼ˆuser_portrait, jd_title, jd_portraitï¼‰
4. å…³é”®é—®é¢˜ï¼ˆ89-90è¡Œï¼‰ï¼š
   - `generate_interview_plan()` è°ƒç”¨å®Œæˆ
   - `interview_plan æ˜¯å¦ä¸º None: True`ï¼ˆå¤±è´¥ï¼‰

### é”™è¯¯æ—¥å¿—åˆ†æ

æŸ¥çœ‹ `app_error.log`ï¼Œå‘ç°ä¸¤æ¬¡é”™è¯¯ï¼š

ç¬¬ä¸€æ¬¡ï¼ˆ00:34:07ï¼‰ï¼š
```
KeyError: '\n  "questions"'
```

ç¬¬äºŒæ¬¡ï¼ˆ00:36:25ï¼‰ï¼š
```
KeyError: '\n      "id"'
```

### æ ¹æœ¬åŸå› 

é—®é¢˜å‡ºåœ¨ `generate_plan_white_collar.txt` æ–‡ä»¶çš„ JSON ç¤ºä¾‹éƒ¨åˆ†ã€‚

å½“å‰æ–‡ä»¶çŠ¶æ€ï¼š
- ç¬¬31è¡Œï¼š`{{`ï¼ˆå·²è½¬ä¹‰ï¼‰
- ç¬¬32è¡Œï¼š`  "questions": [`ï¼ˆæ­£å¸¸ï¼‰
- ç¬¬33è¡Œï¼š`    {`ï¼ˆæœªè½¬ä¹‰ï¼‰
- ç¬¬34-38è¡Œï¼šå†…å®¹æ­£å¸¸
- ç¬¬39è¡Œï¼š`    }`ï¼ˆæœªè½¬ä¹‰ï¼‰
- ç¬¬40è¡Œï¼š`  ]`ï¼ˆæ­£å¸¸ï¼‰
- ç¬¬41è¡Œï¼š`}}`ï¼ˆå·²è½¬ä¹‰ï¼‰

é—®é¢˜ï¼šå¤–å±‚å¤§æ‹¬å·å·²è½¬ä¹‰ï¼Œä½†å†…å±‚åµŒå¥—å¯¹è±¡çš„å¤§æ‹¬å·ï¼ˆç¬¬33è¡Œå’Œç¬¬39è¡Œï¼‰æœªè½¬ä¹‰ã€‚

Python çš„ `.format()` ä¼šæŠŠå•ä¸ª `{` å’Œ `}` å½“ä½œå ä½ç¬¦ï¼Œæ‰€ä»¥ï¼š
- `{` è¢«è§£æä¸ºå ä½ç¬¦å¼€å§‹
- `\n      "id"` è¢«å½“ä½œå ä½ç¬¦åç§°
- æ‰¾ä¸åˆ°å¯¹åº”å‚æ•°ï¼ŒæŠ›å‡º `KeyError`

---

## é—®é¢˜å®šä½æ€»ç»“

| é—®é¢˜ç‚¹ | ä½ç½® | å½“å‰çŠ¶æ€ | åº”è¯¥çš„çŠ¶æ€ |
|--------|------|---------|-----------|
| å¤–å±‚å¼€å§‹ | ç¬¬31è¡Œ | `{{` | `{{` |
| æ•°ç»„å¼€å§‹ | ç¬¬32è¡Œ | `  "questions": [` | `  "questions": [` |
| å†…å±‚å¯¹è±¡å¼€å§‹ | ç¬¬33è¡Œ | `    {` | `    {{` |
| å†…å±‚å¯¹è±¡ç»“æŸ | ç¬¬39è¡Œ | `    }` | `    }}` |
| æ•°ç»„ç»“æŸ | ç¬¬40è¡Œ | `  ]` | `  ]` |
| å¤–å±‚ç»“æŸ | ç¬¬41è¡Œ | `}}` | `}}` |

---

## è§£å†³æ–¹æ¡ˆ

éœ€è¦æŠŠç¬¬33è¡Œå’Œç¬¬39è¡Œçš„å•ä¸ªå¤§æ‹¬å·ä¹Ÿè½¬ä¹‰ï¼š

ç¬¬33è¡Œï¼š`    {` â†’ `    {{`  
ç¬¬39è¡Œï¼š`    }` â†’ `    }}`

ä¿®æ”¹åçš„å®Œæ•´JSONç¤ºä¾‹åº”è¯¥æ˜¯ï¼š
```json
{{
  "questions": [
    {{
      "id": 1,
      "stage": "ç ´å†°é˜¶æ®µ",
      "question": "é—®é¢˜å†…å®¹",
      "dimension": "ä¸»è¦è€ƒå¯Ÿç»´åº¦",
      "difficulty": "ç®€å•/ä¸­ç­‰/å›°éš¾"
    }}
  ]
}}
```

---

## ä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™ä¸ªé—®é¢˜ï¼Ÿ

Python çš„å­—ç¬¦ä¸² `.format()` æ–¹æ³•ä½¿ç”¨ `{` å’Œ `}` ä½œä¸ºå ä½ç¬¦ï¼š
- `{å˜é‡å}` ä¼šè¢«æ›¿æ¢ä¸ºå˜é‡çš„å€¼
- å¦‚æœè¦è¾“å‡ºå­—é¢çš„ `{` æˆ– `}`ï¼Œéœ€è¦è½¬ä¹‰ä¸º `{{` å’Œ `}}`

åœ¨ JSON ç¤ºä¾‹ä¸­ï¼š
- å¤–å±‚å¯¹è±¡ï¼š`{{` å’Œ `}}`ï¼ˆå·²è½¬ä¹‰ï¼‰
- å†…å±‚å¯¹è±¡ï¼š`{` å’Œ `}`ï¼ˆæœªè½¬ä¹‰ï¼Œå¯¼è‡´é”™è¯¯ï¼‰

---

## éªŒè¯æ–¹æ³•

ä¿®æ”¹åï¼Œåº”è¯¥èƒ½çœ‹åˆ°ï¼š
1. æ—¥å¿—ä¸­æ˜¾ç¤ºï¼š`prompt æ ¼å¼åŒ–æˆåŠŸ`
2. æ—¥å¿—ä¸­æ˜¾ç¤ºï¼š`LLM è°ƒç”¨æˆåŠŸ`
3. æ—¥å¿—ä¸­æ˜¾ç¤ºï¼š`JSON è§£ææˆåŠŸ`
4. æ—¥å¿—ä¸­æ˜¾ç¤ºï¼š`âœ… é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±Xä¸ªé—®é¢˜`
5. `interview_plan æ˜¯å¦ä¸º None: False`

---

## æ€»ç»“

- é—®é¢˜ï¼šJSON ç¤ºä¾‹ä¸­å†…å±‚å¯¹è±¡çš„å¤§æ‹¬å·æœªè½¬ä¹‰
- ä½ç½®ï¼š`generate_plan_white_collar.txt` ç¬¬33è¡Œå’Œç¬¬39è¡Œ
- è§£å†³ï¼šå°† `{` æ”¹ä¸º `{{`ï¼Œ`}` æ”¹ä¸º `}}`
- åŸå› ï¼šPython `.format()` æŠŠå•ä¸ªå¤§æ‹¬å·å½“ä½œå ä½ç¬¦

éœ€è¦æˆ‘å¸®ä½ ä¿®æ”¹è¿™ä¸ªæ–‡ä»¶å—ï¼Ÿ

```plaintext
KeyError: '\n  "questions"'
```

```plaintext
KeyError: '\n      "id"'
```

```json
{{
  "questions": [
    {{
      "id": 1,
      "stage": "ç ´å†°é˜¶æ®µ",
      "question": "é—®é¢˜å†…å®¹",
      "dimension": "ä¸»è¦è€ƒå¯Ÿç»´åº¦",
      "difficulty": "ç®€å•/ä¸­ç­‰/å›°éš¾"
    }}
  ]
}}
```

---

**User**

@python (595-653) è¿™ä¸ªç»ˆç«¯è¾“å‡ºï¼Œä½ å¸®æˆ‘åˆ¤æ–­ä¸€ä¸‹æˆåŠŸäº†å—ï¼Ÿä»¥åŠä¸ºä»€ä¹ˆå‘€ï¼Ÿå…³é”®çš„æ ‡å¿—æ˜¯ä»€ä¹ˆï¼Ÿå¸®æˆ‘åˆ†æä¸€ä¸‹ã€‚

---

**Cursor**

æ ¹æ®æ—¥å¿—ï¼Œæµç¨‹å·²æˆåŠŸã€‚å…³é”®æ ‡å¿—å¦‚ä¸‹ï¼š

## æˆåŠŸæ ‡å¿—åˆ†æ

### å…³é”®æˆåŠŸæ ‡å¿—ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰

#### 1. é¦–æ¬¡è°ƒç”¨åˆ†æ”¯è¿›å…¥æˆåŠŸï¼ˆç¬¬606è¡Œï¼‰
```
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - âœ… è¿›å…¥é¦–æ¬¡è°ƒç”¨åˆ†æ”¯
```
è¯´æ˜ï¼šæ­£ç¡®è¯†åˆ«ä¸ºç©ºå¯¹è¯å†å²ï¼Œè¿›å…¥é¦–æ¬¡è°ƒç”¨é€»è¾‘ã€‚

---

#### 2. Promptæ ¼å¼åŒ–æˆåŠŸï¼ˆç¬¬615è¡Œï¼‰
```
[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: test_interview_001 - prompt æ ¼å¼åŒ–æˆåŠŸ
```
è¯´æ˜ï¼šJSONç¤ºä¾‹ä¸­çš„å¤§æ‹¬å·è½¬ä¹‰é—®é¢˜å·²è§£å†³ï¼Œæ ¼å¼åŒ–é€šè¿‡ã€‚

---

#### 3. LLMè°ƒç”¨æˆåŠŸï¼ˆç¬¬618è¡Œï¼‰
```
[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: test_interview_001 - LLM è°ƒç”¨æˆåŠŸ
```
è¯´æ˜ï¼šLLM APIè°ƒç”¨æˆåŠŸï¼ˆè€—æ—¶çº¦21ç§’ï¼š00:38:50 â†’ 00:39:12ï¼‰ã€‚

---

#### 4. JSONè§£ææˆåŠŸï¼ˆç¬¬639è¡Œï¼‰
```
[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: test_interview_001 - JSON è§£ææˆåŠŸ
```
è¯´æ˜ï¼šè™½ç„¶LLMè¿”å›çš„å‰200å­—ç¬¦çœ‹èµ·æ¥æœ‰é‡å¤ï¼ˆç¬¬619-637è¡Œï¼‰ï¼Œä½†æœ€ç»ˆè§£æå‡ºçš„JSONæ˜¯æœ‰æ•ˆçš„ã€‚

---

#### 5. æœ€å…³é”®çš„æ ‡å¿—ï¼ˆç¬¬640è¡Œï¼‰
```
[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: test_interview_001 - âœ… é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±12ä¸ªé—®é¢˜
```
è¯´æ˜ï¼šé¢è¯•è®¡åˆ’å·²ç”Ÿæˆï¼ŒåŒ…å«12ä¸ªé—®é¢˜ã€‚

---

#### 6. æœ€é‡è¦çš„å¯¹æ¯”æ ‡å¿—ï¼ˆç¬¬642è¡Œï¼‰
```
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - interview_plan æ˜¯å¦ä¸º None: False
```
å¯¹æ¯”ä¹‹å‰å¤±è´¥çš„æ—¥å¿—ï¼š
```
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - interview_plan æ˜¯å¦ä¸º None: True  âŒ
```

ç°åœ¨ï¼š
```
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - interview_plan æ˜¯å¦ä¸º None: False  âœ…
```

è¯´æ˜ï¼š`interview_plan` ä¸å†æ˜¯ `None`ï¼Œå·²æˆåŠŸèµ‹å€¼ã€‚

---

#### 7. é¢è¯•è®¡åˆ’éªŒè¯æˆåŠŸï¼ˆç¬¬643è¡Œï¼‰
```
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - âœ… é¢è¯•è®¡åˆ’æœ‰æ•ˆï¼Œå‡†å¤‡è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜
```
è¯´æ˜ï¼šè®¡åˆ’éªŒè¯é€šè¿‡ï¼Œå‡†å¤‡è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ã€‚

---

#### 8. ç¬¬ä¸€ä¸ªé—®é¢˜æˆåŠŸè¿”å›ï¼ˆç¬¬644-646è¡Œï¼‰
```
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - ç¬¬ä¸€ä¸ªé—®é¢˜: å—¨ï¼Œå¼ ä¸‰ï¼Œæœ€è¿‘ä¸€ä»½å·¥ä½œä¸»è¦åšå“ªå—ä¸šåŠ¡ï¼Ÿå¹³æ—¶ç”¨Pythonæœ€å¤šçš„æ˜¯å“ªäº›æ¡†æ¶ï¼Ÿ...
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - âœ… å·²è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œå‡½æ•°å³å°†return
```
è¯´æ˜ï¼šæˆåŠŸæå–å¹¶è¿”å›äº†ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œå‡½æ•°æ­£å¸¸è¿”å›ã€‚

---

## æˆåŠŸåˆ¤æ–­æ€»ç»“

| æ£€æŸ¥ç‚¹ | ä¹‹å‰ï¼ˆå¤±è´¥ï¼‰ | ç°åœ¨ï¼ˆæˆåŠŸï¼‰ | çŠ¶æ€ |
|--------|------------|------------|------|
| Promptæ ¼å¼åŒ– | âŒ KeyError | âœ… æˆåŠŸ | å·²ä¿®å¤ |
| LLMè°ƒç”¨ | âŒ æœªæ‰§è¡Œ | âœ… æˆåŠŸ | æ­£å¸¸ |
| JSONè§£æ | âŒ æœªæ‰§è¡Œ | âœ… æˆåŠŸ | æ­£å¸¸ |
| è®¡åˆ’ç”Ÿæˆ | âŒ å¤±è´¥ | âœ… æˆåŠŸï¼ˆ12ä¸ªé—®é¢˜ï¼‰ | æ­£å¸¸ |
| interview_plan | âŒ None | âœ… Falseï¼ˆæœ‰å€¼ï¼‰ | æ­£å¸¸ |
| ç¬¬ä¸€ä¸ªé—®é¢˜è¿”å› | âŒ æœªè¿”å› | âœ… å·²è¿”å› | æ­£å¸¸ |

---

## å…³é”®æˆåŠŸæ ‡å¿—ï¼ˆæœ€é‡è¦ï¼‰

æœ€å…³é”®çš„æ ‡å¿—æ˜¯ç¬¬642è¡Œï¼š
```
interview_plan æ˜¯å¦ä¸º None: False
```

è¿™è¡¨ç¤ºï¼š
1. `generate_interview_plan()` æˆåŠŸæ‰§è¡Œ
2. è¿”å›äº†æœ‰æ•ˆçš„é¢è¯•è®¡åˆ’å¯¹è±¡
3. è®¡åˆ’è¢«æ­£ç¡®å­˜å‚¨åˆ° `self.interview_plan`
4. åç»­é€»è¾‘å¯ä»¥æ­£å¸¸ä½¿ç”¨è¿™ä¸ªè®¡åˆ’

---

## å…³äºLLMè¿”å›å†…å®¹çš„è¯´æ˜

ç¬¬619-637è¡Œæ˜¾ç¤ºçš„LLMè¿”å›å†…å®¹çœ‹èµ·æ¥æœ‰é‡å¤ï¼š
```
{
  "questions": [
    {
      "id": 1,
      "stage": "ç ´å†°é˜¶æ®µ",
    {
      "id": 1,  â† é‡å¤
      "stage": "ç ´å†°é˜¶æ®µ",  â† é‡å¤
```

è¿™å¯èƒ½æ˜¯ï¼š
1. æ—¥å¿—åªæ˜¾ç¤ºäº†å‰200å­—ç¬¦ï¼Œå¯èƒ½æˆªæ–­äº†
2. LLMè¿”å›çš„å®Œæ•´JSONæ˜¯æœ‰æ•ˆçš„ï¼ˆå› ä¸ºJSONè§£ææˆåŠŸäº†ï¼‰
3. æœ€ç»ˆè§£æå‡ºçš„è®¡åˆ’åŒ…å«12ä¸ªé—®é¢˜ï¼Œè¯´æ˜JSONç»“æ„æ­£ç¡®

---

## æœ€ç»ˆç»“è®º

æˆåŠŸã€‚å…³é”®è¯æ®ï¼š
1. `interview_plan æ˜¯å¦ä¸º None: False`ï¼ˆä¹‹å‰æ˜¯ `True`ï¼‰
2. `âœ… é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±12ä¸ªé—®é¢˜`
3. `âœ… é¢è¯•è®¡åˆ’æœ‰æ•ˆï¼Œå‡†å¤‡è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜`
4. `âœ… å·²è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œå‡½æ•°å³å°†return`
5. ç¬¬ä¸€ä¸ªé—®é¢˜å†…å®¹æ­£ç¡®è¿”å›

æ•´ä¸ªæµç¨‹å·²æ­£å¸¸è¿è¡Œã€‚ä¸‹ä¸€æ­¥å¯ä»¥æµ‹è¯•å¤šè½®å¯¹è¯ï¼ŒéªŒè¯é¢è¯•è®¡åˆ’åœ¨åç»­è°ƒç”¨ä¸­æ˜¯å¦è¢«æ­£ç¡®ä½¿ç”¨ã€‚

```plaintext
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - âœ… è¿›å…¥é¦–æ¬¡è°ƒç”¨åˆ†æ”¯
```

```plaintext
[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: test_interview_001 - prompt æ ¼å¼åŒ–æˆåŠŸ
```

```plaintext
[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: test_interview_001 - LLM è°ƒç”¨æˆåŠŸ
```

```plaintext
[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: test_interview_001 - JSON è§£ææˆåŠŸ
```

```plaintext
[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: test_interview_001 - âœ… é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±12ä¸ªé—®é¢˜
```

```plaintext
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - interview_plan æ˜¯å¦ä¸º None: False
```

```plaintext
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - interview_plan æ˜¯å¦ä¸º None: True  âŒ
```

```plaintext
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - interview_plan æ˜¯å¦ä¸º None: False  âœ…
```

```plaintext
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - âœ… é¢è¯•è®¡åˆ’æœ‰æ•ˆï¼Œå‡†å¤‡è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜
```

```plaintext
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - ç¬¬ä¸€ä¸ªé—®é¢˜: å—¨ï¼Œå¼ ä¸‰ï¼Œæœ€è¿‘ä¸€ä»½å·¥ä½œä¸»è¦åšå“ªå—ä¸šåŠ¡ï¼Ÿå¹³æ—¶ç”¨Pythonæœ€å¤šçš„æ˜¯å“ªäº›æ¡†æ¶ï¼Ÿ...
[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: test_interview_001 - âœ… å·²è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œå‡½æ•°å³å°†return
```

```plaintext
interview_plan æ˜¯å¦ä¸º None: False
```

```plaintext
{
  "questions": [
    {
      "id": 1,
      "stage": "ç ´å†°é˜¶æ®µ",
    {
      "id": 1,  â† é‡å¤
      "stage": "ç ´å†°é˜¶æ®µ",  â† é‡å¤
```

---

**User**

ä½†æ˜¯æ˜¯å¦å¯ä»¥é€šè¿‡æ—¥å¿—çš„æ–¹å¼æ¥æŸ¥çœ‹ è¿™ä¸ª12ä¸ªé—®é¢˜åˆ°åº•æ˜¯å“ªäº›é—®é¢˜å‘¢ï¼Ÿæˆ‘å¥½è¯„ä¼°ä¸€ä¸‹å®ƒç”Ÿæˆçš„é—®é¢˜æ˜¯å¦åˆç†å‘€ï¼Ÿæ˜¯å¦æ·»åŠ é¢å¤–çš„æ—¥å¿—è®°å½•å‘¢ï¼Ÿ

---

**Cursor**

æ·»åŠ æ—¥å¿—ï¼Œè¾“å‡ºå®Œæ•´çš„é¢è¯•è®¡åˆ’å†…å®¹ï¼Œä¾¿äºè¯„ä¼°é—®é¢˜æ˜¯å¦åˆç†ã€‚


æ·»åŠ è¯¦ç»†æ—¥å¿—ï¼Œè¾“å‡ºå®Œæ•´çš„é¢è¯•è®¡åˆ’å†…å®¹ï¼š


```python
import asyncio
import json
import logging
import time
import re
import regex
from pathlib import Path
from openai import OpenAI, AsyncOpenAI
import dashscope
import dashscope.audio.tts_v2
from app.config import ai_config
from sdk.tts.punctuation_based_tts import PunctuationBasedTTS


def load_prompt(filename: str) -> str:
    """
    åŠ è½½promptæ–‡ä»¶
    
    Args:
        filename: æ–‡ä»¶åï¼Œå¦‚ "strategy_white_collar.txt"
    
    Returns:
        promptå†…å®¹
    """
    current_dir = Path(__file__).parent
    prompt_path = current_dir.parent.parent / "prompts" / "interview" / filename
    with open(prompt_path, 'r', encoding='utf-8') as f:
        return f.read()



dashscope.api_key = ai_config['api_key']
class CallInterview:
    def __init__(self,  interview_id: str):

        self.interview_id = interview_id
        self.ai_config = ai_config
        self.request_id = None
        self.current_question = None
        self.timeout_task = None
        self.voice_active = False
        self.jd_title = None
        self.jd_portrait = None
        self.user_portrait = None
        self.company_portrait = None
        self.company_name = None
        self.interview_type = 1
        self.speech_speed = 1.2
        self.interview_language = "zhongwen"
        self.interview_style = None
        self.TTS_model = 'longanxuan'
        self.interview_plan = None
        self.current_question_index = 0
        self.language_dict = {
            'zhongwen': ('ä¸­æ–‡', 'ä½ å¥½ï¼Œé¢è¯•å®˜ï¼è®©æˆ‘ä»¬ç”¨ä¸­æ–‡äº¤æµï¼', 'longanxuan'),
            'yingwen': ('è‹±æ–‡', "Hello, interviewer!Let's communicate in English!", 'longxiaochun_v2'),
            'riyu': ('æ—¥è¯­', 'ã“ã‚“ã«ã¡ã¯ã€é¢æ¥å®˜ï¼æ—¥æœ¬èªã§è©±ã—åˆã„ã¾ã—ã‚‡ã†ï¼', 'loongtomoka_v2'),
            'hanyu': ('éŸ©è¯­', 'ì•ˆë…•í•˜ì„¸ìš”, ë©´ì ‘ê´€ë‹˜!í•œêµ­ì–´ë¡œ ëŒ€í™”í•´ìš”!', 'loongkyong_v2'),
            'xibanyayu': ('è¥¿ç­ç‰™è¯­', 'Â¡Hola, entrevistador!Â¡Hablemos en espaÃ±ol!', 'sambert-camila-v1'),
            'yinniyu': ('å°å°¼è¯­', 'Halo, pewawancara!Ayo berkomunikasi dalam bahasa Indonesia!', 'sambert-indah-v1'),
            'fayu': ('æ³•è¯­', 'Bonjour, intervieweur !Parlons en franÃ§ais !', 'sambert-clara-v1'),
            'deyu': ('å¾·è¯­', 'Hallo, Interviewer!Lass uns auf Deutsch sprechen!', 'sambert-hanna-v1'),
            'taiyu': ('æ³°è¯­', 'à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¹ˆà¸°/à¸„à¸£à¸±à¸š, à¸œà¸¹à¹‰à¸ªà¸±à¸¡à¸ à¸²à¸©à¸“à¹Œ!à¸¡à¸²à¸à¸¹à¸”à¸„à¸¸à¸¢à¸à¸±à¸™à¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸à¸±à¸™à¹€à¸–à¸­à¸°!', 'sambert-waan-v1')
        }




        # åˆ›å»º OpenAI å®¢æˆ·ç«¯å¯¹è±¡
        self.celue_client = OpenAI(
            api_key=self.ai_config['api_key'],
            base_url=self.ai_config['api_uri'],
            timeout=60.0,
        )
        self.AI_client = AsyncOpenAI(
            api_key=self.ai_config['api_key'],
            base_url=self.ai_config['api_uri'],
            timeout=30.0,
        )
        # 1. è·å–å½“å‰æ­£åœ¨è¿è¡Œçš„äº‹ä»¶å¾ªç¯
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        class FutureCallback(dashscope.audio.tts_v2.ResultCallback):
            def __init__(self, future: asyncio.Future):
                self._future = future
                self._audio_buffer = bytearray()  # ç”¨äºæ‹¼æ¥éŸ³é¢‘ç‰‡æ®µ

            def on_open(self):
                pass  # è¿™é‡Œå¯ä»¥åŠ æ—¥å¿—

            def on_complete(self):
                # è¯­éŸ³åˆæˆå®Œæˆï¼Œæ‰€æœ‰ç‰‡æ®µéƒ½å·²æ”¶åˆ°
                # å°†æ‹¼æ¥å¥½çš„å®Œæ•´éŸ³é¢‘æ•°æ®ä½œä¸ºFutureçš„ç»“æœ
                if not self._future.done():
                    self._future.set_result(bytes(self._audio_buffer))

            def on_error(self, message: str):
                # å‘ç”Ÿé”™è¯¯ï¼Œå°†å¼‚å¸¸ä½œä¸ºFutureçš„ç»“æœ
                if not self._future.done():
                    self._future.set_exception(Exception(f"TTS Error: {message}"))

            def on_close(self):
                # è¿æ¥å…³é—­ï¼Œå¦‚æœFutureè¿˜æœªå®Œæˆï¼ˆå¯èƒ½åœ¨on_completeå‰å…³é—­ï¼‰ï¼Œä¹Ÿè®¾ç½®ç»“æœ
                if not self._future.done():
                    self._future.set_result(bytes(self._audio_buffer))

            def on_data(self, data: bytes):
                # æ”¶åˆ°éŸ³é¢‘ç‰‡æ®µï¼Œå°†å…¶æ‹¼æ¥åˆ°ç¼“å†²åŒº
                if data:
                    self._audio_buffer.extend(data)

        # 3. å°†æ–°çš„Callbackç±»ç»‘å®šåˆ°selfï¼Œä»¥ä¾¿synthesis_ttsæ–¹æ³•ä½¿ç”¨
        self.FutureCallback = FutureCallback


    async def generate_interview_plan(self):
        """ç”Ÿæˆé¢è¯•è®¡åˆ’ï¼ˆä»…é¦–æ¬¡è°ƒç”¨ï¼‰"""
        logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - ğŸ¯ è¿›å…¥ generate_interview_plan å‡½æ•°")
        try:
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - æ­¥éª¤1: åŠ è½½ prompt æ–‡ä»¶")
            plan_prompt = load_prompt("generate_plan_white_collar.txt")
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - prompt æ–‡ä»¶åŠ è½½æˆåŠŸï¼Œé•¿åº¦: {len(plan_prompt)}")
            
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - æ­¥éª¤2: æ ¼å¼åŒ– prompt")
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - user_portrait: {self.user_portrait[:50] if self.user_portrait else 'None'}...")
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - jd_title: {self.jd_title}")
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - jd_portrait: {self.jd_portrait[:50] if self.jd_portrait else 'None'}...")
            
            plan_prompt_filled = plan_prompt.format(
                user_portrait=self.user_portrait,
                jd_title=self.jd_title,
                jd_portrait=self.jd_portrait
            )
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - prompt æ ¼å¼åŒ–æˆåŠŸ")
            
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - æ­¥éª¤3: è°ƒç”¨ LLM ç”Ÿæˆè®¡åˆ’")
            response = self.celue_client.chat.completions.create(
                model=self.ai_config['api_model'],
                temperature=0.7,
                messages=[
                    {"role": "system", "content": plan_prompt_filled},
                    {"role": "user", "content": "è¯·ç”Ÿæˆé¢è¯•è®¡åˆ’"}
                ]
            )
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - LLM è°ƒç”¨æˆåŠŸ")
            
            plan_json = response.choices[0].message.content
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - LLM è¿”å›å†…å®¹ï¼ˆå‰200å­—ç¬¦ï¼‰: {plan_json[:200]}")
            
            # æ¸…ç†å¯èƒ½çš„markdownæ ‡è®°
            plan_json = plan_json.replace("```json", "").replace("```", "").strip()
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - æ­¥éª¤4: è§£æ JSON")
            self.interview_plan = json.loads(plan_json)
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - JSON è§£ææˆåŠŸ")
            
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - âœ… é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸï¼Œå…±{len(self.interview_plan['questions'])}ä¸ªé—®é¢˜")
            
            # è¯¦ç»†è¾“å‡ºæ‰€æœ‰é—®é¢˜å†…å®¹
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - " + "="*80)
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - ğŸ“‹ å®Œæ•´é¢è¯•è®¡åˆ’å†…å®¹ï¼š")
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - " + "="*80)
            for idx, question in enumerate(self.interview_plan['questions'], 1):
                logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - ")
                logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - ã€é—®é¢˜ {idx}/{len(self.interview_plan['questions'])}ã€‘")
                logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} -   é˜¶æ®µ: {question.get('stage', 'æœªçŸ¥')}")
                logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} -   è€ƒå¯Ÿç»´åº¦: {question.get('dimension', 'æœªçŸ¥')}")
                logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} -   éš¾åº¦: {question.get('difficulty', 'æœªçŸ¥')}")
                logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} -   é—®é¢˜å†…å®¹: {question.get('question', 'æœªçŸ¥')}")
            logging.info(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - " + "="*80)
            
            return self.interview_plan
            
        except Exception as e:
            logging.error(f"[DEBUG-ç”Ÿæˆè®¡åˆ’] é¢è¯•ID: {self.interview_id} - âŒ ç”Ÿæˆé¢è¯•è®¡åˆ’å¤±è´¥: {e}", exc_info=True)
            # å¤±è´¥æ—¶è¿”å›ç©ºè®¡åˆ’ï¼Œèµ°åŸæœ‰é€»è¾‘
            return None


    async def get_ai_response(self, dialog_history):
        start_time = time.time()

        # è°ƒè¯•æ—¥å¿—
        logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - get_ai_response è¢«è°ƒç”¨")
        logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - dialog_history ç±»å‹: {type(dialog_history)}")
        logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - dialog_history å€¼: {dialog_history}")
        logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - not dialog_history = {not dialog_history}")
        logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - len(dialog_history) = {len(dialog_history) if dialog_history is not None else 'None'}")

        # åˆ¤æ–­æ˜¯å¦é¦–æ¬¡è°ƒç”¨
        if not dialog_history or len(dialog_history) == 0:
            logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - âœ… è¿›å…¥é¦–æ¬¡è°ƒç”¨åˆ†æ”¯")
            # é¦–æ¬¡è°ƒç”¨ï¼šç”Ÿæˆé¢è¯•è®¡åˆ’
            logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - å‡†å¤‡è°ƒç”¨ generate_interview_plan()")
            await self.generate_interview_plan()
            logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - generate_interview_plan() è°ƒç”¨å®Œæˆ")
            logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - interview_plan æ˜¯å¦ä¸º None: {self.interview_plan is None}")
            
            # è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆå¼€åœºç™½ï¼‰
            if self.interview_plan and len(self.interview_plan['questions']) > 0:
                logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - âœ… é¢è¯•è®¡åˆ’æœ‰æ•ˆï¼Œå‡†å¤‡è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜")
                first_question = self.interview_plan['questions'][0]['question']
                self.current_question_index = 1  # ä¸‹æ¬¡ä»ç¬¬äºŒä¸ªé—®é¢˜å¼€å§‹
                
                logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - ç¬¬ä¸€ä¸ªé—®é¢˜: {first_question[:50]}...")
                yield f"data: {json.dumps({'type': 'chunk', 'content': first_question}, ensure_ascii=False)}\n\n"
                yield f"data: {json.dumps({'type': 'DONE', 'content': '[DONE]'}, ensure_ascii=False)}\n\n"
                logging.info(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - âœ… å·²è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œå‡½æ•°å³å°†return")
                return
            else:
                logging.warning(f"[DEBUG-æœåŠ¡å±‚] é¢è¯•ID: {self.interview_id} - âš ï¸ é¢è¯•è®¡åˆ’ä¸ºç©ºæˆ–æ²¡æœ‰é—®é¢˜ï¼Œç»§ç»­æ‰§è¡ŒåŸæœ‰æµç¨‹")

        language, say_hi, self.TTS_model = self.language_dict[self.interview_language]
        system_message = f'''
                        æœ¬æ¬¡é¢è¯•å‚è€ƒèµ„æ–™å¦‚ä¸‹ï¼š
                        #é¢è¯•äº¤æµå…¨ç¨‹ä½¿ç”¨è¯­ç§ï¼š{language}
                        é¢è¯•å¼€å§‹æ—¶é—´ï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}
                        å€™é€‰äººä¿¡æ¯ï¼š{self.user_portrait}
                        å²—ä½ä¿¡æ¯ï¼š{self.jd_title},{self.jd_portrait}
                        å…¬å¸ä¿¡æ¯ï¼š{self.company_portrait}
                        #é¢è¯•äº¤æµå…¨ç¨‹ä½¿ç”¨è¯­ç§ï¼š{language}
                        é¢è¯•æ­£å¼å¼€å§‹ï¼
                                        '''

        self.speech_speed = 1
        
        # åŠ è½½promptæ¨¡æ¿
        celue_template = load_prompt("strategy_white_collar.txt")
        zhixing_template = load_prompt("executor_white_collar.txt")
        lanling_template = load_prompt("executor_blue_collar.txt")
        
        # æ›¿æ¢å˜é‡
        lang = self.language_dict[self.interview_language][0]
        self.celue_sys_prompt = celue_template.format(language=lang)
        self.zhixing_sys_prompt = zhixing_template.format(language=lang)
        self.lanling_sys_prompt = lanling_template.format(language=lang)
        try:
            # å‡†å¤‡é¢è¯•è®¡åˆ’ä¿¡æ¯
            plan_info = ""
            if self.interview_plan:
                remaining_questions = self.interview_plan['questions'][self.current_question_index:]
                plan_info = f"""
                ã€é¢è¯•è®¡åˆ’å‚è€ƒã€‘
                å½“å‰è¿›åº¦ï¼šå·²æé—® {self.current_question_index}/{len(self.interview_plan['questions'])} ä¸ªé—®é¢˜
                å¾…æé—®é¢˜ï¼š{json.dumps(remaining_questions[:3], ensure_ascii=False)}
                ï¼ˆæ³¨æ„ï¼šè¿™åªæ˜¯å‚è€ƒè®¡åˆ’ï¼Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µçµæ´»è°ƒæ•´ï¼‰
                """
            
            #interview_type 1ç™½é¢†ï¼Œ0è“é¢†
            if self.interview_type == 1:
                self.speech_speed = 1.1
                celue_message = [{"role": "system", "content": self.celue_sys_prompt},
                                 {"role": "user",
                                  "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}ï¼Œå½“å‰æ—¶é—´ä¸º{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}{plan_info}"}]
                logging.info(f"-- é¢è¯•ID ï¼š{self.interview_id},å½“å‰ç­–ç•¥promptå¦‚ä¸‹--ï¼š{celue_message}")
                for i in range(2):
                    try:
                        celue = self.celue_client.chat.completions.create(
                            model=self.ai_config['api_model'],
                            temperature=0.4,
                            messages=celue_message,
                            extra_body={"chat_template_kwargs": {"enable_thinking": True}},
                        )
                        celue_text = celue.choices[0].message.content
                        break
                    except Exception as e:
                        logging.error(f"-- é¢è¯•ID : {self.interview_id}---ç­–ç•¥è°ƒç”¨AIå‡ºé”™ï¼Œç¬¬{i + 1}æ¬¡å°è¯•: {e}")
                logging.info(
                    f"-- é¢è¯•ID ï¼š{self.interview_id},æ¨ç†æ—¶é—´ï¼š{time.time() - start_time}s,å½“å‰ç­–ç•¥å¦‚ä¸‹--ï¼š\n{celue_text}")
                # å¯ç”¨æµå¼ä¼ è¾“
                zhixing_message = [{"role": "system", "content": self.zhixing_sys_prompt},
                                   {"role": "user",
                                    "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history},å½“å‰æ—¶é—´ä¸ºï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}ï¼Œå½“å‰å›å¤ç­–ç•¥å¦‚ä¸‹{celue_text}{plan_info}"}]
            else:
                zhixing_message = [{"role": "system", "content": self.lanling_sys_prompt},
                                   {"role": "user",
                                    "content": f"å½“å‰é¢è¯•è®°å½•å¦‚ä¸‹{dialog_history}ï¼Œå½“å‰æ—¶é—´ä¸º{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}ï¼Œå…¶ä»–æ¶ˆæ¯ï¼š{system_message}"}]
            logging.info(f"-- é¢è¯•ID ï¼š{self.interview_id},å½“å‰æ‰§è¡Œpromptå¦‚ä¸‹--ï¼š{zhixing_message}")
            #é‡è¯•æ¬¡æ•°
            for i in range(2):
                try:
                    response = await self.AI_client.chat.completions.create(
                        stream=True,  # å¯ç”¨æµå¼
                        model=self.ai_config['api_model'],
                        temperature=0.7,
                        seed=1,
                        messages=zhixing_message
                    )
                    break
                except Exception as e:
                    logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµå¼è°ƒç”¨AIå‡ºé”™ï¼Œç¬¬{i + 1}æ¬¡å°è¯•: {e}")
            if not response:
                logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµå¼è°ƒç”¨AIå¤±è´¥")
            ai_response = ""
            async for chunk in response:
                if chunk.choices and chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    ai_response += content

                    yield f"data: {json.dumps({'type': 'chunk', 'content': content}, ensure_ascii=False)}\n\n"

            yield f"data: {json.dumps({'type': 'DONE', 'content': "[DONE]"}, ensure_ascii=False)}\n\n"

            # æ›´æ–°é—®é¢˜ç´¢å¼•
            if self.interview_plan:
                self.current_question_index += 1

            end_time = time.time()
            execution_time = end_time - start_time
            logging.info(f"-- é¢è¯•ID : {self.interview_id}---æ‰§è¡Œæ—¶é—´ä¸º: {execution_time} ç§’,AIæµå¼è¿”å›å†…å®¹: {ai_response}")
            try:
                flash_time = time.time()
                #åˆ¤æ–­é¢è¯•æ˜¯å¦ç»“æŸï¼Œæ¯ä¸ªå¾ªç¯éƒ½æœ‰
                liucheng_prompt = load_prompt("judge_flow.txt")
                liucheng = self.celue_client.chat.completions.create(
                    model="qwen-flash",
                    temperature=0.4,
                    messages=[{"role": "system", "content": liucheng_prompt},
                              {"role": "user", "content": f"é¢è¯•å®˜å›å¤å†…å®¹å¦‚ä¸‹{ai_response}"}],
                )
                liucheng_flag = liucheng.choices[0].message.content
                logging.info(
                    f"-- é¢è¯•ID : {self.interview_id}---æµç¨‹åˆ¤æ–­å†…å®¹:  {liucheng_flag}  ,è€—æ—¶{time.time() - flash_time}ç§’")
                if "ç»“æŸ" in liucheng_flag:
                    yield f"data: {json.dumps({'type': 'over', 'content': "over"}, ensure_ascii=False)}\n\n"
            except Exception as e:
                logging.error(f"-- é¢è¯•ID : {self.interview_id}---æµç¨‹åˆ¤æ–­å‡ºé”™: {e}")
                yield f"data: {json.dumps({'type': 'error', 'content': str(e)}, ensure_ascii=False)}\n\n"

        except Exception as e:
            logging.error(f"-- é¢è¯•ID : {self.interview_id}---AIæµå¼å“åº”è·å–é”™è¯¯: {e}")
            yield f"data: {json.dumps({'type': 'error', 'content': "æ¨¡å‹æ£€æµ‹åˆ°éæ³•è¾“å…¥ï¼Œè¯·é‡æ–°å¼€å§‹æœ¬æ¬¡é¢è¯•"+str(e)}, ensure_ascii=False)}\n\n"






```

å·²æ·»åŠ è¯¦ç»†æ—¥å¿—è¾“å‡ºã€‚ç°åœ¨æ—¥å¿—ä¼šæ˜¾ç¤ºå®Œæ•´çš„é¢è¯•è®¡åˆ’å†…å®¹ã€‚

## æ·»åŠ çš„æ—¥å¿—å†…å®¹

åœ¨é¢è¯•è®¡åˆ’ç”ŸæˆæˆåŠŸåï¼Œä¼šè¾“å‡ºä»¥ä¸‹ä¿¡æ¯ï¼š

### æ—¥å¿—æ ¼å¼ç¤ºä¾‹ï¼š

```
================================================================================
ğŸ“‹ å®Œæ•´é¢è¯•è®¡åˆ’å†…å®¹ï¼š
================================================================================

ã€é—®é¢˜ 1/12ã€‘
  é˜¶æ®µ: ç ´å†°é˜¶æ®µ
  è€ƒå¯Ÿç»´åº¦: å²—ä½ä¸“ä¸šæŠ€èƒ½è€ƒæ ¸
  éš¾åº¦: ç®€å•
  é—®é¢˜å†…å®¹: å—¨ï¼Œå¼ ä¸‰ï¼Œæœ€è¿‘ä¸€ä»½å·¥ä½œä¸»è¦åšå“ªå—ä¸šåŠ¡ï¼Ÿå¹³æ—¶ç”¨Pythonæœ€å¤šçš„æ˜¯å“ªäº›æ¡†æ¶ï¼Ÿ

ã€é—®é¢˜ 2/12ã€‘
  é˜¶æ®µ: ç ´å†°é˜¶æ®µ
  è€ƒå¯Ÿç»´åº¦: èŒä¸šè§„åˆ’æ¢è®¨
  éš¾åº¦: ç®€å•
  é—®é¢˜å†…å®¹: ...

ã€é—®é¢˜ 3/12ã€‘
  ...
```

### æ¯ä¸ªé—®é¢˜ä¼šæ˜¾ç¤ºï¼š

1. é—®é¢˜ç¼–å·ï¼š`ã€é—®é¢˜ X/12ã€‘`
2. é˜¶æ®µï¼šç ´å†°é˜¶æ®µ / æ ¸å¿ƒæ¢ç´¢ / æ”¶å°¾é˜¶æ®µ
3. è€ƒå¯Ÿç»´åº¦ï¼šå²—ä½ä¸“ä¸šæŠ€èƒ½è€ƒæ ¸ / èŒä¸šè§„åˆ’æ¢è®¨ / è½¯ç¡¬æ€§èƒ½åŠ›è¯„ä¼° / è–ªé…¬é¢„æœŸåˆç†æ€§åˆ¤æ–­
4. éš¾åº¦ï¼šç®€å• / ä¸­ç­‰ / å›°éš¾
5. é—®é¢˜å†…å®¹ï¼šå®Œæ•´çš„é—®é¢˜æ–‡æœ¬

---

## å¦‚ä½•æŸ¥çœ‹

### æ–¹æ³•1ï¼šæŸ¥çœ‹æ—¥å¿—æ–‡ä»¶
æ‰“å¼€ `project_code/career/code/logs/app_info.log`ï¼Œæœç´¢ `ğŸ“‹ å®Œæ•´é¢è¯•è®¡åˆ’å†…å®¹`ï¼Œå³å¯çœ‹åˆ°æ‰€æœ‰é—®é¢˜ã€‚

### æ–¹æ³•2ï¼šåœ¨Terminalä¸­æŸ¥çœ‹
é‡å¯æœåŠ¡åï¼Œå‘é€è¯·æ±‚ï¼ŒTerminalä¼šå®æ—¶æ˜¾ç¤ºå®Œæ•´çš„é¢è¯•è®¡åˆ’ã€‚

---

## ä¸‹ä¸€æ­¥æ“ä½œ

1. é‡å¯æœåŠ¡ï¼ˆä½¿æ–°ä»£ç ç”Ÿæ•ˆï¼‰
2. ç”¨Postmanå‘é€ä¸€æ¬¡æ–°çš„è¯·æ±‚
3. æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶æˆ–Terminalï¼Œæ‰¾åˆ° `ğŸ“‹ å®Œæ•´é¢è¯•è®¡åˆ’å†…å®¹` éƒ¨åˆ†
4. è¯„ä¼°ç”Ÿæˆçš„12ä¸ªé—®é¢˜æ˜¯å¦åˆç†

---

## è¯„ä¼°è¦ç‚¹

è¯„ä¼°æ—¶å…³æ³¨ï¼š
- é—®é¢˜æ•°é‡ï¼šæ˜¯å¦åœ¨10-12ä¸ªä¹‹é—´
- é˜¶æ®µåˆ†å¸ƒï¼šç ´å†°ã€æ ¸å¿ƒæ¢ç´¢ã€æ”¶å°¾æ˜¯å¦éƒ½æœ‰
- ç»´åº¦è¦†ç›–ï¼š5ä¸ªç»´åº¦æ˜¯å¦éƒ½è¦†ç›–åˆ°
- é—®é¢˜è´¨é‡ï¼šæ˜¯å¦ç»“åˆå€™é€‰äººèƒŒæ™¯ï¼Œæ˜¯å¦å£è¯­åŒ–
- éš¾åº¦æ¢¯åº¦ï¼šæ˜¯å¦ä»ç®€å•åˆ°å¤æ‚

é‡å¯æœåŠ¡å¹¶å‘é€è¯·æ±‚åï¼Œä½ å°±èƒ½åœ¨æ—¥å¿—ä¸­çœ‹åˆ°å®Œæ•´çš„12ä¸ªé—®é¢˜äº†ã€‚

```plaintext
================================================================================
ğŸ“‹ å®Œæ•´é¢è¯•è®¡åˆ’å†…å®¹ï¼š
================================================================================

ã€é—®é¢˜ 1/12ã€‘
  é˜¶æ®µ: ç ´å†°é˜¶æ®µ
  è€ƒå¯Ÿç»´åº¦: å²—ä½ä¸“ä¸šæŠ€èƒ½è€ƒæ ¸
  éš¾åº¦: ç®€å•
  é—®é¢˜å†…å®¹: å—¨ï¼Œå¼ ä¸‰ï¼Œæœ€è¿‘ä¸€ä»½å·¥ä½œä¸»è¦åšå“ªå—ä¸šåŠ¡ï¼Ÿå¹³æ—¶ç”¨Pythonæœ€å¤šçš„æ˜¯å“ªäº›æ¡†æ¶ï¼Ÿ

ã€é—®é¢˜ 2/12ã€‘
  é˜¶æ®µ: ç ´å†°é˜¶æ®µ
  è€ƒå¯Ÿç»´åº¦: èŒä¸šè§„åˆ’æ¢è®¨
  éš¾åº¦: ç®€å•
  é—®é¢˜å†…å®¹: ...

ã€é—®é¢˜ 3/12ã€‘
  ...
```

---

**User**

ç°åœ¨å‘¢ æ ¹æ®å‰é¢é‚£äº›å¯¹è¯ï¼Œä½ å¸®æˆ‘æ€»ç»“ä¸€ä¸‹ï¼Œæˆ‘æ”¹äº†è¿™äº›åŠŸèƒ½ä»¥åŠå¦‚ä½•æŠŠè¿™äº›æ”¾å…¥åˆ°æˆ‘çš„commit å‘½ä»¤çš„ æ¶ˆæ¯é‡Œé¢ï¼Œå› ä¸ºæˆ‘æ˜¯è¦é€šè¿‡commitå‘½ä»¤æäº¤åˆ°ä»“åº“é‡Œé¢å‘€ã€‚git commit -m "xxx" è¿™é‡Œçš„ä¿¡æ¯æˆ‘åº”è¯¥å†™å“ªäº›ä¿¡æ¯å‘¢ï¼Ÿè¦æ±‚æ˜¯ä¸­æ–‡çš„ä¿¡æ¯ï¼Œè€Œä¸”è¦ç¬¦åˆcommitæ ¼å¼ã€‚

---

**Cursor**

æ€»ç»“æœ¬æ¬¡æ”¹åŠ¨ï¼Œå¹¶ç”Ÿæˆç¬¦åˆè§„èŒƒçš„ commit æ¶ˆæ¯ï¼š


## æœ¬æ¬¡æ”¹åŠ¨æ€»ç»“

### æ ¸å¿ƒåŠŸèƒ½
1. æ–°å¢é¢è¯•è®¡åˆ’ç”ŸæˆåŠŸèƒ½
   - é¦–æ¬¡è°ƒç”¨æ—¶è‡ªåŠ¨ç”Ÿæˆ10-12ä¸ªé—®é¢˜çš„é¢è¯•è®¡åˆ’
   - è¦†ç›–5ä¸ªè¯„ä¼°ç»´åº¦ï¼ˆä¸“ä¸šæŠ€èƒ½ã€èŒä¸šè§„åˆ’ã€è½¯ç¡¬èƒ½åŠ›ã€è–ªé…¬ã€æ€§æ ¼ï¼‰
   - åˆ†ä¸º3ä¸ªé˜¶æ®µï¼ˆç ´å†°ã€æ ¸å¿ƒæ¢ç´¢ã€æ”¶å°¾ï¼‰

2. ä¼˜åŒ–é¢è¯•æµç¨‹
   - é¦–æ¬¡è°ƒç”¨è¿”å›è®¡åˆ’ä¸­çš„ç¬¬ä¸€ä¸ªé—®é¢˜
   - åç»­è°ƒç”¨æ—¶ï¼Œç­–ç•¥å±‚å’Œæ‰§è¡Œå±‚å‚è€ƒé¢è¯•è®¡åˆ’
   - ä¿æŒåŠ¨æ€è°ƒæ•´èƒ½åŠ›

3. æ·»åŠ è°ƒè¯•æ—¥å¿—
   - è·¯ç”±å±‚ã€æœåŠ¡å±‚ã€ç”Ÿæˆè®¡åˆ’å‡½æ•°æ·»åŠ è¯¦ç»†æ—¥å¿—
   - å¯å®Œæ•´æŸ¥çœ‹ç”Ÿæˆçš„é¢è¯•è®¡åˆ’å†…å®¹

### æ–‡ä»¶æ”¹åŠ¨
- `virtual_interview.py` - æ–°å¢æ–¹æ³•ã€ä¿®æ”¹æµç¨‹é€»è¾‘
- `interview_process_router.py` - æ·»åŠ è°ƒè¯•æ—¥å¿—
- `generate_plan_white_collar.txt` - æ–°å¢é¢è¯•è®¡åˆ’ç”Ÿæˆ prompt

---

## Git Commit æ¶ˆæ¯ï¼ˆæ¨èï¼‰

### æ–¹æ¡ˆ1ï¼šç®€æ´ç‰ˆï¼ˆæ¨èï¼‰

```bash
git commit -m "feat: æ–°å¢æ¨¡æ‹Ÿé¢è¯•è®¡åˆ’ç”ŸæˆåŠŸèƒ½

- é¦–æ¬¡è°ƒç”¨æ—¶è‡ªåŠ¨ç”Ÿæˆ10-12ä¸ªé—®é¢˜çš„é¢è¯•è®¡åˆ’
- è®¡åˆ’è¦†ç›–5ä¸ªè¯„ä¼°ç»´åº¦ï¼ˆä¸“ä¸šæŠ€èƒ½ã€èŒä¸šè§„åˆ’ã€è½¯ç¡¬èƒ½åŠ›ã€è–ªé…¬ã€æ€§æ ¼ï¼‰
- ç­–ç•¥å±‚å’Œæ‰§è¡Œå±‚å‚è€ƒé¢è¯•è®¡åˆ’è¿›è¡ŒåŠ¨æ€è°ƒæ•´
- æ·»åŠ è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—ä¾¿äºé—®é¢˜æ’æŸ¥å’Œè®¡åˆ’è¯„ä¼°"
```

---

### æ–¹æ¡ˆ2ï¼šè¯¦ç»†ç‰ˆ

```bash
git commit -m "feat: æ–°å¢æ¨¡æ‹Ÿé¢è¯•è®¡åˆ’ç”ŸæˆåŠŸèƒ½

åŠŸèƒ½å˜æ›´ï¼š
- æ–°å¢ generate_interview_plan() æ–¹æ³•ï¼Œåœ¨é¦–æ¬¡è°ƒç”¨æ—¶ç”Ÿæˆå®Œæ•´é¢è¯•è®¡åˆ’
- é¢è¯•è®¡åˆ’åŒ…å«10-12ä¸ªé—®é¢˜ï¼Œåˆ†ä¸ºç ´å†°ã€æ ¸å¿ƒæ¢ç´¢ã€æ”¶å°¾ä¸‰ä¸ªé˜¶æ®µ
- è®¡åˆ’è¦†ç›–å²—ä½ä¸“ä¸šæŠ€èƒ½ã€èŒä¸šè§„åˆ’ã€è½¯ç¡¬æ€§èƒ½åŠ›ã€è–ªé…¬é¢„æœŸã€æ€§æ ¼ç‰¹å¾ç­‰ç»´åº¦
- ç­–ç•¥å±‚å’Œæ‰§è¡Œå±‚åœ¨åç»­å¯¹è¯ä¸­å‚è€ƒé¢è¯•è®¡åˆ’è¿›è¡Œå†³ç­–

ä»£ç å˜æ›´ï¼š
- virtual_interview.py: æ–°å¢é¢è¯•è®¡åˆ’ç”Ÿæˆé€»è¾‘å’Œè¯¦ç»†æ—¥å¿—
- interview_process_router.py: æ·»åŠ è·¯ç”±å±‚è°ƒè¯•æ—¥å¿—
- generate_plan_white_collar.txt: æ–°å¢é¢è¯•è®¡åˆ’ç”Ÿæˆ prompt æ–‡ä»¶

æŠ€æœ¯ç»†èŠ‚ï¼š
- ä½¿ç”¨ LLM ç”Ÿæˆç»“æ„åŒ– JSON æ ¼å¼çš„é¢è¯•è®¡åˆ’
- ä¿®å¤ prompt æ–‡ä»¶ä¸­ JSON ç¤ºä¾‹çš„å¤§æ‹¬å·è½¬ä¹‰é—®é¢˜
- æ·»åŠ å®Œæ•´çš„è°ƒè¯•æ—¥å¿—è¾“å‡ºï¼Œä¾¿äºè¯„ä¼°ç”Ÿæˆçš„é—®é¢˜è´¨é‡"
```

---

### æ–¹æ¡ˆ3ï¼šæœ€ç®€æ´ç‰ˆ

```bash
git commit -m "feat: æ–°å¢é¢è¯•è®¡åˆ’è‡ªåŠ¨ç”ŸæˆåŠŸèƒ½

é¦–æ¬¡è°ƒç”¨æ—¶ç”Ÿæˆ10-12ä¸ªé—®é¢˜çš„é¢è¯•è®¡åˆ’ï¼Œè¦†ç›–5ä¸ªè¯„ä¼°ç»´åº¦ï¼Œç­–ç•¥å±‚å’Œæ‰§è¡Œå±‚å‚è€ƒè®¡åˆ’è¿›è¡ŒåŠ¨æ€è°ƒæ•´"
```

---

## æ¨èä½¿ç”¨æ–¹æ¡ˆ1

ç†ç”±ï¼š
- ç¬¦åˆå¸¸è§ commit è§„èŒƒï¼ˆfeat: å‰ç¼€ï¼‰
- ç®€æ´æ¸…æ™°ï¼ŒåŒ…å«å…³é”®ä¿¡æ¯
- ä¾¿äºåç»­æŸ¥çœ‹å†å²è®°å½•

---

## Commit æ¶ˆæ¯æ ¼å¼è¯´æ˜

```
<type>: <subject>

<body>

<footer>
```

- type: `feat`ï¼ˆæ–°åŠŸèƒ½ï¼‰ã€`fix`ï¼ˆä¿®å¤ï¼‰ã€`docs`ï¼ˆæ–‡æ¡£ï¼‰ã€`style`ï¼ˆæ ¼å¼ï¼‰ã€`refactor`ï¼ˆé‡æ„ï¼‰ã€`test`ï¼ˆæµ‹è¯•ï¼‰ã€`chore`ï¼ˆæ„å»º/å·¥å…·ï¼‰
- subject: ç®€çŸ­æè¿°ï¼ˆ50å­—ä»¥å†…ï¼‰
- body: è¯¦ç»†è¯´æ˜ï¼ˆå¯é€‰ï¼‰
- footer: ç›¸å…³ issueï¼ˆå¯é€‰ï¼‰

---

## æœ€ç»ˆæ¨èå‘½ä»¤

```bash
git add .
git commit -m "feat: æ–°å¢æ¨¡æ‹Ÿé¢è¯•è®¡åˆ’ç”ŸæˆåŠŸèƒ½

- é¦–æ¬¡è°ƒç”¨æ—¶è‡ªåŠ¨ç”Ÿæˆ10-12ä¸ªé—®é¢˜çš„é¢è¯•è®¡åˆ’
- è®¡åˆ’è¦†ç›–5ä¸ªè¯„ä¼°ç»´åº¦ï¼ˆä¸“ä¸šæŠ€èƒ½ã€èŒä¸šè§„åˆ’ã€è½¯ç¡¬èƒ½åŠ›ã€è–ªé…¬ã€æ€§æ ¼ï¼‰
- ç­–ç•¥å±‚å’Œæ‰§è¡Œå±‚å‚è€ƒé¢è¯•è®¡åˆ’è¿›è¡ŒåŠ¨æ€è°ƒæ•´
- æ·»åŠ è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—ä¾¿äºé—®é¢˜æ’æŸ¥å’Œè®¡åˆ’è¯„ä¼°"
```

å¯ä»¥ç›´æ¥ä½¿ç”¨è¿™ä¸ªå‘½ä»¤æäº¤ã€‚

```shellscript
git commit -m "feat: æ–°å¢æ¨¡æ‹Ÿé¢è¯•è®¡åˆ’ç”ŸæˆåŠŸèƒ½

- é¦–æ¬¡è°ƒç”¨æ—¶è‡ªåŠ¨ç”Ÿæˆ10-12ä¸ªé—®é¢˜çš„é¢è¯•è®¡åˆ’
- è®¡åˆ’è¦†ç›–5ä¸ªè¯„ä¼°ç»´åº¦ï¼ˆä¸“ä¸šæŠ€èƒ½ã€èŒä¸šè§„åˆ’ã€è½¯ç¡¬èƒ½åŠ›ã€è–ªé…¬ã€æ€§æ ¼ï¼‰
- ç­–ç•¥å±‚å’Œæ‰§è¡Œå±‚å‚è€ƒé¢è¯•è®¡åˆ’è¿›è¡ŒåŠ¨æ€è°ƒæ•´
- æ·»åŠ è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—ä¾¿äºé—®é¢˜æ’æŸ¥å’Œè®¡åˆ’è¯„ä¼°"
```

```shellscript
git commit -m "feat: æ–°å¢æ¨¡æ‹Ÿé¢è¯•è®¡åˆ’ç”ŸæˆåŠŸèƒ½

åŠŸèƒ½å˜æ›´ï¼š
- æ–°å¢ generate_interview_plan() æ–¹æ³•ï¼Œåœ¨é¦–æ¬¡è°ƒç”¨æ—¶ç”Ÿæˆå®Œæ•´é¢è¯•è®¡åˆ’
- é¢è¯•è®¡åˆ’åŒ…å«10-12ä¸ªé—®é¢˜ï¼Œåˆ†ä¸ºç ´å†°ã€æ ¸å¿ƒæ¢ç´¢ã€æ”¶å°¾ä¸‰ä¸ªé˜¶æ®µ
- è®¡åˆ’è¦†ç›–å²—ä½ä¸“ä¸šæŠ€èƒ½ã€èŒä¸šè§„åˆ’ã€è½¯ç¡¬æ€§èƒ½åŠ›ã€è–ªé…¬é¢„æœŸã€æ€§æ ¼ç‰¹å¾ç­‰ç»´åº¦
- ç­–ç•¥å±‚å’Œæ‰§è¡Œå±‚åœ¨åç»­å¯¹è¯ä¸­å‚è€ƒé¢è¯•è®¡åˆ’è¿›è¡Œå†³ç­–

ä»£ç å˜æ›´ï¼š
- virtual_interview.py: æ–°å¢é¢è¯•è®¡åˆ’ç”Ÿæˆé€»è¾‘å’Œè¯¦ç»†æ—¥å¿—
- interview_process_router.py: æ·»åŠ è·¯ç”±å±‚è°ƒè¯•æ—¥å¿—
- generate_plan_white_collar.txt: æ–°å¢é¢è¯•è®¡åˆ’ç”Ÿæˆ prompt æ–‡ä»¶

æŠ€æœ¯ç»†èŠ‚ï¼š
- ä½¿ç”¨ LLM ç”Ÿæˆç»“æ„åŒ– JSON æ ¼å¼çš„é¢è¯•è®¡åˆ’
- ä¿®å¤ prompt æ–‡ä»¶ä¸­ JSON ç¤ºä¾‹çš„å¤§æ‹¬å·è½¬ä¹‰é—®é¢˜
- æ·»åŠ å®Œæ•´çš„è°ƒè¯•æ—¥å¿—è¾“å‡ºï¼Œä¾¿äºè¯„ä¼°ç”Ÿæˆçš„é—®é¢˜è´¨é‡"
```

```shellscript
git commit -m "feat: æ–°å¢é¢è¯•è®¡åˆ’è‡ªåŠ¨ç”ŸæˆåŠŸèƒ½

é¦–æ¬¡è°ƒç”¨æ—¶ç”Ÿæˆ10-12ä¸ªé—®é¢˜çš„é¢è¯•è®¡åˆ’ï¼Œè¦†ç›–5ä¸ªè¯„ä¼°ç»´åº¦ï¼Œç­–ç•¥å±‚å’Œæ‰§è¡Œå±‚å‚è€ƒè®¡åˆ’è¿›è¡ŒåŠ¨æ€è°ƒæ•´"
```

```plaintext
<type>: <subject>

<body>

<footer>
```

```shellscript
git add .
git commit -m "feat: æ–°å¢æ¨¡æ‹Ÿé¢è¯•è®¡åˆ’ç”ŸæˆåŠŸèƒ½

- é¦–æ¬¡è°ƒç”¨æ—¶è‡ªåŠ¨ç”Ÿæˆ10-12ä¸ªé—®é¢˜çš„é¢è¯•è®¡åˆ’
- è®¡åˆ’è¦†ç›–5ä¸ªè¯„ä¼°ç»´åº¦ï¼ˆä¸“ä¸šæŠ€èƒ½ã€èŒä¸šè§„åˆ’ã€è½¯ç¡¬èƒ½åŠ›ã€è–ªé…¬ã€æ€§æ ¼ï¼‰
- ç­–ç•¥å±‚å’Œæ‰§è¡Œå±‚å‚è€ƒé¢è¯•è®¡åˆ’è¿›è¡ŒåŠ¨æ€è°ƒæ•´
- æ·»åŠ è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—ä¾¿äºé—®é¢˜æ’æŸ¥å’Œè®¡åˆ’è¯„ä¼°"
```

---

**User**

@bash (950-959) æˆ‘åœ¨å¾€è¿œç¨‹æ¨çš„æ—¶å€™å¥½åƒä¸è¡Œå•Šè¿™æ˜¯ä¸ºä»€ä¹ˆå‘¢ï¼Ÿæ˜¯ä¸æ˜¯è¿œç¨‹çš„åˆ†æ”¯æœ‰äº†æ–°çš„ä¿®æ”¹ï¼Ÿ

---

**Cursor**

è¿™æ˜¯å¸¸è§çš„ Git æ¨é€å†²çªé—®é¢˜ã€‚åŸå› å’Œè§£å†³æ–¹æ³•å¦‚ä¸‹ï¼š

## é—®é¢˜åŸå› 

é”™è¯¯ä¿¡æ¯æ˜¾ç¤ºï¼š
```
Updates were rejected because the remote contains work that you do not have locally
```

å«ä¹‰ï¼š
- è¿œç¨‹åˆ†æ”¯ `feature/puyuyang_mock-interview` æœ‰æ–°çš„æäº¤
- æœ¬åœ°åˆ†æ”¯ä¹Ÿæœ‰æ–°çš„æäº¤
- ä¸¤ä¸ªåˆ†æ”¯å†å²åˆ†å‰ï¼ŒGit æ— æ³•ç›´æ¥æ¨é€

å¯èƒ½åŸå› ï¼š
1. å…¶ä»–äººå‘è¯¥åˆ†æ”¯æ¨é€äº†æ–°ä»£ç 
2. ä½ åœ¨å…¶ä»–ä½ç½®ï¼ˆå¦‚å¦ä¸€å°ç”µè„‘ï¼‰æ¨é€è¿‡ä»£ç 
3. é€šè¿‡ Web ç•Œé¢åˆå¹¶äº†å…¶ä»– PR

---

## è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ1ï¼šå…ˆæ‹‰å–å†æ¨é€ï¼ˆæ¨èï¼‰

```bash
# 1. å…ˆæ‹‰å–è¿œç¨‹çš„æœ€æ–°ä»£ç 
git pull origin feature/puyuyang_mock-interview

# 2. å¦‚æœæœ‰å†²çªï¼Œè§£å†³å†²çªåï¼š
git add .
git commit -m "merge: åˆå¹¶è¿œç¨‹åˆ†æ”¯æ›´æ”¹"

# 3. å†æ¨é€
git push origin feature/puyuyang_mock-interview
```

### æ–¹æ¡ˆ2ï¼šä½¿ç”¨ rebaseï¼ˆä¿æŒæäº¤å†å²æ›´æ¸…æ™°ï¼‰

```bash
# 1. æ‹‰å–å¹¶ rebase
git pull --rebase origin feature/puyuyang_mock-interview

# 2. å¦‚æœæœ‰å†²çªï¼Œè§£å†³å†²çªåï¼š
git add .
git rebase --continue

# 3. å†æ¨é€
git push origin feature/puyuyang_mock-interview
```

### æ–¹æ¡ˆ3ï¼šæŸ¥çœ‹è¿œç¨‹æ›´æ”¹åå†å†³å®š

```bash
# 1. å…ˆæŸ¥çœ‹è¿œç¨‹æœ‰ä»€ä¹ˆæ–°æäº¤
git fetch origin
git log HEAD..origin/feature/puyuyang_mock-interview

# 2. æŸ¥çœ‹æœ¬åœ°æœ‰ä»€ä¹ˆæ–°æäº¤
git log origin/feature/puyuyang_mock-interview..HEAD

# 3. å¦‚æœè¿œç¨‹çš„æ›´æ”¹ä¸é‡è¦ï¼Œå¯ä»¥å¼ºåˆ¶æ¨é€ï¼ˆè°¨æ…ä½¿ç”¨ï¼ï¼‰
# git push --force origin feature/puyuyang_mock-interview
```

---

## æ¨èæ“ä½œæ­¥éª¤

### æ­¥éª¤1ï¼šæŸ¥çœ‹çŠ¶æ€
```bash
# æŸ¥çœ‹å½“å‰åˆ†æ”¯çŠ¶æ€
git status

# æŸ¥çœ‹æœ¬åœ°å’Œè¿œç¨‹çš„å·®å¼‚
git log --oneline --graph --all
```

### æ­¥éª¤2ï¼šæ‹‰å–è¿œç¨‹æ›´æ”¹
```bash
git pull origin feature/puyuyang_mock-interview
```

### æ­¥éª¤3ï¼šå¤„ç†å†²çªï¼ˆå¦‚æœæœ‰ï¼‰

å¦‚æœå‡ºç°å†²çªï¼ŒGit ä¼šæç¤ºï¼š
```
Auto-merging <æ–‡ä»¶å>
CONFLICT (content): Merge conflict in <æ–‡ä»¶å>
```

è§£å†³æ­¥éª¤ï¼š
1. æ‰“å¼€å†²çªæ–‡ä»¶ï¼Œæ‰¾åˆ°å†²çªæ ‡è®°ï¼š
   ```
   <<<<<<< HEAD
   ä½ çš„ä»£ç 
   =======
   è¿œç¨‹çš„ä»£ç 
   >>>>>>> origin/feature/puyuyang_mock-interview
   ```
2. æ‰‹åŠ¨è§£å†³å†²çªï¼Œä¿ç•™éœ€è¦çš„ä»£ç 
3. åˆ é™¤å†²çªæ ‡è®°ï¼ˆ`<<<<<<<`, `=======`, `>>>>>>>`ï¼‰
4. ä¿å­˜æ–‡ä»¶

### æ­¥éª¤4ï¼šå®Œæˆåˆå¹¶
```bash
# æ·»åŠ è§£å†³åçš„æ–‡ä»¶
git add .

# å®Œæˆåˆå¹¶æäº¤
git commit -m "merge: åˆå¹¶è¿œç¨‹åˆ†æ”¯æ›´æ”¹"

# æ¨é€
git push origin feature/puyuyang_mock-interview
```

---

## æ³¨æ„äº‹é¡¹

### ä¸è¦ä½¿ç”¨ `--force`ï¼ˆé™¤éç¡®å®šï¼‰

```bash
# âŒ å±é™©æ“ä½œï¼ä¼šè¦†ç›–è¿œç¨‹çš„æ›´æ”¹
git push --force origin feature/puyuyang_mock-interview
```

ä»…åœ¨ä»¥ä¸‹æƒ…å†µä½¿ç”¨ï¼š
- ç¡®å®šè¿œç¨‹çš„æ›´æ”¹ä¸é‡è¦
- ç¡®å®šæ²¡æœ‰å…¶ä»–äººä¾èµ–è¿œç¨‹åˆ†æ”¯
- ä½ å®Œå…¨ç†è§£åæœ

---

## å¿«é€Ÿæ£€æŸ¥å‘½ä»¤

æ‰§è¡Œä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹æƒ…å†µï¼š

```bash
# æŸ¥çœ‹è¿œç¨‹åˆ†æ”¯çš„æœ€æ–°æäº¤
git fetch origin
git log origin/feature/puyuyang_mock-interview --oneline -5

# æŸ¥çœ‹æœ¬åœ°åˆ†æ”¯çš„æœ€æ–°æäº¤
git log --oneline -5

# æŸ¥çœ‹ä¸¤ä¸ªåˆ†æ”¯çš„å·®å¼‚
git diff origin/feature/puyuyang_mock-interview HEAD
```

---

## æ€»ç»“

- åŸå› ï¼šè¿œç¨‹åˆ†æ”¯æœ‰æ–°çš„æäº¤ï¼Œæœ¬åœ°ä¹Ÿæœ‰æ–°æäº¤ï¼Œå†å²åˆ†å‰
- è§£å†³ï¼šå…ˆ `git pull` æ‹‰å–è¿œç¨‹æ›´æ”¹ï¼Œè§£å†³å†²çªåå†æ¨é€
- æ¨èï¼šä½¿ç”¨ `git pull origin feature/puyuyang_mock-interview`ï¼Œç„¶åè§£å†³å†²çªå¹¶æ¨é€

å…ˆæ‰§è¡Œ `git pull origin feature/puyuyang_mock-interview`ï¼Œçœ‹çœ‹æ˜¯å¦æœ‰å†²çªã€‚å¦‚æœæœ‰å†²çªï¼Œå‘Šè¯‰æˆ‘å…·ä½“æ–‡ä»¶ï¼Œæˆ‘å¯ä»¥å¸®ä½ è§£å†³ã€‚

```plaintext
Updates were rejected because the remote contains work that you do not have locally
```

```shellscript
# 1. å…ˆæ‹‰å–è¿œç¨‹çš„æœ€æ–°ä»£ç 
git pull origin feature/puyuyang_mock-interview

# 2. å¦‚æœæœ‰å†²çªï¼Œè§£å†³å†²çªåï¼š
git add .
git commit -m "merge: åˆå¹¶è¿œç¨‹åˆ†æ”¯æ›´æ”¹"

# 3. å†æ¨é€
git push origin feature/puyuyang_mock-interview
```

```shellscript
# 1. æ‹‰å–å¹¶ rebase
git pull --rebase origin feature/puyuyang_mock-interview

# 2. å¦‚æœæœ‰å†²çªï¼Œè§£å†³å†²çªåï¼š
git add .
git rebase --continue

# 3. å†æ¨é€
git push origin feature/puyuyang_mock-interview
```

```shellscript
# 1. å…ˆæŸ¥çœ‹è¿œç¨‹æœ‰ä»€ä¹ˆæ–°æäº¤
git fetch origin
git log HEAD..origin/feature/puyuyang_mock-interview

# 2. æŸ¥çœ‹æœ¬åœ°æœ‰ä»€ä¹ˆæ–°æäº¤
git log origin/feature/puyuyang_mock-interview..HEAD

# 3. å¦‚æœè¿œç¨‹çš„æ›´æ”¹ä¸é‡è¦ï¼Œå¯ä»¥å¼ºåˆ¶æ¨é€ï¼ˆè°¨æ…ä½¿ç”¨ï¼ï¼‰
# git push --force origin feature/puyuyang_mock-interview
```

```shellscript
# æŸ¥çœ‹å½“å‰åˆ†æ”¯çŠ¶æ€
git status

# æŸ¥çœ‹æœ¬åœ°å’Œè¿œç¨‹çš„å·®å¼‚
git log --oneline --graph --all
```

```shellscript
git pull origin feature/puyuyang_mock-interview
```

```plaintext
Auto-merging <æ–‡ä»¶å>
CONFLICT (content): Merge conflict in <æ–‡ä»¶å>
```

```plaintext
   <<<<<<< HEAD
   ä½ çš„ä»£ç 
   =======
   è¿œç¨‹çš„ä»£ç 
   >>>>>>> origin/feature/puyuyang_mock-interview
```

```shellscript
# æ·»åŠ è§£å†³åçš„æ–‡ä»¶
git add .

# å®Œæˆåˆå¹¶æäº¤
git commit -m "merge: åˆå¹¶è¿œç¨‹åˆ†æ”¯æ›´æ”¹"

# æ¨é€
git push origin feature/puyuyang_mock-interview
```

```shellscript
# âŒ å±é™©æ“ä½œï¼ä¼šè¦†ç›–è¿œç¨‹çš„æ›´æ”¹
git push --force origin feature/puyuyang_mock-interview
```

```shellscript
# æŸ¥çœ‹è¿œç¨‹åˆ†æ”¯çš„æœ€æ–°æäº¤
git fetch origin
git log origin/feature/puyuyang_mock-interview --oneline -5

# æŸ¥çœ‹æœ¬åœ°åˆ†æ”¯çš„æœ€æ–°æäº¤
git log --oneline -5

# æŸ¥çœ‹ä¸¤ä¸ªåˆ†æ”¯çš„å·®å¼‚
git diff origin/feature/puyuyang_mock-interview HEAD
```

---

**User**

@project_progress.md è¿™ä¸ªæ—¶å€™çš„é¡¹ç›®è¿›åº¦å¯¹æ­¤èƒ½ä¸èƒ½å¸®æˆ‘è¿›è¡Œä¸€äº›ä¿®æ”¹å’Œæ¶¦è‰²å‘¢ï¼Ÿæ„Ÿè§‰é‡Œé¢çš„ä¸œè¥¿æ˜¯æœ‰ç‚¹é‡å¤ï¼Œç„¶åå¸®æˆ‘ä¿®æ”¹ä¸€ä¸‹å¯ä»¥å—ï¼Ÿå› ä¸ºè¿™æ˜¯è¦ç»™æŠ¥å‘Šç»™è€æ¿çœ‹çš„ï¼Œè€æ¿å¿ƒé‡Œé¢ä¼šæœ‰åº•æ­¢çš„é¡¹ç›®è¿›åº¦æœ‰ä»€ä¹ˆå˜åŒ– 

---

**Cursor**

å®¡é˜…å¹¶ä¼˜åŒ–é¡¹ç›®è¿›åº¦æ–‡æ¡£ï¼Œå»é™¤é‡å¤ï¼Œä½¿å…¶æ›´ä¸“ä¸šã€æ¸…æ™°ã€‚


ä¼˜åŒ–é¡¹ç›®è¿›åº¦æ–‡æ¡£ï¼Œå»é™¤é‡å¤ï¼Œä½¿å…¶æ›´ä¸“ä¸šã€æ¸…æ™°ã€‚


```markdown
# é¡¹ç›®è¿›åº¦æŠ¥å‘Š

## ä¸€ã€ä»£ç æ¶æ„ä¼˜åŒ–

### 1.1 æç¤ºè¯ç®¡ç†ä¼˜åŒ–
**ç›®æ ‡**ï¼šé™ä½ä»£ç è€¦åˆæ€§ï¼Œæå‡å¯ç»´æŠ¤æ€§

**å®ç°**ï¼š
- å°†åŸæœ¬ç¡¬ç¼–ç åœ¨ä»£ç ä¸­çš„æ¨¡å‹æç¤ºè¯æŠ½ç¦»åˆ°ç‹¬ç«‹çš„ TXT æ–‡ä»¶
- å»ºç«‹ç»Ÿä¸€çš„æç¤ºè¯ç®¡ç†ç›®å½•ç»“æ„
- ä¾¿äºåç»­æç¤ºè¯ä¼˜åŒ–å’Œç‰ˆæœ¬ç®¡ç†

**ä»·å€¼**ï¼š
- ä»£ç ä¸ä¸šåŠ¡é€»è¾‘è§£è€¦ï¼Œæå‡ä»£ç å¯è¯»æ€§
- éæŠ€æœ¯äººå‘˜å¯ç›´æ¥ä¿®æ”¹æç¤ºè¯ï¼Œæ— éœ€æ”¹åŠ¨ä»£ç 
- ä¾¿äº A/B æµ‹è¯•å’Œæç¤ºè¯æ•ˆæœå¯¹æ¯”

---

## äºŒã€æ¨¡æ‹Ÿé¢è¯•è®¡åˆ’ç”ŸæˆåŠŸèƒ½

### 2.1 åŠŸèƒ½æ¦‚è¿°
å®ç°äº†æ™ºèƒ½é¢è¯•è®¡åˆ’è‡ªåŠ¨ç”ŸæˆåŠŸèƒ½ï¼Œåœ¨é¢è¯•å¼€å§‹æ—¶æ ¹æ®å€™é€‰äººèƒŒæ™¯å’Œå²—ä½è¦æ±‚ï¼Œè‡ªåŠ¨ç”Ÿæˆç»“æ„åŒ–çš„é¢è¯•é—®é¢˜è®¡åˆ’ã€‚

### 2.2 æ ¸å¿ƒåŠŸèƒ½ç‰¹æ€§

**æ™ºèƒ½è®¡åˆ’ç”Ÿæˆ**ï¼š
- é¦–æ¬¡è°ƒç”¨æ—¶ï¼ˆ`dialog_history` ä¸ºç©ºï¼‰è‡ªåŠ¨ç”Ÿæˆ 10-12 ä¸ªé¢è¯•é—®é¢˜
- é—®é¢˜æ•°é‡æ ¹æ®å²—ä½å’Œå€™é€‰äººæƒ…å†µåŠ¨æ€è°ƒæ•´
- ç”Ÿæˆçš„é—®é¢˜ä»¥ JSON æ ¼å¼ç»“æ„åŒ–å­˜å‚¨

**å¤šç»´åº¦è¦†ç›–**ï¼š
- **å²—ä½ä¸“ä¸šæŠ€èƒ½è€ƒæ ¸**ï¼ˆ3-4ä¸ªé—®é¢˜ï¼‰ï¼šè¯„ä¼°æŠ€æœ¯èƒ½åŠ›å’Œé¡¹ç›®ç»éªŒ
- **èŒä¸šè§„åˆ’æ¢è®¨**ï¼ˆ2ä¸ªé—®é¢˜ï¼‰ï¼šäº†è§£èŒä¸šåŠ¨æœºå’Œå‘å±•æ–¹å‘
- **è½¯ç¡¬æ€§èƒ½åŠ›è¯„ä¼°**ï¼ˆ2-3ä¸ªé—®é¢˜ï¼‰ï¼šç»¼åˆè¯„ä¼°é—®é¢˜è§£å†³å’Œå›¢é˜Ÿåä½œèƒ½åŠ›
- **è–ªé…¬é¢„æœŸåˆç†æ€§åˆ¤æ–­**ï¼ˆ1ä¸ªé—®é¢˜ï¼‰ï¼šè¯„ä¼°è‡ªæˆ‘è®¤çŸ¥å’Œå¸‚åœºåŒ¹é…åº¦
- **æ€§æ ¼ç‰¹å¾åˆ†æ**ï¼šé€šè¿‡å›ç­”æ–¹å¼é—´æ¥è§‚å¯Ÿï¼Œä¸å•ç‹¬æé—®

**åˆ†é˜¶æ®µè®¾è®¡**ï¼š
- **ç ´å†°é˜¶æ®µ**ï¼ˆ2-3ä¸ªé—®é¢˜ï¼‰ï¼šè½»æ¾å¼€åœºï¼Œå»ºç«‹ä¿¡ä»»
- **æ ¸å¿ƒæ¢ç´¢é˜¶æ®µ**ï¼ˆ6-8ä¸ªé—®é¢˜ï¼‰ï¼šæ·±å…¥æŒ–æ˜èƒ½åŠ›å’Œæ½œåŠ›
- **æ”¶å°¾é˜¶æ®µ**ï¼ˆ2ä¸ªé—®é¢˜ï¼‰ï¼šç¡®è®¤æœŸæœ›ï¼Œç»“æŸé¢è¯•

### 2.3 æŠ€æœ¯å®ç°

**ä¸šåŠ¡æµç¨‹**ï¼š
```
é¦–æ¬¡è°ƒç”¨ â†’ ç”Ÿæˆé¢è¯•è®¡åˆ’ â†’ è¿”å›ç¬¬ä¸€ä¸ªé—®é¢˜
         â†“
åç»­è°ƒç”¨ â†’ ç­–ç•¥å±‚å‚è€ƒè®¡åˆ’ â†’ æ‰§è¡Œå±‚ç”Ÿæˆå›å¤ â†’ æµç¨‹åˆ¤æ–­
```

**æ ¸å¿ƒæœºåˆ¶**ï¼š
- ç­–ç•¥å±‚ï¼ˆ`strategy_white_collar.txt`ï¼‰åœ¨æ‰§è¡Œå†³ç­–æ—¶å‚è€ƒé¢è¯•è®¡åˆ’
- æ‰§è¡Œå±‚ï¼ˆ`executor_white_collar.txt`ï¼‰åŸºäºç­–ç•¥å’Œè®¡åˆ’ç”Ÿæˆå…·ä½“å›å¤
- ä¿æŒåŸæœ‰åŠ¨æ€è°ƒæ•´èƒ½åŠ›ï¼Œè®¡åˆ’ä½œä¸ºå‚è€ƒè€Œéç¡¬æ€§çº¦æŸ

### 2.4 ä»£ç å˜æ›´

**æ–°å¢æ–‡ä»¶**ï¼š
- `prompts/interview/generate_plan_white_collar.txt`ï¼šé¢è¯•è®¡åˆ’ç”Ÿæˆæç¤ºè¯æ¨¡æ¿

**ä¿®æ”¹æ–‡ä»¶**ï¼š
- `service/virtual_interview.py`ï¼š
  - æ–°å¢ `generate_interview_plan()` æ–¹æ³•
  - æ–°å¢ `interview_plan` å’Œ `current_question_index` å±æ€§
  - ä¼˜åŒ– `get_ai_response()` æ–¹æ³•ï¼Œæ”¯æŒé¦–æ¬¡è°ƒç”¨åˆ¤æ–­å’Œè®¡åˆ’å‚è€ƒ
  - æ·»åŠ å®Œæ•´çš„è°ƒè¯•æ—¥å¿—ç³»ç»Ÿ

- `routers/interview_process_router.py`ï¼š
  - æ·»åŠ è·¯ç”±å±‚è°ƒè¯•æ—¥å¿—ï¼Œä¾¿äºé—®é¢˜æ’æŸ¥

### 2.5 æŠ€æœ¯ç»†èŠ‚

- **LLM è°ƒç”¨**ï¼šä½¿ç”¨ä¸»æ¨¡å‹ç”Ÿæˆç»“æ„åŒ– JSON æ ¼å¼çš„é¢è¯•è®¡åˆ’
- **é”™è¯¯å¤„ç†**ï¼šå®Œå–„çš„å¼‚å¸¸æ•è·å’Œæ—¥å¿—è®°å½•æœºåˆ¶
- **æ ¼å¼å¤„ç†**ï¼šè‡ªåŠ¨æ¸…ç† LLM è¿”å›çš„ markdown æ ‡è®°ï¼Œç¡®ä¿ JSON è§£ææˆåŠŸ
- **è°ƒè¯•æ”¯æŒ**ï¼šè¯¦ç»†çš„æ—¥å¿—è¾“å‡ºï¼Œå¯å®Œæ•´æŸ¥çœ‹ç”Ÿæˆçš„é¢è¯•è®¡åˆ’å†…å®¹ï¼Œä¾¿äºè¯„ä¼°é—®é¢˜è´¨é‡

---

## ä¸‰ã€é¡¹ç›®ä»·å€¼

### 3.1 ä¸šåŠ¡ä»·å€¼
- **æå‡é¢è¯•è´¨é‡**ï¼šç»“æ„åŒ–çš„é—®é¢˜è®¾è®¡ç¡®ä¿å…¨é¢è¯„ä¼°å€™é€‰äºº
- **æé«˜æ•ˆç‡**ï¼šè‡ªåŠ¨ç”Ÿæˆè®¡åˆ’ï¼Œå‡å°‘äººå·¥å‡†å¤‡æ—¶é—´
- **æ ‡å‡†åŒ–æµç¨‹**ï¼šç»Ÿä¸€çš„é¢è¯•ç»´åº¦ï¼Œä¾¿äºæ¨ªå‘å¯¹æ¯”å’Œè¯„ä¼°

### 3.2 æŠ€æœ¯ä»·å€¼
- **ä»£ç è´¨é‡æå‡**ï¼šé™ä½è€¦åˆæ€§ï¼Œæå‡å¯ç»´æŠ¤æ€§
- **å¯æ‰©å±•æ€§å¢å¼º**ï¼šæç¤ºè¯ç‹¬ç«‹ç®¡ç†ï¼Œä¾¿äºåç»­åŠŸèƒ½æ‰©å±•
- **å¯è§‚æµ‹æ€§æ”¹å–„**ï¼šå®Œå–„çš„æ—¥å¿—ç³»ç»Ÿï¼Œä¾¿äºé—®é¢˜æ’æŸ¥å’Œæ•ˆæœè¯„ä¼°

---

## å››ã€ä¸‹ä¸€æ­¥è®¡åˆ’

1. **æ•ˆæœè¯„ä¼°**ï¼šæ”¶é›†ç”Ÿæˆçš„é¢è¯•è®¡åˆ’æ ·æœ¬ï¼Œè¯„ä¼°é—®é¢˜è´¨é‡å’Œåˆç†æ€§
2. **ä¼˜åŒ–è¿­ä»£**ï¼šæ ¹æ®è¯„ä¼°ç»“æœä¼˜åŒ–æç¤ºè¯ï¼Œæå‡ç”Ÿæˆè´¨é‡
3. **åŠŸèƒ½æ‰©å±•**ï¼šè€ƒè™‘æ”¯æŒæ›´å¤šå²—ä½ç±»å‹å’Œé¢è¯•åœºæ™¯


```

å·²ä¼˜åŒ–é¡¹ç›®è¿›åº¦æ–‡æ¡£ï¼Œä¸»è¦æ”¹è¿›å¦‚ä¸‹ï¼š

## ä¸»è¦æ”¹è¿›

### 1. ç»“æ„ä¼˜åŒ–
- åˆ†ä¸ºå››ä¸ªéƒ¨åˆ†ï¼šä»£ç æ¶æ„ä¼˜åŒ–ã€åŠŸèƒ½å®ç°ã€é¡¹ç›®ä»·å€¼ã€ä¸‹ä¸€æ­¥è®¡åˆ’
- ä½¿ç”¨æ¸…æ™°çš„æ ‡é¢˜å±‚çº§ï¼Œä¾¿äºå¿«é€Ÿæµè§ˆ

### 2. å»é™¤é‡å¤
- æ•´åˆäº†é‡å¤çš„â€œç”Ÿæˆé¢è¯•è®¡åˆ’â€å†…å®¹
- ç»Ÿä¸€æè¿°ï¼Œé¿å…ä¿¡æ¯å†—ä½™

### 3. ä¸“ä¸šè¡¨è¾¾
- ä½¿ç”¨æ›´ä¸“ä¸šçš„æœ¯è¯­å’Œè¡¨è¾¾
- çªå‡ºä¸šåŠ¡ä»·å€¼å’ŒæŠ€æœ¯ä»·å€¼
- å¢åŠ â€œé¡¹ç›®ä»·å€¼â€éƒ¨åˆ†ï¼Œè¯´æ˜å®é™…æ”¶ç›Š

### 4. ä¾¿äºç†è§£
- æ¯ä¸ªéƒ¨åˆ†éƒ½æœ‰æ˜ç¡®çš„â€œç›®æ ‡-å®ç°-ä»·å€¼â€ç»“æ„
- ä½¿ç”¨æµç¨‹å›¾å’Œåˆ—è¡¨ï¼Œæ¸…æ™°å±•ç¤ºæŠ€æœ¯å®ç°
- çªå‡ºå…³é”®ä¿¡æ¯ï¼Œä¾¿äºå¿«é€ŸæŠŠæ¡é‡ç‚¹

### 5. çªå‡ºæˆæœ
- æ˜ç¡®è¯´æ˜å·²å®Œæˆçš„åŠŸèƒ½
- å±•ç¤ºæŠ€æœ¯æ”¹è¿›å’Œä¸šåŠ¡ä»·å€¼
- æä¾›ä¸‹ä¸€æ­¥è®¡åˆ’ï¼Œä½“ç°æŒç»­ä¼˜åŒ–

---

## æ–‡æ¡£ç»“æ„

1. ä»£ç æ¶æ„ä¼˜åŒ–ï¼šè¯´æ˜ä»£ç é‡æ„çš„ä»·å€¼
2. æ¨¡æ‹Ÿé¢è¯•è®¡åˆ’ç”ŸæˆåŠŸèƒ½ï¼šè¯¦ç»†æè¿°æ–°åŠŸèƒ½
3. é¡¹ç›®ä»·å€¼ï¼šä¸šåŠ¡ä»·å€¼å’ŒæŠ€æœ¯ä»·å€¼
4. ä¸‹ä¸€æ­¥è®¡åˆ’ï¼šåç»­ä¼˜åŒ–æ–¹å‘

æ–‡æ¡£å·²æ›´ä¸“ä¸šã€æ¸…æ™°ï¼Œä¾¿äºå‘è€æ¿æ±‡æŠ¥é¡¹ç›®è¿›åº¦ã€‚å¦‚éœ€è°ƒæ•´ï¼Œå‘Šè¯‰æˆ‘å³å¯ã€‚

---

**User**

ä½ æ˜¯å•¥äº†å§ï¼Ÿä¸éœ€è¦è¯¦ç»†çš„æŠ¥å‘Šå‘€ï¼Œå°±ä»…æ˜¯è¿›åº¦çš„ä¸€ä¸ªè¯´æ˜å°±è¡Œ

---

**Cursor**

ä½ è¯´å¾—å¯¹ï¼Œæˆ‘ç†è§£äº†ã€‚è®©æˆ‘ç®€åŒ–æˆç®€æ´çš„è¿›åº¦è¯´æ˜ã€‚

```markdown
# é¡¹ç›®è¿›åº¦

## å·²å®Œæˆ

### 1. ä»£ç æ¶æ„ä¼˜åŒ–
- æç¤ºè¯ä»ç¡¬ç¼–ç æŠ½ç¦»åˆ°ç‹¬ç«‹çš„ TXT æ–‡ä»¶
- é™ä½ä»£ç è€¦åˆæ€§ï¼Œæå‡å¯ç»´æŠ¤æ€§

### 2. æ¨¡æ‹Ÿé¢è¯•è®¡åˆ’ç”ŸæˆåŠŸèƒ½
- é¦–æ¬¡è°ƒç”¨æ—¶è‡ªåŠ¨ç”Ÿæˆ 10-12 ä¸ªé¢è¯•é—®é¢˜
- é—®é¢˜è¦†ç›– 5 ä¸ªç»´åº¦ï¼šå²—ä½ä¸“ä¸šæŠ€èƒ½ã€èŒä¸šè§„åˆ’ã€è½¯ç¡¬æ€§èƒ½åŠ›ã€è–ªé…¬é¢„æœŸã€æ€§æ ¼ç‰¹å¾
- åˆ†ä¸º 3 ä¸ªé˜¶æ®µï¼šç ´å†°é˜¶æ®µã€æ ¸å¿ƒæ¢ç´¢é˜¶æ®µã€æ”¶å°¾é˜¶æ®µ
- ç­–ç•¥å±‚å’Œæ‰§è¡Œå±‚å‚è€ƒé¢è¯•è®¡åˆ’è¿›è¡Œå†³ç­–

### 3. ä»£ç æ”¹åŠ¨
- `virtual_interview.py`ï¼šæ–°å¢é¢è¯•è®¡åˆ’ç”Ÿæˆæ–¹æ³•å’Œæ—¥å¿—
- `interview_process_router.py`ï¼šæ·»åŠ è°ƒè¯•æ—¥å¿—
- `generate_plan_white_collar.txt`ï¼šæ–°å¢é¢è¯•è®¡åˆ’ç”Ÿæˆ prompt æ–‡ä»¶

## ä¸‹ä¸€æ­¥
- æ•ˆæœè¯„ä¼°å’Œä¼˜åŒ–è¿­ä»£
- æ”¯æŒæ›´å¤šå²—ä½ç±»å‹å’Œåœºæ™¯

```

å¥½äº†ï¼Œå·²ç®€åŒ–ä¸ºç®€æ´çš„è¿›åº¦è¯´æ˜ã€‚åªä¿ç•™äº†å…³é”®ä¿¡æ¯ï¼Œä¾¿äºå¿«é€Ÿäº†è§£é¡¹ç›®è¿›åº¦ã€‚

---

**User**

ä½ æ˜¯å‚»ä¸æ‹‰å‡ çš„æ€ä¹ˆä¿®æ”¹çš„ï¼Ÿå¯¹ï¼Œæ‡’å¾—éª‚ä½ äº†ï¼Œæˆ‘å°±è®©ä½ æ¶¦è‰²ä¸€ä¸‹å‘€ã€‚

---

**Cursor**

ä½ è¯´å¾—å¯¹ï¼Œæˆ‘ç†è§£äº†ã€‚è®©æˆ‘é‡æ–°æ¶¦è‰²è¿™ä¸ªæ–‡æ¡£ï¼Œä¿æŒåŸæœ‰çš„å†…å®¹ç»“æ„ï¼Œåªæ˜¯æ”¹è¿›è¡¨è¾¾å’Œæ ¼å¼ã€‚

```markdown
## ä»£ç ç»“æ„ä¼˜åŒ–

å°†æ¨¡å‹æç¤ºè¯ä»ç¡¬ç¼–ç æŠ½ç¦»åˆ°ç‹¬ç«‹çš„ TXT æ–‡ä»¶ï¼Œé™ä½ä»£ç è€¦åˆæ€§ï¼Œæå‡å¯ç»´æŠ¤æ€§ã€‚

---

## æ¨¡æ‹Ÿé¢è¯•è®¡åˆ’ç”ŸæˆåŠŸèƒ½

### åŠŸèƒ½è¦æ±‚

- ç”Ÿæˆ 10-12 ä¸ªé¢è¯•é—®é¢˜
- è¦†ç›– 5 ä¸ªç»´åº¦ï¼šå²—ä½ä¸“ä¸šæŠ€èƒ½ã€æ€§æ ¼ç‰¹å¾ã€èŒä¸šè§„åˆ’ã€è½¯ç¡¬æ€§èƒ½åŠ›ã€è–ªé…¬é¢„æœŸ
- åˆ†ä¸º 3 ä¸ªé˜¶æ®µï¼šç ´å†°é˜¶æ®µã€æ ¸å¿ƒæ¢ç´¢é˜¶æ®µã€æ”¶å°¾é˜¶æ®µ
- é¢è¯•è®¡åˆ’æç¤ºè¯å­˜æ”¾åœ¨ `generate_plan_white_collar.txt`

### ä¸šåŠ¡æµç¨‹

- åˆ¤æ–­æ˜¯å¦é¦–æ¬¡è°ƒç”¨ï¼ˆ`dialog_history` ä¸ºç©ºï¼‰
- ç”Ÿæˆé¢è¯•è®¡åˆ’ â†’ ç­–ç•¥å±‚å‚è€ƒ â†’ æ‰§è¡Œå±‚ç”Ÿæˆå›å¤ â†’ åˆ¤æ–­å¯¹è¯æ˜¯å¦ç»“æŸ
- ç­–ç•¥å±‚åŠ å…¥é¢è¯•è®¡åˆ’å…·ä½“ä¿¡æ¯ï¼Œä½œä¸ºå†³ç­–å‚è€ƒ

### ä»£ç å®ç°

**æ–°å¢æ–¹æ³•**ï¼š
- `generate_interview_plan()` - é¦–æ¬¡è°ƒç”¨æ—¶ç”Ÿæˆå®Œæ•´é¢è¯•è®¡åˆ’

**ä¿®æ”¹æ–‡ä»¶**ï¼š
- `virtual_interview.py` - æ–°å¢é¢è¯•è®¡åˆ’ç”Ÿæˆé€»è¾‘å’Œè¯¦ç»†æ—¥å¿—
- `interview_process_router.py` - æ·»åŠ è·¯ç”±å±‚è°ƒè¯•æ—¥å¿—
- `generate_plan_white_collar.txt` - æ–°å¢é¢è¯•è®¡åˆ’ç”Ÿæˆ prompt æ–‡ä»¶

**æŠ€æœ¯ç»†èŠ‚**ï¼š
- ä½¿ç”¨ LLM ç”Ÿæˆç»“æ„åŒ– JSON æ ¼å¼çš„é¢è¯•è®¡åˆ’
- ä¿®å¤ prompt æ–‡ä»¶ä¸­ JSON ç¤ºä¾‹çš„å¤§æ‹¬å·è½¬ä¹‰é—®é¢˜
- æ·»åŠ å®Œæ•´çš„è°ƒè¯•æ—¥å¿—è¾“å‡ºï¼Œä¾¿äºè¯„ä¼°ç”Ÿæˆçš„é—®é¢˜è´¨é‡

```

å¥½äº†ï¼Œå·²æ¶¦è‰²ã€‚ä¿ç•™äº†åŸæœ‰çš„å†…å®¹å’Œç»“æ„ï¼Œåªæ˜¯ï¼š
- æ”¹è¿›äº†è¡¨è¾¾å’Œæ ¼å¼
- æ›´æ¸…æ™°çš„å±‚çº§ç»„ç»‡
- å»æ‰äº†é‡å¤éƒ¨åˆ†
- æ›´ä¸“ä¸šçš„è¯­è¨€é£æ ¼

è¿™æ ·æ—¢ä¿æŒäº†åŸæ„ï¼Œåˆæ›´æ˜“è¯»å’Œä¸“ä¸šã€‚

