[3 tools called]

## 运行流程梳理（limit = 10）

- **准备阶段**
  - `INPUT_MANIFEST="data/hip_code_independent_pure_complete.json"`：已包含若干条任务（至少 7 条）；每条有 `id`, `kernel_file`, `status`, `reason` 等字段。
  - `OUTPUT_FILE="data/result_10.jsonl"`：首次运行通常不存在；脚本已确保目录存在 `mkdir -p data`。
  - 模板目录 `templates/EN/v1`、HIP 源码目录 `../repo_cuda2hip_code`、配置 `llm_providers/config_llm.json` 均与代码逻辑对应。

- **脚本启动 (`hip_to_prompt.py`)**
  - 将 `../..`（`liuji_support`）加入 `sys.path`，从 `llm_providers.prompt_generator`、`llm_providers.llm_generator` 中导入模块。
  - 读取 `llm_providers/config_llm.json`，根据 `MODEL_NAME=openai` 获取相应配置并实例化 `LLMGenerator`。

- **加载任务**
  - 打开 `INPUT_MANIFEST`；解析出任务列表 `all_tasks`，每个任务包含 `kernel_file`。
  - `limit=10`：若任务数 ≥ 10，取前 10 条；若不足，则全部处理（会提示限制值）。
  - 若 `OUTPUT_FILE` 已存在，读取每行 JSON，收集 `pass == true` 的 `id`，实现断点续传；首次运行不会命中。

- **并发处理**
  - 筛选待处理任务 `pending_tasks`，多线程并发数为 `MAX_WORKERS=8`。
  - 对每个任务：
    - 构造 HIP 文件路径：`../repo_cuda2hip_code/<kernel_file>`。
    - 读取源代码，使用模板生成 LLM prompt。
    - 调用 LLM：
      - 成功返回：将原任务字典复制一份，追加 `instruction`（原样写入 LLM 返回文本）、`pass=True`、`error_msg=None`。
      - 失败或解析不到内容：`instruction=None`，`pass=False`，`error_msg` 记录原因（“LLM调用失败”或“无法从响应中提取提示词”）。
      - 如果读取 HIP 文件时异常，也会写入 `pass=False` 及具体错误。
    - 以 JSON 行写入 `OUTPUT_FILE`（保留原字段 + 新增字段）。并发写入使用线程锁，避免相互覆盖。

- **完成状态**
  - 所有任务执行完后输出总结日志：总任务 / 已完成 / 待处理数量、成功处理的文件数、结果保存路径。
  - 再次运行脚本时会跳过 `result_10.jsonl` 中 `pass=true` 的任务；失败项会重新尝试。

## 重点检查结果
- `hip_code_independent_pure_complete.json` 的 `kernel_file` 是否都存在于 `../repo_cuda2hip_code`（保证不会触发文件缺失）。
- `templates/EN/v1` 的 Prompt 可直接产出最终文本（当前逻辑不再解析 JSON）。
- 真实运行前，可选预备步骤：
  - 删除或备份旧的 `data/result_10.jsonl`。
  - 验证 `python -u hip_to_prompt.py --help` 是否正常，确保依赖路径无误。

综上，流程闭环清晰，随时可执行脚本；如需试运行，可直接执行 `bash run_hip_to_prompt.sh`。


### 2. 主要处理阶段：执行代码到指令的转换

这是项目的核心环节，通过运行 `run_hip_to_prompt.sh` 脚本来启动。

*   **执行脚本**: `run_hip_to_prompt.sh`
*   **作用**: 这是一个启动脚本，它负责设置好所有必要的参数（如输入路径、输出文件名、使用的模型、并发线程数等），然后在后台运行主程序 `hip_to_prompt.py`。

`hip_to_prompt.py` 脚本的具体工作流程如下：

1.  **读取任务**: 程序首先读取 `data/manifest.json` 文件，获取需要处理的所有 HIP 文件列表。
2.  **断点续传**: 程序会检查指定的输出文件（例如 `data/result_1000.jsonl`），找出哪些文件已经处理成功，从而避免重复工作，实现断点续传。
3.  **并发处理**: 它会启动一个多线程的线程池（线程数由 `--max-workers` 参数控制），并发地处理多个 HIP 文件，以提高效率。
4.  **构造 Prompt**: 对于每一个 HIP 文件，程序会：
    *   读取该文件的源代码。
    *   加载 `templates/v1/` 目录下的 **提示词模板** (`prompt_template.txt`) 和 **系统指令** (`system_message.txt`)。
    *   将 HIP 源代码填入模板，构造出一个完整的、用于请求 LLM 的 Prompt。这个 Prompt 的大意是：“请根据以下 HIP 代码，生成一段创建这段代码的英文指令”。
5.  **调用 LLM**: 将构造好的 Prompt 发送给在配置中指定的大型语言模型（例如 OpenAI）。
6.  **保存结果**: LLM 会返回生成的英文指令。程序会将**原始文件名**和**LLM返回的指令**等信息，作为一个 JSON 对象，追加写入到输出文件 `data/result_1000.jsonl` 中。每个文件一行，形成了 JSON Lines 格式的数据集。

### 3. 辅助模块

*   `prompt_generator.py`: 这个文件定义了 `HipToPromptGenerator` 类，它封装了从模板和 HIP 代码构造最终 Prompt 的逻辑。
*   `templates/`: 这个目录存放着与 LLM 沟通的“话术”。通过修改这里的模板，可以调整向 LLM 提问的方式，以获得更好的生成结果。

### 流程总结

简单来说，整个项目的流程就是：

`create_manifest.py` (扫描代码生成任务清单) -> `run_hip_to_prompt.sh` (启动主程序) -> `hip_to_prompt.py` (读取任务清单，并发处理) -> `prompt_generator.py` (为每个代码文件构造 Prompt) -> **LLM** (接收 Prompt 并生成指令) -> `hip_to_prompt.py` (将结果保存到文件)。

希望这个梳理能帮助您理解该项目的运作方式。如果您对某个具体细节有疑问，随时可以提出来。
