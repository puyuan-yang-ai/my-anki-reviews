
<html>
<head>
<meta charset="UTF-8">
<title>Export Notes by xxhk.org</title>
<style>

body { font-family: Arial, sans-serif; }
.card { border: 2px solid #000; padding: 10px; border-radius: 10px; margin-bottom: 20px; }
img {
max-width: 100%;
height: auto;
border: 1px solid lightgray;
border-radius: 10px;
display: inline-block;
margin: 10px 0px;
}
.separator { border-top: 1px dashed #000; margin: 10px 0; }
.tags { background-color: lightgray; padding: 5px 10px; margin-bottom: 10px; border-radius: 5px; display: inline-block; }
.footer {
text-align: center;
color: grey;
margin-top: 20px;
padding: 10px;
}

.video-container {
margin: 10px 0;
}
.video-placeholder {
position: relative;
cursor: pointer;
}
.play-button {
position: absolute;
top: 50%;
left: 50%;
transform: translate(-50%, -50%);
width: 68px;
height: 48px;
background-color: rgba(0, 0, 0, 0.7);
border-radius: 14px;
cursor: pointer;
}
.play-button::before {
content: '';
position: absolute;
top: 50%;
left: 55%;
transform: translate(-50%, -50%);
border-style: solid;
border-width: 12px 0 12px 20px;
border-color: transparent transparent transparent white;
}
.play-button:hover {
background-color: red;
}

</style>

</head>
<body>
<div id="cards-container">
<div class="card">
<div class="field_0">批量归一化，英文翻译以及英文简写</div>
<div class="separator"></div>
<div class="field_1">Batch Normalization<br>BN</div>
</div>
<div class="card">
<div class="field_0"><div>eval() 这个代码是否禁止了反向传播和参数更新？以及为什么？<br></div></div>
<div class="separator"></div>
<div class="field_1">没有，因为它只是禁用 Dropout 和 Batch Normalization 的训练行为， 反向传播和 loss.backward()有关，而参数更新的执行和optimizer.step()有关。</div>
</div>
<div class="card">
<div class="field_0"><div>激活函数 ，正则化层</div><div>分别的作用是什么？</div></div>
<div class="separator"></div>
<div class="field_1"><div>激活函数通常用于<b><span style="color: rgb(255, 85, 0);">增加非线性</span></b>，</div><div>正则化层（如 Dropout 或 Batch Normalization）用于<b><span style="color: rgb(255, 85, 0);">控制模型复杂度、提高泛化能力。</span></b></div></div>
</div>
<div class="card">
<div class="field_0">Batch Normalization,BN 里面，应用了什么操作？</div>
<div class="separator"></div>
<div class="field_1">减去均值，除以标准差，将激活标准化到均值0、方差1，然后再应用可学习的缩放和偏移系数。</div>
</div>
<div class="card">
<div class="field_0"><div>Batch Normalization 会根据哪里的 那两个数据 对数据进行归一化。<br></div></div>
<div class="separator"></div>
<div class="field_1">每个批次的均值和方差。</div>
</div>
<div class="card">
<div class="field_0">BN 在深度学习里面表示什么以及？中文以及英文全称</div>
<div class="separator"></div>
<div class="field_1">批量归一化，Batch Normalization</div>
</div>
<div class="card">
<div class="field_0"><div>Dropout 和 Batch Normalization 能否一起使用？以及为什么<div><br></div></div><div></div></div>
<div class="separator"></div>
<div class="field_1">&nbsp;可以一起使用。因为这两者解决的问题不同，并且不会相互冲突</div>
</div>
<div class="card">
<div class="field_0"><div>在 NLP 模型中，eval() 模式主要禁用的是 一个还是两个，是什么？以及为什么？<br></div></div>
<div class="separator"></div>
<div class="field_1">仅仅 禁用Dropout。因为NLP 模型使用 Layer Normalization (LN) 而不是 Batch Normalization (BN)。</div>
</div>
<div class="card">
<div class="field_0"><div><div>Dropout 和 批量归一化 确实解决了不同的问题：Dropout 减少<u>过拟合</u>，Batch Normalization 稳定<u>输入分布</u>和加速<u>收敛。</u><br></div></div><div></div></div>
</div>
<div class="card">
<div class="field_0"><div>Batch Normalization&nbsp;</div>在训练模式下，基于什么来进行什么操作？<br></div>
<div class="separator"></div>
<div class="field_1">基于当前批次的均值和方差进行标准化操作。</div>
</div>
<div class="card">
<div class="field_0">在文本处理的模型中，如果模型没有使用 Batch Normalization (BN)，而是使用 Layer Normalization (LN)，那么在 <b><span style="color: rgb(255, 85, 0);">eval()</span></b> 模式下，禁用的是谁以及为什么？</div>
<div class="separator"></div>
<div class="field_1"><b><span style="color: rgb(255, 85, 0);">仅禁用了 Dropout</span></b>，因为 Layer Normalization 的行为在<b><span style="color: rgb(255, 85, 0);">训练和推理模式下是一致</span></b>的。</div>
</div>
<div class="card">
<div class="field_0">正则化 方法 常用的两种是什么？</div>
<div class="separator"></div>
<div class="field_1"><ul><li><b><span style="color: rgb(255, 85, 0);">Dropout&nbsp;</span></b></li><li>批量归一化（Batch Normalization, <b><span style="color: rgb(255, 85, 0);">BN</span></b>）</li></ul></div>
</div>
<div class="card">
<div class="field_0">model.eval()<br>torch.no_grad()<br>他们两个分别禁止了什么？</div>
<div class="separator"></div>
<div class="field_1"><ul><li>eval()：禁用训练专属的行为，如 Dropout 和 Batch Normalization 的批次统计。</li><li>no_grad()：禁用计算图的构建。</li></ul></div>
</div>
<div class="card">
<div class="field_0">BatchNorm（BN）和 LayerNorm（LN）<br>分别的适合场景是什么？</div>
<div class="separator"></div>
<div class="field_1">BN：适合固定大小的输入张量（如<span style="color: rgb(255, 85, 0);"><b>图像</b></span>任务）<br>LN：适合变长输入张量（如 <b><span style="color: rgb(255, 85, 0);">NLP </span></b>中的序列任务）</div>
</div>
<div class="card">
<div class="field_0">LayerNorm 函数中的 __init__(self, features, eps=1e-6) 代码里面，eps=1e-6的作用?</div>
<div class="separator"></div>
<div class="field_1">eps 是一个<span style="color: rgb(255, 85, 0);"><b>非常小的常数</b></span>（通常是 1e-6），它是为了避免在标准化时除以零。标准化过程中可能会出现<b><span style="color: rgb(255, 85, 0);">方差为零</span></b>的情况，此时如果没有 eps，就会导致<span style="color: rgb(255, 85, 0);"><b>除零错误</b></span>。</div>
</div>
<div class="card">
<div class="field_0"><div>Batch Normalization通过将每一层的输入<u>标准</u>化来减轻这个问题。</div><br></div>
</div>
<div class="card">
<div class="field_0">BatchNorm 和 LayerNorm，谁针对单个特征维度进行归一化，谁针对所有特征维度进行归一化？</div>
<div class="separator"></div>
<div class="field_1"><ul><li>BatchNorm 针对<b><span style="color: rgb(255, 85, 0);">每个特征维度</span></b>进行归一化。</li><li>LayerNorm 针对每个样本的<b><span style="color: rgb(255, 85, 0);">所有特征维度</span></b>进行归一化。</li></ul></div>
</div>
<div class="card">
<div class="field_0">Batch Normalization 是如何来加速深度神经网络训练并提高稳定性？</div>
<div class="separator"></div>
<div class="field_1">在训练过程中，batch normalization 层使用<span style="color: rgb(255, 85, 0);"><b>当前批次</b></span>的<b><span style="color: rgb(255, 85, 0);">均值和方差</span></b>进行归一化。</div>
</div>
<div class="card">
<div class="field_0">批量归一化（Batch Normalization）层 是针对谁，怎么样？</div>
<div class="separator"></div>
<div class="field_1">需要在<span style="color: rgb(255, 85, 0);"><b>一个批次的数据</b></span>上计算<b><span style="color: rgb(255, 85, 0);">均值和方差</span></b></div>
</div>
<div class="card">
<div class="field_0">Transformer 使用 BatchNorm 的归一化 ，为什么归一化结果不准确？</div>
<div class="separator"></div>
<div class="field_1">填充值会干扰有效 Token 的分布。</div>
</div>
<div class="card">
<div class="field_0">Transformer 里面，LayerNorm 与 什么 配合使用，可以完全屏蔽填充值的影响？</div>
<div class="separator"></div>
<div class="field_1">Padding Mask，填充掩码。</div>
</div>
<div class="card">
<div class="field_0">Layer Normalization 的标准化步骤确实是先 减去均值，然后 除以标准差，从而使得谁具有什么效果？</div>
<div class="separator"></div>
<div class="field_1">使得<span style="color: rgb(255, 85, 0);"><b>每个位置的输出数据</b></span>具有<b><span style="color: rgb(255, 85, 0);">均值为 0 和方差为 1。</span></b></div>
</div>
<div class="card">
<div class="field_0">LayerNorm 函数中的 __init__(self, features, eps=1e-6)中，features 是指什么？</div>
<div class="separator"></div>
<div class="field_1"><span style="color: rgb(255, 85, 0);"><b>输入数据的特征维度</b></span>（即要进行标准化的维度）。如果输入数据是一个形状为 (batch_size, seq_len, <span style="color: rgb(255, 85, 0);"><b>d_model</b></span>) 的张量，那么 features 就是 d_model，表示<b><span style="color: rgb(255, 85, 0);">每个位置的表示（词向量）维度</span></b>。<br></div>
</div>
<div class="card">
<div class="field_0">Transformer 使用 LayerNorm 还是 BatchNorm？</div>
<div class="separator"></div>
<div class="field_1">LayerNorm 。</div>
</div>
<div class="card">
<div class="field_0">当你调用 model.eval() 时，PyTorch 会自动将哪两个层切换到评估模式</div>
<div class="separator"></div>
<div class="field_1">所有 <span style="color: rgb(255, 85, 0);"><b>dropout </b></span>和 <b><span style="color: rgb(255, 85, 0);">batch normalization</span></b> 层</div>
</div>
<div class="card">
<div class="field_0">【面试题】Transformer 里面，为什么使用LayerNorm 而不是&nbsp;<span style="color: rgb(13, 13, 13); background-color: rgb(255, 255, 255);">BatchNorm？</span></div>
<div class="separator"></div>
<div class="field_1"><span style="background-color: rgb(255, 255, 255);"><span style="color: rgb(13, 13, 13);">因为 NLP 里的</span><span style="color: rgb(255, 85, 0);"><b>句子长度不一样</b></span><span style="color: rgb(13, 13, 13);">，</span><span style="color: rgb(255, 85, 0);"><b>短句</b></span><span style="color: rgb(13, 13, 13);">会被填充（</span><span style="color: rgb(255, 85, 0);"><b>padding</b></span><span style="color: rgb(13, 13, 13);">）。</span><br><span style="color: rgb(255, 85, 0);"><b>BatchNorm </b></span><span style="color: rgb(13, 13, 13);">要依赖 mini-batch 的统计量，</span><span style="color: rgb(255, 85, 0);"><b>填充值会干扰均值和标准差的计算/</b></span></span><span style="color: rgb(255, 85, 0);"><b>统计计算</b></span><span style="background-color: rgb(255, 255, 255);"><span style="color: rgb(13, 13, 13);">，影响</span><span style="color: rgb(255, 85, 0);"><b>归一化结果</b></span></span>，导致归一化<b><span style="color: rgb(255, 85, 0);">不准确</span></b>。<span style="background-color: rgb(255, 255, 255);"><br><span style="color: rgb(13, 13, 13);">而 </span><span style="color: rgb(255, 85, 0);"><b>LayerNorm </b></span><span style="color: rgb(13, 13, 13);">是对每个</span><span style="color: rgb(255, 85, 0);"><b>句子单独归一化</b></span><span style="color: rgb(13, 13, 13);">，只考虑每个句子自身的特征，</span><span style="color: rgb(255, 85, 0);"><b>不受填充值影响</b></span><span style="color: rgb(13, 13, 13);">。这样就避免了填充带来的问题，更适合 NLP 的场景。</span></span></div>
</div>
<div class="card">
<div class="field_0">Layer Normalization 层归一化，是简单的标准化吗？</div>
<div class="separator"></div>
<div class="field_1">不仅仅是简单的归一化。我们还引入了 缩放因子（a_2） 和 偏置项（b_2）。</div>
</div>
<div class="card">
<div class="field_0">LayerNorm&nbsp;的归一化是否会受到填充值的干扰？</div>
<div class="separator"></div>
<div class="field_1">不会。</div>
</div>
<div class="card">
<div class="field_0">Transformer结构里面，每个子层后都有一组什么和什么的组合操作？</div>
<div class="separator"></div>
<div class="field_1">Add &amp; Norm<br><span style="color: rgb(255, 85, 0);"><b>残差</b></span>连接（Residual Connection），即<span style="color: rgb(255, 85, 0);"><b>输入直接加到输出</b></span>上，然后进行<b><span style="color: rgb(255, 85, 0);">层归一化</span></b>（Layer Normalization）。</div>
</div>
<div class="card">
<div class="field_0">残差连接和层归一化（Residual&nbsp;Connections&nbsp;and Layer&nbsp;Normalization）<div>分别的作用是什么？</div></div>
<div class="separator"></div>
<div class="field_1">1 防止梯度消失，<br>2 促进信息流动，加速收敛。</div>
</div>
<div class="card">
<div class="field_0">Layer Normalization（层归一化）中：<br>偏置项，的作用是什么？</div>
<div class="separator"></div>
<div class="field_1">对数据分布进行平移，用于调整标准化后数据的 均值。</div>
</div>
<div class="card">
<div class="field_0">Layer Normalization&nbsp; 中谁调整数据的尺度，以及谁调整数据的均值。</div>
<div class="separator"></div>
<div class="field_1"><li><code>a_2</code> 是 <strong>缩放因子</strong>，调整数据的尺度。</li><li><code>b_2</code> 是 <strong>偏置项</strong>，调整数据的均值。</li></div>
</div>
<div class="card">
<div class="field_0">“Add &amp; Norm”指的是Transformer模型中的两个连续操作，分别是什么</div>
<div class="separator"></div>
<div class="field_1">残差连接（Residual Connection）和层归一化（Layer Normalization）</div>
</div>
<div class="card">
<div class="field_0">Layer Normalization（层归一化）中：<br>包含哪两个重要的数值，以及作用是什么？</div>
<div class="separator"></div>
<div class="field_1"><li><strong><code>a_2</code></strong> 缩放因子，调节数据的 <strong>方差</strong>（尺度）。</li><li><strong><code>b_2</code></strong> 偏置项，调整数据的 <strong>均值</strong>（位置）。</li></div>
</div>
<div class="card">
<div class="field_0">LayerNorm 则是针对每个句子单独计算特征维度的均值和标准差，是否依赖 batch？</div>
<div class="separator"></div>
<div class="field_1">不依赖。</div>
</div>
<div class="card">
<div class="field_0">Layer Normalization 层归一化，包含了标准化，但是呢，为什么还要包含缩放（a_2） 和 偏移（b_2）额外操作呢？</div>
<div class="separator"></div>
<div class="field_1">标准化后数据的均值变为 0，方差变为 1，这个会导致数据分布比较固定，缺乏灵活性。而额外的缩放和偏移呢可以让模型自动学习每一层的最佳的分布，进而均值和方差可以根据不同的任务的需求来进行动态调整。</div>
</div>
<div class="card">
<div class="field_0"><div>在训练模式 和 评估模式下，dropout层 和 batch normalization层各自的区别是什么？<br></div></div>
<div class="separator"></div>
<div class="field_1"><ul><li>在训练模式下，dropout层会<span style="color: rgb(255, 85, 0);"><b>随机丢弃</b></span>一些神经元以防止过拟合，batch normalization层会使用<b><span style="color: rgb(255, 85, 0);">当前批次的数据统计量。</span></b></li><li>在评估模式下，dropout层<span style="color: rgb(255, 85, 0);"><b>不会丢弃</b></span>神经元，batch normalization层会使用训练过程中<b><span style="color: rgb(255, 85, 0);">累积的全局统计量。</span></b></li></ul></div>
</div>
<div class="card">
<div class="field_0">残差连接和层归一化（Residual&nbsp;Connections&nbsp;and Layer&nbsp;Normalization）<div>作用：改善模型的训练效果和稳定性。</div>如何实现？<br></div>
<div class="separator"></div>
<div class="field_1">&nbsp;在子层输出加上输入，然后进行归一化处理。</div>
</div>
<div class="card">
<div class="field_0"><u>Batch Normalization</u>（<u>批归一化</u>，简称BN）</div>
</div>
<div class="card">
<div class="field_0">Transformer代码里面，self.sublayer = clones(SublayerConnection(size, dropout), 2)&nbsp;<br>SublayerConnection&nbsp;的作用是将什么封装起来？</div>
<div class="separator"></div>
<div class="field_1">&nbsp;Add &amp; Norm 。</div>
</div>
<div class="card">
<div class="field_0">Layer Normalization（层归一化）中：<br>缩放因子，作用是什么？</div>
<div class="separator"></div>
<div class="field_1">用于调整标准化后数据的 方差/尺度。</div>
</div>
<div class="card">
<div class="field_0">Batch Normalization 是针对 哪里 中 什么进行归一化的，而 Layer Normalization 是针对哪里的 什么进行归一化。</div>
<div class="separator"></div>
<div class="field_1">mini-batch，每个特征维度。<br>每个样本序列，所有特征维度。</div>
</div>
<div class="card">
<div class="field_0">Layer Normalization（层归一化）中：<br>a_2 用来缩放数据的什么，b_2 用来平移数据的什么？</div>
<div class="separator"></div>
<div class="field_1">方差，均值。</div>
</div>
<div class="card">
<div class="field_0"><div>model.eval() 方法将模型切换到什么模式，这会影响模型中的哪些层的行为？<br></div></div>
<div class="separator"></div>
<div class="field_1">评估模式， dropout层和batch normalization层。</div>
</div>
<div class="card">
<div class="field_0">Layer Normalization 层归一化，谁是可训练的参数，以及分别的作用？</div>
<div class="separator"></div>
<div class="field_1">a_2 和 b_2 是可训练的参数，用于控制输出的 尺度 和 平移【缩放因子对应的是调整方差，偏置项对应的是调整均值。】</div>
</div>
<div class="card">
<div class="field_0">如图所示，属于BatchNorm 还是 LayerNorm ，以及为什么？<br><img src="images/paste-1a03c937f500ccb14d2ed963d7c9d4a4be0fdccc.jpg"></div>
<div class="separator"></div>
<div class="field_1">BatchNorm ，因为BatchNorm&nbsp;针对每个特征维度进行归一化。</div>
</div>
<div class="card">
<div class="field_0"><span style="color: rgb(13, 13, 13); background-color: rgb(255, 255, 255);"><span style="font-style: italic;">O</span><span style="font-style: italic;">u</span><span style="font-style: italic;">tp</span><span style="font-style: italic;">u</span><span style="font-style: italic;">t</span>=</span><span style="color: rgb(13, 13, 13); background-color: rgb(255, 255, 255);"><span style="font-style: italic;">L</span><span style="font-style: italic;">a</span><span style="font-style: italic;">yer</span><span style="font-style: italic;">N</span><span style="font-style: italic;">or</span><span style="font-style: italic;">m</span>(<span style="font-style: italic;">x</span>+</span><span style="color: rgb(13, 13, 13); background-color: rgb(255, 255, 255);"><span style="font-style: italic;">S</span><span style="font-style: italic;">u</span><span style="font-style: italic;">b</span><span style="font-style: italic;">l</span><span style="font-style: italic;">a</span><span style="font-style: italic;">yer</span>(<span style="font-style: italic;">x</span>))</span><br>这个公式表述了什么过程？</div>
<div class="separator"></div>
<div class="field_1">残差连接和层归一化。<br>对每个子层，输出与输入相加，然后进行层归一化。</div>
</div>
<div class="card">
<div class="field_0">RNNs模型里面，Layer Normalization 是否会破坏时间步之间的依赖关系，以及为什么？</div>
<div class="separator"></div>
<div class="field_1">不会，因为是在每个时间步都进行归一化&nbsp;</div>
</div>
<div class="card">
<div class="field_0">一个立方体，三个维度分别是batch_size，seq_len，以及 embedding_size，绘制LayerNorm&nbsp;的可视化图？&nbsp;</div>
<div class="separator"></div>
<div class="field_1"><img src="images/paste-0b673fe29431dda7031987d3afe2a2cff5eda91a.jpg"></div>
</div>
<div class="card">
<div class="field_0">LayerNorm 是否与 mini-batch 的大小有关，以及为什么？</div>
<div class="separator"></div>
<div class="field_1">无关，因为单个序列为单位。<br>只对单个样本的特征维度进行归一化，不依赖于其他样本。</div>
</div>
<div class="card">
<div class="field_0">BatchNorm 的归一化在 NLP 中，变长序列的填充值是否会干扰统计量？</div>
<div class="separator"></div>
<div class="field_1">是。</div>
</div>
<div class="card">
<div class="field_0">残差连接和层归一化 的英文</div>
<div class="separator"></div>
<div class="field_1">Residual&nbsp;Connections&nbsp;and Layer&nbsp;Normalization</div>
</div>
<div class="card">
<div class="field_0">Batch Normalization 是针对 哪里 中 什么进行归一化的?</div>
<div class="separator"></div>
<div class="field_1">mini-batch，每个特征维度。</div>
</div>
<div class="card">
<div class="field_0">self.sublayer[0]&nbsp;负责执行&nbsp;Add &amp; Norm&nbsp;操作，它将输入&nbsp;x&nbsp;和&nbsp;self.self_attn(x, x, x, target_mask)作为输入【代码】</div>
<div class="separator"></div>
<div class="field_1">x = self.sublayer[0](x, <b><span style="color: rgb(255, 85, 0);">lambda x: </span></b>self.self_attn(x, x, x, target_mask))</div>
</div>
<div class="card">
<div class="field_0">Layer Normalization 的标准化步骤 有那四个步骤？</div>
<div class="separator"></div>
<div class="field_1">先 减去均值，然后 除以标准差差，以及缩放和偏移。</div>
</div>
<div class="card">
<div class="field_0">Layer Normalization&nbsp; 的作用是以便模型能更好地调整什么？</div>
<div class="separator"></div>
<div class="field_1">每一层的输出。</div>
</div>
<div class="card">
<div class="field_0">BN层（<u>Batch Normalization</u>）</div>
</div>
<div class="card">
<div class="field_0">为什么在NLP 里面，BatchNorm 不合适？</div>
<div class="separator"></div>
<div class="field_1">但在 <span style="color: rgb(255, 85, 0);"><b>NLP </b></span>中，句子<span style="color: rgb(255, 85, 0);"><b>长度不同</b></span>，需要<span style="color: rgb(255, 85, 0);"><b>填充值</b></span>，BatchNorm 会受到这些<b><span style="color: rgb(255, 85, 0);">填充值的干扰。</span></b><br>填充项会干扰归一化的计算。</div>
</div>
<div class="card">
<div class="field_0">Layer Normalization 的标准化步骤确实是先 执行什么操作，然后 再怎么样，从而使得每个位置的输出数据具有均值为 0 和方差为 1</div>
<div class="separator"></div>
<div class="field_1">减去均值，&nbsp;除以标准差。</div>
</div>
<div class="card">
<div class="field_0">LayerNorm&nbsp;的归一化是针对 什么范围 内的 什么进行<span style="color: rgb(13, 13, 13); background-color: rgb(255, 255, 255);">归一化？</span></div>
<div class="separator"></div>
<div class="field_1">针对&nbsp;<span style="color: rgb(255, 85, 0);"><b>单个样本</b></span>的<b><span style="color: rgb(255, 85, 0);">所有特征维度</span></b>，它在&nbsp;每个样本的特征维度范围内&nbsp;计算均值和标准差。</div>
</div>
<div class="card">
<div class="field_0">BatchNorm（BN）和 LayerNorm（LN）<br>谁会受到填充（padding）影响？</div>
<div class="separator"></div>
<div class="field_1">BN：填充值干扰统计量，导致归一化不准确。<br>LN：不受填充值影响，天然适配序列任务。</div>
</div>
<div class="card">
<div class="field_0">Layer Normalization 是针对哪里的 什么进行归一化？</div>
<div class="separator"></div>
<div class="field_1">每个样本序列，所有特征维度。</div>
</div>
<div class="card">
<div class="field_0">如图所示，属于BatchNorm 还是 LayerNorm ，以及为什么？<br><img src="images/paste-0b673fe29431dda7031987d3afe2a2cff5eda91a.jpg"></div>
<div class="separator"></div>
<div class="field_1">LayerNorm ，单个样本，整个序列，所有特征维度进行归一化。</div>
</div>
<div class="card">
<div class="field_0"><div>model.eval() 这行代码的作用是什么？【切换到什么模型，关闭了哪两个环节】<br></div><div></div></div>
<div class="separator"></div>
<div class="field_1">切换模型到评估模式 ，关闭 Dropout 和 BatchNorm</div>
</div>
<div class="card">
<div class="field_0">一个立方体，三个维度分别是batch_size，seq_len，以及 embedding_size，绘制BatchNorm&nbsp;的可视化图？&nbsp;</div>
<div class="separator"></div>
<div class="field_1"><img src="images/paste-1a03c937f500ccb14d2ed963d7c9d4a4be0fdccc.jpg"></div>
</div>
<div class="card">
<div class="field_0">Batch Normalization 的中文名称</div>
<div class="separator"></div>
<div class="field_1">（批归一化）</div>
</div>
<div class="card">
<div class="field_0">Batch Normalization 层，在训练以及评估，这两种模式下的行为是什么？<div></div></div>
<div class="separator"></div>
<div class="field_1"><li><strong>训练模式</strong>：使用<b><span style="color: rgb(255, 85, 0);">当前批次</span></b>的均值和方差。</li><li><strong>评估模式</strong>：使用训练过程中计算的<b><span style="color: rgb(255, 85, 0);">移动平均</span></b>均值和方差。</li></div>
</div>
<div class="card">
<div class="field_0">Layer Normalization 中，a_2 和 b_2 是可训练的 缩放因子（scale） 和 偏置项（bias）,分别用于控制归一化后数据的什么和什么？</div>
<div class="separator"></div>
<div class="field_1">尺度和位置，也就是方差和均值。</div>
</div>
<div class="card">
<div class="field_0">在Transformer模型中，Add &amp; Norm层由两个部分组成，分别是什么名称以及作用？</div>
<div class="separator"></div>
<div class="field_1">残差连接，将输入加到输出上<br>层归一化，对一层的数据进行标准化。<br><br><ol><li><strong>Add（加法）</strong>：这是指<strong>残差连接（Residual Connection）</strong>，即将子层的输入与输出相加。</li><li><strong>Norm（归一化）</strong>：这是指<strong>层归一化（Layer Normalization）</strong>，对相加后的结果进行标准化处理。</li></ol></div>
</div>
<div class="card">
<div class="field_0">Transformer 里用 BatchNorm，为什么会导致统计结果不准确？</div>
<div class="separator"></div>
<div class="field_1">填充的 padding&nbsp;会影响统计结果，导致归一化不准确。</div>
</div>
<div class="card">
<div class="field_0">Transformer 使用LayerNorm 的归一化 是否与句子长度和填充值有关？以及为什么？</div>
<div class="separator"></div>
<div class="field_1">无关，因为针对的是单个样本的所有特征维度，单个的序列为单位。</div>
</div>
<div class="card">
<div class="field_0">BatchNorm 的归一化是针对 什么范围 内的每一个什么进行<span style="color: rgb(13, 13, 13); background-color: rgb(255, 255, 255);">归一化？</span></div>
<div class="separator"></div>
<div class="field_1">mini-batch，特征维度。</div>
</div>
<div class="card">
<div class="field_0">为什么Batch Normalization 在 RNN 中确实不是很有效？</div>
<div class="separator"></div>
<div class="field_1">RNNs 处理的是序列数据，<span style="color: rgb(255, 85, 0);"><b>每个时间步</b></span>的数据都<span style="color: rgb(255, 85, 0);"><b>依赖于前一个时间步</b></span>的输出。Batch Normalization 在每个时间步都会计算均值和方差，这可能会<b><span style="color: rgb(255, 85, 0);">破坏时间步之间的依赖关系</span></b>，从而导致模型表现不稳定。</div>
</div>
<div class="card">
<div class="field_0">BatchNorm（BN）和 LayerNorm（LN）<br>均值和标准差的计算的范围是什么？</div>
<div class="separator"></div>
<div class="field_1"><ul><li>BatchNorm（BN）：在整个 <span style="color: rgb(255, 85, 0);"><b>mini-batch</b></span> 中计算其中<b><span style="color: rgb(255, 85, 0);">一个维度</span></b>。</li><li>LayerNorm（LN）：在<span style="color: rgb(255, 85, 0);"><b>单个样本序列</b></span>的<span style="color: rgb(255, 85, 0);"><b>所有特征维度</b></span>范围内计算。</li></ul></div>
</div>
<div class="card">
<div class="field_0">Transformer代码里面，Add &amp; Norm&nbsp;操作被封装在 哪个类中？</div>
<div class="separator"></div>
<div class="field_1">SublayerConnection 。子层连接结构。</div>
</div>
<div class="card">
<div class="field_0">在Transformer模型中，Add &amp; Norm层<br>Norm 指的是什么？</div>
<div class="separator"></div>
<div class="field_1"><ol><li><strong>Norm（归一化）</strong>：这是指<strong>层归一化（Layer Normalization）</strong>，对相加后的结果进行标准化处理。</li></ol></div>
</div>
<div class="card">
<div class="field_0">Layer Normalization ，是如何避免了时间步之间依赖关系的破坏</div>
<div class="separator"></div>
<div class="field_1">单独对每个时间步的隐藏状态进行归一化</div>
</div>
<div class="card">
<div class="field_0">Transformer 使用 BatchNorm 的归一化 ，什么会干扰有效 Token 的分布，导致归一化结果不准确？</div>
<div class="separator"></div>
<div class="field_1">填充的token【填充项】</div>
</div>
<div class="card">
<div class="field_0">BatchNorm 是在 什么的范围内，对什么维度归一化？</div>
<div class="separator"></div>
<div class="field_1">整个 mini-batch 的范围，每个/单个特征维度。</div>
</div>
<div class="card">
<div class="field_0">Layer Normalization 对什么进行归一化？</div>
<div class="separator"></div>
<div class="field_1">每个时间步 t 的隐藏层</div>
</div>
<div class="card">
<div class="field_0">Layer Normalization（层归一化）中：<br>缩放因子，为什么可以调整标准化后数据的 方差？</div>
<div class="separator"></div>
<div class="field_1">缩放因子可以调整数据的扩展性，即 放大 或 缩小 数据的分布/尺度，而数据的分布对应的就是方差。</div>
</div>
<div class="card">
<div class="field_0"><div>Layer Normalization对 什么 进行规范化，稳定什么？<br></div></div>
<div class="separator"></div>
<div class="field_1">每一层的输出<br>数据分布</div>
</div>
<div class="card">
<div class="field_0">LayerNorm 则是针对谁单独计算特征维度的均值和标准差 ？</div>
<div class="separator"></div>
<div class="field_1">每个句子/序列。</div>
</div>
<div class="card">
<div class="field_0">&nbsp; 针对 Batch Normalization 在序列模型中的局限性 的替代方案是什么？</div>
<div class="separator"></div>
<div class="field_1">Layer Normalization（层归一化）</div>
</div>
<div class="card">
<div class="field_0">Batch Normalization 在 RNN 中，是否是有效的方法？</div>
<div class="separator"></div>
<div class="field_1">Batch Normalization 在 RNN 中确实不<span style="color: rgb(255, 85, 0);"><b>是很有效</b></span>，而 <b><span style="color: rgb(255, 85, 0);">Layer Normalization</span></b> 是主流的方法之一</div>
</div>
<div class="card">
<div class="field_0">在模型测试时务必使用 model.eval()，原因是什么</div>
<div class="separator"></div>
<div class="field_1">避免 dropout 和 batch normalization 层 【批归一化】 在测试时引入不确定性</div>
</div>
<div class="card">
<div class="field_0">Layer Normalization 中，哪两个参数可以在训练过程中学习到，以便模型能更好地调整每一层的输出。</div>
<div class="separator"></div>
<div class="field_1">a_2 和 b_2 是可训练的 缩放因子（scale） 和 偏置项（bias），它们分别用于控制归一化后数据的<span style="color: rgb(255, 85, 0);"><b>尺度</b></span>和<span style="color: rgb(255, 85, 0);"><b>位置</b></span>。&nbsp;</div>
</div>

<script>
document.addEventListener('click', function(e) {
const container = e.target.closest('.video-container');
if (!container) return;

const placeholder = container.querySelector('.video-placeholder');
if (!placeholder) return;

const videoId = container.dataset.videoId;
const startParam = container.dataset.start || '';

const iframe = document.createElement('iframe');
iframe.width = '100%';
iframe.height = '100%';
iframe.src = `https://www.youtube.com/embed/${videoId}?${startParam.slice(1)}`;
iframe.title = 'YouTube video player';
iframe.frameBorder = '0';
iframe.style.position = 'absolute';
iframe.style.top = '0';
iframe.style.left = '0';
iframe.style.borderRadius = '15px';
iframe.allow = 'accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share';
iframe.allowFullscreen = true;

placeholder.innerHTML = '';
placeholder.appendChild(iframe);
});
</script>

</div>
<div class="footer">Made by <a href="https://xxhk.org">Export Notes - XXHK</a></div>
</body>
</html>

